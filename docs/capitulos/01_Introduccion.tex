% !TeX root = ../proyecto.tex

\chapter{Introducción}\label{ch:introduccion}

%Contexto: Breve introducción al tema y su relevancia.
En la actualidad, vivimos en una era marcada por una constante y acelerada generación de datos.
Este fenómeno ha incrementado la necesidad de desarrollar métodos eficaces para el procesamiento y análisis de grandes
volúmenes de información.
En este contexto, los \textbf{modelos de aprendizaje profundo}, y en particular las
\textbf{redes neuronales convolucionales} (CNN), han demostrado un notable rendimiento en tareas como la
\textbf{clasificación de imágenes}, el \textbf{reconocimiento de patrones} y diversas aplicaciones de alta complejidad.
No obstante, el entrenamiento de estos modelos suele requerir volúmenes elevados de datos, lo que plantea desafíos
significativos tanto en términos de \textbf{tiempo} como de \textbf{costes} asociados a su obtención.



Conforme los sistemas de inteligencia artificial evolucionan hacia estructuras más sofisticadas y precisas, la
disponibilidad de conjuntos de datos amplios y adecuados se vuelve un requisito cada vez más crucial.
Sin embargo, la recopilación, almacenamiento y tratamiento de estos datos suponen obstáculos importantes, especialmente
para aquellas instituciones u organizaciones que cuentan con recursos limitados.
Esta situación pone de relieve la necesidad de investigar estrategias innovadoras que permitan
\textbf{reducir y optimizar los conjuntos de datos} sin comprometer el rendimiento de los modelos entrenados.


En consecuencia, surge la necesidad de \textbf{reducir los conjuntos de datos de entrenamiento} 
para mejorar la \textbf{eficiencia} en el desarrollo de modelos de aprendizaje profundo.
Aunque las redes neuronales convolucionales han demostrado un rendimiento notable en diversas tareas, su implementación
conlleva \textbf{altos costes computacionales} y una gran demanda de datos, lo que  supone un obstáculo importante en muchos escenarios,
especialmente aquellos con recursos limitados.
Una estrategia prometedora consiste en entrenar estos modelos utilizando únicamente una fracción de los datos
disponibles, seleccionados de manera óptima.
Esta aproximación permitiría disminuir significativamente el consumo de recursos sin comprometer la precisión del
modelo, lo que supondría un avance importante para la \textbf{inteligencia artificial}, en particular en aplicaciones
donde existen \textbf{restricciones de recursos}.


En este contexto, la selección inteligente de subconjuntos representativos constituye una estrategia eficaz para reducir tanto
los tiempos de entrenamiento como el uso de recursos, sin afectar negativamente el rendimiento.
Es precisamente en este punto donde las \textbf{metaheurísticas} adquieren un papel relevante.
Estas técnicas de optimización están diseñadas para abordar problemas complejos en los que los métodos tradicionales
resultan ineficaces, gracias a sus \textbf{estrategias de búsqueda} y \textbf{exploración del espacio de soluciones}.
Al combinar diversas heurísticas, permiten encontrar soluciones aproximadas en tiempos razonables, lo que las convierte
en una herramienta especialmente útil cuando la obtención de una solución exacta resulta inviable desde el punto de
vista computacional.


Gracias a estas características, las metaheurísticas se posicionan como una alternativa sólida 
para mejorar la eficiencia y accesibilidad del aprendizaje profundo, incluso en contextos con recursos limitados.
Esto es especialmente relevante para avanzar hacia una inteligencia artificial más abierta, \textbf{democrática} y aplicable
en una mayor variedad de escenarios.


En este contexto, el presente Trabajo de Fin de Grado tiene como objetivo aplicar técnicas metaheurísticas para realizar una selección
inteligente de ejemplos, con el fin de reducir el tamaño de los conjuntos de entrenamiento sin que ello afecte
significativamente a los resultados obtenidos.
La investigación aspira a contribuir al desarrollo de modelos más \textbf{eficientes}, accesibles y económicamente
sostenibles, fomentando así un futuro en el que la inteligencia artificial sea más \textbf{inclusiva} y
\textbf{sostenible}.


%Objetivos: Especifica qué pretendes lograr con tu TFG.
El objetivo principal de este TFG es investigar la aplicación de \textbf{metaheurísticas} para la
\textbf{reducción de conjuntos de datos de entrenamiento} en modelos de \textbf{aprendizaje profundo convolucionales}.
Este estudio permitirá evaluar el impacto de dichos algoritmos en la \textbf{eficiencia computacional} y en el
\textbf{rendimiento de los modelos}.
Para lograr estos objetivos, se desarrollarán y compararán distintas técnicas de selección de instancias aplicadas
sobre modelos convolucionales, incluyendo enfoques aleatorios, de búsqueda local, genéticos y meméticos.


Para cumplir con este objetivo general, se plantean los siguientes \textbf{objetivos específicos}:

\begin{itemize}
    \item \textbf{Estudiar} la conveniencia del uso de metaheurísticas para la reducción de conjuntos de imágenes
          en modelos de aprendizaje profundo.
    \item \textbf{Desarrollar} y \textbf{mejorar} algoritmos metaheurísticos orientados a la selección de ejemplos representativos,
          con el objetivo de optimizar el entrenamiento de modelos convolucionales sin comprometer su rendimiento.
    \item \textbf{Evaluar} el impacto de la reducción de datos en el rendimiento de los modelos, comparando aspectos clave
          como la precisión, eficacia y el tiempo de entrenamiento, en modelos entrenados con conjuntos de datos completos frente a conjuntos reducidos.
    \item \textbf{Comparar} el rendimiento de metaheurísticas con porcentajes de selección fijos frente a aquellos que
          permiten una selección flexible del número de ejemplos, analizando sus ventajas y limitaciones.
\end{itemize}

A través de este estudio, se busca no solo mejorar el rendimiento y la eficiencia de los modelos convolucionales,
sino también fomentar el desarrollo de soluciones más \textbf{sostenibles y accesibles} en el ámbito de la inteligencia artificial,
mediante la incorporación de estrategias metaheurísticas como \textbf{propuestas innovadoras} para la reducción de datos de entrenamiento.
