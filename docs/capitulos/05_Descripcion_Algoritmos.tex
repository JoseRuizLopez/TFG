% !TeX root = ../proyecto.tex

\chapter{Descripción de los Algoritmos}\label{ch:descripcion-algoritmos}
En este capítulo se describen los diferentes algoritmos utilizados en el desarrollo de este trabajo.
Todos ellos tienen como objetivo principal reducir el tamaño del conjunto de datos de entrenamiento utilizado en los
modelos de aprendizaje profundo, con el fin de optimizar el rendimiento y reducir el costo computacional.
El enfoque adoptado en este trabajo es la aplicación de algoritmos meméticos, los cuales combinan principios de
algoritmos genéticos con estrategias de búsqueda local.


A continuación, se detallan los algoritmos principales implementados en este proyecto: el \textbf{algoritmo aleatorio},
el \textbf{algoritmo de búsqueda local}, el \textbf{algoritmo genético}y el \textbf{algoritmo memético}.

\section{Algoritmo Aleatorio}\label{sec:algoritmo-aleatorio}
%Algoritmos Meméticos: Detalla qué son y cómo se aplican a la reducción de datos.
El \textbf{algoritmo aleatorio} sirve como referencia básica para medir la efectividad de los algoritmos más avanzados.
Este enfoque selecciona subconjuntos de datos de manera completamente aleatoria, sin aplicar ningún tipo de estrategia
de optimización.

\subsection{Descripción}\label{subsec:descripcion}
El algoritmo comienza tomando el conjunto de datos completo y seleccionando una fracción de los ejemplos de
entrenamiento de forma aleatoria.
Esta selección se realiza sin ningún criterio basado en la relevancia de los datos, lo que implica que el conjunto de
entrenamiento resultante puede no ser representativo o puede contener redundancias innecesarias.

\subsection{Aplicación en la reducción de datos}\label{subsec:aplicacion-en-la-reduccion-de-datos}
A pesar de su simplicidad, el \textbf{algoritmo aleatorio} puede ser útil como método de comparación.
En muchos casos, los algoritmos más complejos deben demostrar que pueden superar este enfoque básico en términos de
precisión y eficiencia.
Al seleccionar datos de manera aleatoria, este método a menudo produce conjuntos de entrenamiento subóptimos, lo que
resulta en modelos menos precisos o con mayor varianza.

\subsection{Resultados esperados}\label{subsec:resultados-esperados}
Debido a la naturaleza aleatoria del algoritmo, los resultados son altamente variables.
Es probable que en muchas ejecuciones el rendimiento del modelo entrenado sea inferior al obtenido con métodos más
estructurados.
Este algoritmo proporciona una línea base importante para evaluar la efectividad de los algoritmos más avanzados.


\section{Algoritmo Búsqueda Local}\label{sec:algoritmo-busqueda-local}
El \textbf{algoritmo de búsqueda local} es una técnica más sofisticada que explora el espacio de soluciones de manera
más estructurada, buscando mejorar progresivamente una solución inicial.

\subsection{Descripción}\label{subsec:descripcion2}
La búsqueda local se basa en la idea de comenzar con una solución inicial (un subconjunto de datos) y realizar pequeños
cambios o `movimientos' en esa solución para explorar otras soluciones cercanas.
En este contexto, cada solución es un subconjunto de datos.
El algoritmo evalúa diferentes subconjuntos de datos probando si estos mejoran el rendimiento del modelo de aprendizaje
profundo al entrenarlo con ellos.


El proceso básico de la búsqueda local es el siguiente:
\begin{enumerate}
      \item Se genera una solución inicial, por ejemplo, seleccionando un subconjunto de datos aleatoriamente.
      \item Se realizan cambios locales en la solución, como añadir o eliminar ejemplos del conjunto de datos.
      \item Se evalúa la nueva solución según el rendimiento del modelo de aprendizaje profundo.
      \item Si la nueva solución es mejor, se reemplaza la solución actual por esta.
      \item El proceso se repite hasta que no se observan mejoras significativas o hasta que se alcanza un número
      \item predefinido de iteraciones.
\end{enumerate}

\subsection{Aplicación en la reducción de datos}\label{subsec:aplicacion-en-la-reduccion-de-datos2}
En el contexto de la reducción de datos, el objetivo de la búsqueda local es identificar un subconjunto más pequeño de
ejemplos que sea suficiente para entrenar el modelo con un rendimiento similar al obtenido con el conjunto de datos
completo.
La búsqueda local explora el espacio de posibles subconjuntos, eliminando ejemplos redundantes o irrelevantes, y
conservando solo aquellos que son cruciales para el rendimiento del modelo.

\subsection{Ventajas y limitaciones}\label{subsec:ventajas-y-limitaciones}
\textbf{Ventajas}: Este enfoque permite una exploración más exhaustiva del espacio de soluciones que un algoritmo
aleatorio.
Al hacer pequeños ajustes en cada iteración, el algoritmo puede encontrar mejores soluciones de manera eficiente.


\textbf{Limitaciones}: Sin embargo, la búsqueda local puede quedarse atrapada en \textbf{óptimos locales}, es decir,
soluciones que parecen buenas en comparación con las cercanas, pero que no son globalmente óptimas.

\section{Algoritmos Genéticos}\label{sec:algoritmos-geneticos}
Los \textbf{algoritmos genéticos} son algoritmos de búsqueda inspirados en los principios de la evolución natural.
En este trabajo, se aplican con el objetivo de encontrar subconjuntos óptimos de datos de entrenamiento, reduciendo el
tamaño del conjunto mientras se mantiene o mejora el rendimiento del modelo de aprendizaje profundo.

\subsection{Descripción}\label{subsec:descripcion3}
El funcionamiento de los algoritmos genéticos se basa en los conceptos de \textbf{selección natural},
\textbf{cruzamiento} y \textbf{mutación}.
El proceso se puede resumir en los siguientes pasos:
\begin{enumerate}
      \item \textbf{Inicialización}: Se genera una población inicial de posibles soluciones, cada una de ellas
            representando un subconjunto del conjunto de datos.
      \item \textbf{Evaluación}: Cada subconjunto de datos (o `individuo') es evaluado entrenando el modelo con ese
            subconjunto y midiendo su rendimiento.
      \item \textbf{Selección}: Se seleccionan los mejores individuos de la población basándose en su rendimiento.
            Los mejores individuos tienen más probabilidades de ser seleccionados para la siguiente generación.
      \item \textbf{Cruzamiento}: Se combinan pares de individuos seleccionados para crear nuevos subconjuntos.
            Esto se realiza intercambiando ejemplos entre los subconjuntos.
      \item \textbf{Mutación}: Con una pequeña probabilidad, se realizan cambios aleatorios en algunos individuos, como
            añadir o eliminar ejemplos del subconjunto.
      \item \textbf{Iteración}: El proceso de evaluación, selección, cruzamiento y mutación se repite durante varias
            generaciones, con la esperanza de que cada generación produzca soluciones mejores que la anterior.
\end{enumerate}

\subsection{Aplicación en la reducción de datos}\label{subsec:aplicacion-en-la-reduccion-de-datos3}
Los \textbf{algoritmos genéticos} son especialmente adecuados para la reducción de datos porque permiten explorar un
espacio de soluciones muy amplio de manera eficiente.
La combinación de individuos y la introducción de mutaciones aleatorias permiten al algoritmo escapar de los óptimos
locales, un problema común en la búsqueda local.

El uso de algoritmos genéticos para reducir datos en este contexto implica encontrar subconjuntos de entrenamiento que
proporcionen un buen equilibrio entre tamaño y rendimiento.
Esto se logra al evaluar diferentes subconjuntos y mejorar las soluciones generación tras generación.

\subsection{Ventajas y limitaciones}\label{subsec:ventajas-y-limitaciones2}
\textbf{Ventajas}: Los algoritmos genéticos son efectivos para explorar grandes espacios de soluciones y tienen una
gran capacidad para evitar quedar atrapados en óptimos locales.
Son especialmente útiles en problemas donde la solución óptima no es evidente desde el principio.


\textbf{Limitaciones}: Estos algoritmos pueden ser costosos computacionalmente, ya que requieren evaluar muchas
soluciones a lo largo de múltiples generaciones.
Además, su convergencia a veces puede ser lenta, dependiendo del tamaño del espacio de búsqueda y de los
parámetros del algoritmo (tamaño de la población, tasa de mutación, etc.).

\subsection{Versiones}\label{subsec:versiones}
A lo largo del desarrollo del proyecto, se implementaron diversas versiones del algoritmo genético con el fin de
mejorar su rendimiento, adaptabilidad y capacidad para escapar de óptimos locales.

\subsubsection{Versión 1 (Genético clásico)}
Esta primera versión emplea un operador de cruce simple que intercambia subconjuntos de imágenes seleccionadas entre dos padres.
La selección se realiza mediante torneos y se aplica una tasa de mutación fija.
El elitismo garantiza la preservación del mejor individuo.
Aunque funcional, esta versión presentaba ciertas limitaciones en la calidad de los hijos generados,
especialmente en fases avanzadas de la evolución.

\subsubsection{Versión 2 (Genético con cruce ponderado)}
En esta versión se introdujo un operador de cruce mejorado (\textit{weighted\_crossover}), que asigna más imágenes al hijo
desde el progenitor con mejor fitness, favoreciendo la herencia de características exitosas.
Además, se selecciona solo el mejor de los hijos generados en cada cruce, de esta forma se reduce la propagación de malas soluciones.

\subsubsection{Versión 2 libre (ajuste dinámico de tamaño)}
Se trata de una variante de la versión 2 donde se permite que el número de imágenes seleccionadas por los hijos varíe
dentro de un rango flexible (entre el 50\% y el 150\% del objetivo inicial).
Esto se logra mediante el parámetro \textit{adjust\_size}, lo que introduce mayor diversidad en la población y fomenta la exploración del espacio de búsqueda.

\subsubsection{Versión 3 (Genético con reinicio poblacional)}
Esta versión añade un mecanismo de reinicio que se activa si el segundo mejor individuo no mejora en dos generaciones consecutivas.
En ese caso, se mantiene el mejor individuo y se reemplaza el resto de la población,
generando así nuevos candidatos aleatorios y dando la posibilidad de escapar de estancamientos evolutivos.


\section{Algoritmo Memético}\label{sec:algoritmo-memetico}
El \textbf{algoritmo memético} es una extensión del enfoque genético tradicional que incorpora técnicas de búsqueda
local dentro del proceso evolutivo.
Su objetivo es combinar la exploración global del espacio de soluciones (propia de los algoritmos evolutivos)
con la explotación local de buenas soluciones (propia de la optimización heurística).
En este trabajo, se implementó una versión adaptada del algoritmo memético orientada a la selección de subconjuntos
óptimos de imágenes para el entrenamiento de modelos convolucionales.


\subsection{Descripción}\label{subsec:descripcion4}
El funcionamiento general del algoritmo memético se basa en una estructura genética clásica:
generación de población inicial, selección por torneo, cruce, mutación y reemplazo elitista.
Sin embargo, introduce una novedad clave: la aplicación probabilística de una búsqueda local sobre los individuos recién generados.
Este procedimiento local intenta mejorar los hijos generados por cruce, evaluando soluciones vecinas
mediante pequeñas modificaciones (mutaciones controladas).


En concreto, el algoritmo:
\begin{enumerate}
      \item Aplica cruce y mutación para generar dos hijos por pareja de padres.
      \item Con una cierta probabilidad, aplica búsqueda local a uno de los hijos.
      \item Evalúa múltiples vecinos durante un número limitado de evaluaciones (\textit{local\_search\_evaluations}), seleccionando el mejor encontrado.
      \item Incorpora elitismo para asegurar que el mejor individuo no se pierde.
\end{enumerate}


La búsqueda local está limitada por el número total de evaluaciones permitidas y permite realizar mejoras incrementales únicamente cuando resultan beneficiosas.


\subsection{Aplicación en la reducción de datos}\label{subsec:aplicacion-en-la-reduccion-de-datos4}
El \textbf{algoritmo memético} fue diseñado para encontrar subconjuntos de datos que optimicen métricas
como la \textit{accuracy} o el \textit{F1-score} en un número reducido de evaluaciones.
Su capacidad para ajustar localmente los subconjuntos permite afinar las selecciones iniciales generadas
por los operadores evolutivos, lo que se traduce en soluciones de mayor calidad con menor dispersión.

\subsection{Ventajas y limitaciones}\label{subsec:ventajas-y-limitaciones3}
\textbf{Ventajas}:
\begin{itemize}
      \item Mejora puntual de soluciones mediante refinamiento local.
      \item Reduce la probabilidad de estancamiento en óptimos locales.
      \item Genera soluciones más consistentes y robustas frente a la aleatoriedad.
\end{itemize}

\textbf{Limitaciones}:
\begin{itemize}
      \item Aumenta ligeramente el coste computacional por las evaluaciones adicionales de la búsqueda local.
      \item Requiere calibración adicional de parámetros como la probabilidad de búsqueda local o el tamaño del vecindario.
\end{itemize}
