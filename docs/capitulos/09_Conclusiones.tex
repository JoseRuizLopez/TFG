% !TeX root = ../proyecto.tex

\chapter{Conclusiones}\label{ch:conclusiones}
Este Trabajo de Fin de Grado aborda el problema de la reducción de conjuntos de datos en el contexto del aprendizaje profundo,
proponiendo el uso de algoritmos meméticos como técnica de selección de instancias.
A lo largo del desarrollo se diseñan, implementan y evalúan diferentes enfoques evolutivos (genéticos y meméticos),
con el objetivo de identificar subconjuntos representativos de imágenes que permitan entrenar modelos convolucionales de forma eficiente y sin pérdida significativa de rendimiento.

El estudio se centra en evaluar cómo diferentes variantes algorítmicas impactan sobre la precisión, la estabilidad y el tamaño final del conjunto de entrenamiento.
Para ello, se utilizan dos modelos de referencia ampliamente extendidos (\texttt{ResNet50} y \texttt{MobileNetV2}) y tres conjuntos de datos con distintos niveles de complejidad: `\texttt{Rock, Paper, Scissors}', `\texttt{PAINTING}' y `\texttt{CIFAR-10}'.

Los resultados experimentales permiten extraer varias conclusiones clave:

\begin{itemize}
      \item El uso de algoritmos evolutivos mejora notablemente la calidad de los subconjuntos generados en comparación con estrategias aleatorias o deterministas.
            Incluso las versiones básicas, como el algoritmo genético clásico, superan con claridad al enfoque aleatorio.

      \item Las mejoras introducidas en los operadores de cruce y mutación (ponderación basada en fitness y mutación adaptativa)
            incrementan la estabilidad y precisión de los resultados, especialmente en escenarios con porcentajes bajos de datos iniciales.

      \item La inclusión del reinicio poblacional ofrece cierto beneficio en términos de diversidad,
            aunque no aporta mejoras significativas respecto a las versiones mejoradas del genético en cuanto a precisión media.

      \item Las versiones \textit{libres} de los algoritmos, donde el tamaño del subconjunto se ajusta dinámicamente,
            permiten alcanzar un equilibrio más eficaz entre precisión y tamaño final.
            Este enfoque demuestra ser especialmente útil cuando no se conoce a priori la porcentaje óptima de datos para entrenar.

      \item El algoritmo memético, y en particular su versión libre (MA-F), alcanza los mejores resultados globales.
            Este enfoque logra una elevada precisión, una notable reducción del conjunto de datos y una gran estabilidad entre ejecuciones,
            consolidándose como la solución más eficaz del estudio.
\end{itemize}


La validación con un segundo conjunto de datos (`\texttt{PAINTING}'), más complejo y con mayor número de clases, y con un tercer conjunto (`\texttt{CIFAR-10}'),
considerablemente más exigente por su variabilidad intraclase y baja resolución, permite comprobar la capacidad de generalización de los enfoques propuestos.
El algoritmo memético libre no solo mantiene su rendimiento, sino que en algunos casos supera a la precisión obtenida con el 100\% del conjunto completo.
Además, se confirma que las soluciones generadas preservan una distribución equilibrada de clases y ajustan el tamaño del subconjunto según la dificultad del problema.


Los resultados obtenidos tienen implicaciones prácticas significativas.
Queda demostrado que es posible reducir de forma significativa los conjuntos de datos para entrenar modelos convolucionales sin comprometer el rendimiento,
siempre que se utilicen estrategias de selección guiadas y adaptativas.
Aunque la selección de instancias conlleva un coste computacional adicional durante su ejecución, 
este esfuerzo inicial se justifica al permitir una notable reducción en el tiempo de entrenamiento e inferencia de los modelos explorados.
Además, facilita la auditoría y trazabilidad de los datos, aspectos fundamentales en entornos donde estas exigencias normativas son prioritarias.
A medio plazo, el uso de subconjuntos optimizados puede reducir el coste global cuando se entrenan múltiples modelos o configuraciones diferentes.

Como líneas futuras, se plantea:
\begin{itemize}
      \item Aplicar las técnicas propuestas en tareas más complejas, como la segmentación semántica o la clasificación multietiqueta,
            donde la reducción del conjunto de datos puede tener un impacto aún más relevante.
      \item Incorporar múltiples objetivos en el proceso de selección, convirtiéndolo en un problema de optimización multiobjetivo.
            Por ejemplo, se podría considerar aspectos como el tiempo de entrenamiento, el coste computacional total o la eficiencia en la fase de inferencia,
            además de las métricas clásicas de rendimiento.
      \item Explorar la integración de los algoritmos meméticos con otras estrategias de reducción de datos, como la destilación de conocimiento o la poda de redes neuronales,
            con el objetivo de combinar distintas aproximaciones complementarias.
\end{itemize}

En resumen, los hallazgos de este estudio ponen de manifiesto el potencial de los algoritmos meméticos en la reducción eficiente de conjuntos de datos para aprendizaje profundo.
Este trabajo demuestra que la combinación de técnicas evolutivas y búsqueda local, aplicada de forma flexible y adaptativa,
constituye una herramienta poderosa para optimizar el entrenamiento de modelos de aprendizaje profundo.
Más allá de la mejora en precisión o eficiencia, esta línea de investigación apunta hacia un aprendizaje más sostenible, trazable y accesible,
en línea con las necesidades actuales de la inteligencia artificial moderna.

Desde un punto de vista personal, este proyecto ha representado un desafío enriquecedor tanto a nivel técnico como organizativo.
La implementación completa de los algoritmos, la gestión eficiente del entorno experimental y
la interpretación rigurosa de los resultados me han permitido consolidar conocimientos clave en aprendizaje automático y optimización.
Asimismo, la dificultad de compaginar este trabajo con responsabilidades laborales ha reforzado mi compromiso con el rigor y la constancia,
valores que considero fundamentales en mi futuro profesional.