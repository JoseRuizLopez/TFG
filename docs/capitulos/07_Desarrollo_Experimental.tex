% !TeX root = ../proyecto.tex

\chapter{Desarrollo Experimental}\label{ch:desarrollo-experimental}
En este capítulo se exponen los experimentos realizados, los ajustes implementados en los algoritmos y los resultados
obtenidos en los distintos escenarios evaluados.
El objetivo principal fue analizar el rendimiento de los modelos entrenados con conjuntos de datos reducidos,
seleccionados mediante algoritmos meméticos y evolutivos.

\section{Datasets utilizados}\label{sec:datasets}
En el aprendizaje profundo, los datasets son colecciones de datos etiquetados o no etiquetados que se utilizan para
entrenar modelos.
Estos conjuntos de datos contienen ejemplos organizados que representan la entrada para el modelo y, en muchos casos,
también las etiquetas correspondientes que indican la salida deseada.
Los datasets varían en tamaño, calidad y tipo, dependiendo de la tarea a resolver, como la clasificación de imágenes,
el reconocimiento de patrones o la predicción de series temporales.


A continuación, se van a explicar cada uno de los Datasets que se han utilizado en el desarrollo del proyecto.

\newpage

\subsection{Rock, Paper, Scissors (Piedra, Papel, Tijera)}\label{subsec:rock-paper-scissors}
\textbf{Rock, Paper, Scissors}~\cite{RockPaperScissors} es un conjunto de datos creado por Laurence Moroney
que se utiliza para la clasificación de imágenes de manos representando los gestos de `piedra', `papel' y `tijeras'.

\begin{figure}[H]
    \centering
    \begin{subfigure}[t]{0.3\textwidth}
        \centering
        \includegraphics[width=\linewidth]{imagenes/dataset_examples/rock.jpg}
        \caption*{Rock}
    \end{subfigure}
    \begin{subfigure}[t]{0.3\textwidth}
        \centering
        \includegraphics[width=\linewidth]{imagenes/dataset_examples/paper.jpg}
        \caption*{Paper}
    \end{subfigure}
    \begin{subfigure}[t]{0.3\textwidth}
        \centering
        \includegraphics[width=\linewidth]{imagenes/dataset_examples/scissors.jpg}
        \caption*{Scissors}
    \end{subfigure}
    \caption{Ejemplos de imágenes del dataset Rock, Paper, Scissors}
    \label{fig:ejemplos-rps}
\end{figure}

En la figura~\ref{fig:ejemplos-rps} se han mostrado una imagen de cada una de las clases del dataset Rock, Paper, Scissors,
para que se pueda observar la similitud entre las distintas clases.

\subsubsection{Estructura del Dataset}
El conjunto de datos contiene aproximadamente 2,500 imágenes, distribuidas en tres categorías: piedra, papel y tijeras.
Las imágenes están en color y tienen un tamaño de 300x300 píxeles.

\begin{figure}[ht]
    \centering
    \begin{forest}mydirstyle
        [RPS
            [train
                    [rock
                            [image1.jpg]
                            [image2.jpg]
                            [\dots]
                    ]
                    [paper
                            [image1.jpg]
                            [image2.jpg]
                            [\dots]
                    ]
                    [scissors
                            [image1.jpg]
                            [image2.jpg]
                            [\dots]
                    ]
            ]
            [test (originalmente valid)
                [rock]
                    [paper]
                    [scissors]
            ]
            [valid (originalmente test)
                [rock]
                    [paper]
                    [scissors]
            ]
        ]
    \end{forest}
    \caption{Estructura de carpetas del dataset Rock, Paper, Scissors}
    \label{fig:estructura-rps}
\end{figure}


Como se puede observar en la Figura~\ref{fig:estructura-rps}, las imágenes están organizadas en directorios según su función en el entrenamiento
(entrenamiento, validación o prueba) y, dentro de cada partición, se dividen a su vez por clases del dataset.

\subsubsection{Formato de los Datos}
Las imágenes están en formato JPEG (\texttt{.jpg}). Para su procesamiento, se han aplicado técnicas de preprocesamiento
adaptadas a los requerimientos del modelo.

\subsubsection{Uso del Dataset}
Este dataset se ha utilizado para evaluar el rendimiento del modelo en un problema de clasificación de imágenes con
múltiples clases, pero siendo un dataset sencillo y con un número de clases pequeño.
Además, permite explorar la eficacia de los algoritmos meméticos en un entorno más cercano al reconocimiento de objetos.

\subsubsection{Correcciones en la División de Datos}
Según la nota observada en el README del dataset:
\begin{quote}
    \textit{Note: in the source, Laurence calls ``validation'' as the ``test'', and ``test'' the ``validation''.}
\end{quote}
se han renombrado las particiones de \texttt{test} y \texttt{valid} para que correspondan correctamente con sus
propósitos.

\subsubsection{Licencia y uso}
Este conjunto de datos se distribuye bajo la licencia
\textbf{Creative Commons Attribution 4.0 International (CC BY 4.0)}, lo que permite su uso, modificación y distribución
con la condición de otorgar el crédito adecuado a los creadores originales~\cite{moroneyLaurenceMoroneyAI}.


\subsection{PAINTING (Art Images: Drawing/Painting/Sculptures/Engravings)}\label{subsec:painting}
El dataset \textbf{Art Images: Drawing/Painting/Sculptures/Engravings} es una colección de aproximadamente 9,000
imágenes organizadas en cinco categorías de arte: dibujos, pinturas, esculturas, grabados y arte iconográfico.

\begin{figure}[H]
    \centering
    \begin{subfigure}[t]{0.3\textwidth}
        \centering
        \includegraphics[width=\linewidth]{imagenes/dataset_examples/drawings.jpg}
        \caption*{Drawings}
    \end{subfigure}
    \begin{subfigure}[t]{0.3\textwidth}
        \centering
        \includegraphics[width=\linewidth]{imagenes/dataset_examples/painting.jpg}
        \caption*{Painting}
    \end{subfigure}
    \begin{subfigure}[t]{0.3\textwidth}
        \centering
        \includegraphics[width=\linewidth]{imagenes/dataset_examples/iconography.jpg}
        \caption*{Iconography}
    \end{subfigure}
    \begin{subfigure}[t]{0.3\textwidth}
        \centering
        \includegraphics[width=\linewidth]{imagenes/dataset_examples/engraving.jpg}
        \caption*{Engraving}
    \end{subfigure}
    \begin{subfigure}[t]{0.3\textwidth}
        \centering
        \includegraphics[width=\linewidth]{imagenes/dataset_examples/sculpture.jpg}
        \caption*{Sculpture}
    \end{subfigure}
    \caption{Ejemplos de clases en el dataset PAINTING}
    \label{fig:ejemplos-painting}
\end{figure}

En la figura~\ref{fig:ejemplos-painting} se han mostrado una imagen de cada una de las clases del dataset PAINTING,
para que se pueda observar la variabilidad de las imágenes.

\subsubsection{Estructura del Dataset}
\begin{figure}[ht]
    \centering
    \begin{forest}mydirstyle
        [Dataset
            [Train (originalmente training\_set)
                [drawings
                            [image1.jpg]
                            [image2.jpg]
                            [\dots]
                    ]
                    [paintings
                            [image1.jpg]
                            [image2.jpg]
                            [\dots]
                    ]
                    [sculptures]
                    [engravings]
                    [iconography]
            ]
            [Test (originalmente validation\_set)
                [drawings]
                    [paintings]
                    [sculptures]
                    [engravings]
                    [iconography]
            ]
        ]
    \end{forest}
    \caption{Estructura de carpetas del dataset PAINTING}
    \label{fig:estructura-painting}
\end{figure}

Como se puede observar en la Figura~\ref{fig:estructura-painting}, las imágenes están organizadas en directorios según su categoría artística,
que en este caso corresponden a las distintas clases del dataset, previamente divididas en conjuntos de entrenamiento y prueba.

\subsubsection{Formato de los Datos}
Todas las imágenes están en formato JPEG (\texttt{.jpg}) y presentan variaciones en resolución y dimensiones.
Se han aplicado técnicas de preprocesamiento para homogenizar las características de las imágenes.

\subsubsection{Uso del Dataset}
Este dataset se ha utilizado para entrenar y evaluar modelos de clasificación de imágenes en un entorno diferente al
RPS\@.
Con este dataset, se ha comprobado el funcionamiento para evaluar los algoritmos con un dataset un poco mas complejo
que el RPS, con un par de clases más y con un número mayor de imágenes.

\subsubsection{Correcciones en la División de Datos}
Observando los tamaños de la división de los datos, y teniendo en cuenta que la divisón de los datos suele ser en train
y test, se ha decidido por renombrar las particiones de \texttt{valid} por \texttt{test} para que corresponda
correctamente con su propósito.
Y el set de validation lo he obtenido separando el set de train, normalmente haciendo una división 80\% test y 20\%
valid.

\subsubsection{Acceso al Dataset}
Inicialmente, el dataset se descargó desde Kaggle~\cite{OriginalArtImages}

Sin embargo, debido a la presencia de archivos innecesarios y algunas imágenes corruptas, se optó por una versión
limpia disponible en Kaggle~\cite{CleanedArtImages}.

\subsubsection{Licencia y Uso}
Antes de su uso, se revisaron los términos y condiciones establecidos en la página de Kaggle para asegurar el
cumplimiento con las licencias y restricciones aplicables.

% \subsection{MNIST (Modified National Institute of Standards and Technology)}\label{subsec:mnist}
% \textbf{MNIST}~\cite{MNISTDataset} es un dataset ampliamente utilizado en aprendizaje profundo.
% Contiene 70,000 imágenes de dígitos escritos a mano, divididas en 60,000 imágenes para el entrenamiento y 10,000 para
% la prueba.

% \subsubsection{Estructura del Dataset}
% Las imágenes tienen un tamaño de 28x28 píxeles y están en escala de grises, con valores de intensidad entre 0 (negro) y
% 255 (blanco).

% \subsubsection{Formato de los Datos}
% Las imágenes están almacenadas en formato IDX, un formato binario específico para este dataset.
% Se ha realizado una conversión a matrices NumPy para su procesamiento eficiente.

% \subsubsection{Uso del Dataset}
% Este conjunto de datos se ha empleado como benchmark para evaluar modelos de clasificación de imágenes, especialmente
% en arquitecturas convolucionales.

% \subsubsection{Licencia y uso}
% El dataset MNIST se distribuye bajo una licencia de dominio público.
% Fue creado a partir de los datos originales del NIST y está disponible en diversas plataformas, incluyendo la página
% oficial de Yann LeCun.


\subsection{Comparación entre datasets}\label{subsec:comparacion-entre-datasets}
%La selección de estos dos datasets responde a la necesidad de evaluar los algoritmos meméticos en distintos niveles de
%complejidad.
A continuación, se muestra una tabla resumen con las características más relevantes de los datasets utilizados.
Esta comparación permite entender mejor la complejidad relativa de cada conjunto y cómo pueden influir en el comportamiento de los algoritmos:

% \begin{table}[H]
%     \centering
%     \begin{tabular}{|l|c|c|c|c|}
%         \hline
%         \textbf{Dataset}      & \textbf{Nº Imágenes} & \textbf{Nº Clases} & \textbf{Formato} & \textbf{Tamaño Imagen} \\
%         \hline
%         Rock, Paper, Scissors & \textasciitilde2.500 & 3                  & JPG              & 300$*$300 px           \\
%         PAINTING              & \textasciitilde9.000 & 5                  & JPG              & Variable               \\
% MNIST                 & 70.000               & 10 & IDX/PNG & 28×28 px (grayscale) \\
%         \hline
%     \end{tabular}
%     \caption{Resumen comparativo de los datasets utilizados}
%     \label{tab:resumen-datasets}
% \end{table}
\begin{table}[H]
    \centering
    \resizebox{\textwidth}{!}{
        \begin{tabular}{|l|c|c|c|c|}
            \hline
            \textbf{Dataset}      & \textbf{Nº Imágenes} & \textbf{Nº Clases} & \textbf{Formato} & \textbf{Tamaño Imagen} \\
            \hline
            Rock, Paper, Scissors & \textasciitilde2.500 & 3                  & JPG              & 300×300 px             \\
            PAINTING              & \textasciitilde9.000 & 5                  & JPG              & Variable               \\
            \hline
        \end{tabular}
    }
    \caption{Resumen comparativo de los datasets utilizados}
    \label{tab:resumen-datasets}
\end{table}

El dataset \textbf{Rock, Paper, Scissors} se utilizó en las primeras fases del proyecto como punto de partida,
ya que ofrecía un entorno sencillo y controlado, con un número reducido de clases y una estructura equilibrada.
Esto permitió desarrollar las bases del sistema y probar las primeras versiones de los algoritmos de manera más ágil y con menor complejidad computacional.

Por su parte, el dataset \textbf{PAINTING} se empleó posteriormente para validar el comportamiento de los algoritmos en un entorno más exigente.
Al incluir cinco categorías de arte con distintos estilos visuales, este conjunto introdujo una mayor variabilidad tanto semántica como estructural,
lo que permitió evaluar la robustez y capacidad de generalización de las soluciones desarrolladas.


\section{Diseño de los experimentos}\label{sec:diseño-de-los-experimentos}
La fase experimental se organizó en varias etapas.
Inicialmente se optó por un dataset simple (\textit{Rock, Paper, Scissors}) para validar el funcionamiento general del sistema.
Posteriormente, se realizaron pruebas con datasets más exigentes.
Los experimentos se repitieron utilizando diferentes porcentajes iniciales de datos (10\%, 25\%, 50\% y 75\%)
para estudiar cómo afecta la cantidad de datos seleccionados al rendimiento del modelo.

Con el fin de asegurar la consistencia entre ejecuciones experimentales, se aplicaron las medidas de control de reproducibilidad
descritas en el \hyperref[sec:consideraciones-de-optimizacion]{apartado~\ref*{sec:consideraciones-de-optimizacion}}.
Esto permitió comparar algoritmos en condiciones homogéneas, evitando variaciones indeseadas causadas por componentes aleatorios del entorno de ejecución.

En cada prueba, se realizaron 5 ejecuciones en paralelo, cada una utilizando una semilla distinta.
Esta estrategia permitió obtener resultados promedio más robustos frente a la aleatoriedad del proceso evolutivo, asegurando una mayor fiabilidad estadística.

Los apartados siguientes explican con mayor detalle el procedimiento adoptado para llevar a cabo dichas ejecuciones.


\section{Procedimiento de Ejecución y Evaluación}\label{sec:procedimiento-de-ejecucion-y-evaluacion}

Para garantizar la consistencia y objetividad en la comparación entre algoritmos, se diseñó un procedimiento experimental sistemático y replicable.
Cada ejecución se realizó bajo las mismas condiciones computacionales y utilizando los mismos parámetros base,
salvo en aquellos casos en que se deseaba estudiar una variación concreta, como los distintos porcentajes iniciales o el uso de metaheurísticas con porcentajes libres.


\subsection{Métricas de Evaluación}\label{sec:metricas-de-evaluacion}
Para evaluar el rendimiento de los modelos se utilizaron métricas estándar como \textbf{accuracy}, \textbf{precisión}, \textbf{recall} y \textbf{F1-score},
calculadas sobre el conjunto de validación tras cada evaluación.
Para una definición formal de estas métricas, véase el \hyperref[sec:metricas-evaluacion]{apartado~\ref*{sec:metricas-evaluacion}}.

Estas métricas fueron calculadas utilizando las funciones de \texttt{scikit-learn}, a partir de las predicciones del modelo y las etiquetas
reales correspondientes a los subconjuntos de imágenes seleccionados por cada algoritmo.

\subsection{Evaluaciones por Ejecución}\label{sec:evaluaciones-por-ejecucion}
Se buscó un equilibrio entre la cantidad de evaluaciones y el tiempo de ejecución, permitiendo una exploración suficiente del espacio de soluciones sin comprometer la eficiencia computacional.
Por ello, cada algoritmo fue configurado para realizar un máximo de 100 evaluaciones por ejecución,
independientemente del tipo de algoritmo utilizado, con el fin de mantener la equidad comparativa.

Cada evaluación consistía en generar un subconjunto de datos, entrenar el modelo correspondiente (ResNet50 o MobileNetV2),
y calcular su \textit{fitness} de acuerdo con las métricas mencionadas.

El número de evaluaciones sin mejora también fue monitorizado para aplicar criterios de parada anticipada,
explicados previamente en el \hyperref[sec:consideraciones-de-optimizacion]{apartado~\ref*{sec:consideraciones-de-optimizacion}},
reduciendo así el tiempo computacional en caso de estancamiento.

\subsection{Repeticiones y Semillas}\label{sec:repeticiones-y-semillas}
Con el objetivo de obtener resultados estadísticamente significativos y reducir el efecto de la aleatoriedad,
cada configuración experimental fue ejecutada 5 veces, utilizando 5 semillas distintas.
Los resultados presentados en las tablas y gráficos corresponden a la media de esas ejecuciones,
junto con medidas de dispersión cuando procede, como los boxplots.

Cabe destacar que, en el caso de los boxplots, en lugar de representar la media de las 5 ejecuciones por configuración, 
se optó por incluir todos los valores individuales obtenidos con las distintas semillas. 
Esta decisión permite visualizar una distribución más realista del comportamiento de cada algoritmo, 
resaltando mejor la mediana, así como los valores máximos y mínimos alcanzados durante las ejecuciones.


\subsection{Tiempos de Ejecución}\label{sec:tiempos-de-ejecucion}
Cada evaluación implicaba entrenar un modelo desde cero, por lo que los tiempos de ejecución fueron considerables.
Por ejemplo, una ejecución completa con 100 evaluaciones podía tardar entre 30 minutos y 2 horas,
dependiendo del algoritmo y del modelo utilizado.

Los algoritmos más complejos, como el memético o las versiones con reinicio poblacional,
requerían un mayor tiempo de ejecución debido a las operaciones adicionales de mejora local o regeneración de población.


\section{Evaluación con modelos base}\label{sec:evaluacion-con-modelos-base}
Los primeros experimentos se realizaron con los algoritmos aleatorios, tanto en ResNet50 como en MobileNet, como línea base comparativa.
En ambos modelos, se observó una mejora gradual de las métricas al aumentar el porcentaje de datos utilizados.
Como era de esperar, ResNet50 ofreció mejores resultados en cuanto a precisión, pero a costa de un mayor tiempo de entrenamiento.
MobileNet, en cambio, ofreció una alternativa más rápida y eficiente, aunque con una leve pérdida de rendimiento.

\begin{table}[htp]
    \centering
    \resizebox{\textwidth}{!}{
        \begin{tabular}{lp{2cm}lp{2cm}p{2cm}p{2cm}p{2cm}p{2.2cm}}
            \toprule
            \textbf{Algoritmo}       & \textbf{Porcentaje Inicial} & \textbf{Duración}       & \textbf{Accuracy (Avg)} &
            \textbf{Precision (Avg)} & \textbf{Recall (Avg)}       & \textbf{F1-score (Avg)} &
            \textbf{Evaluaciones Realizadas}                                                                                                               \\
            \midrule
            \multicolumn{8}{l}{\textbf{Modelo ResNet50}}                                                                                                   \\
            \midrule
            aleatorio                & 10                          & 00:45:08                & 76,55\%                 & 81,80\% & 76,55\% & 76,25\% & 100 \\
            aleatorio                & 20                          & 01:10:27                & 81,77\%                 & 84,70\% & 81,77\% & 81,59\% & 100 \\
            aleatorio                & 50                          & 02:24:49                & 87,14\%                 & 88,09\% & 87,14\% & 86,97\% & 100 \\
            aleatorio                & 100                         & 00:02:42                & 87,90\%                 & 88,96\% & 87,90\% & 87,81\% & 1   \\
            \midrule
            \multicolumn{8}{l}{\textbf{Modelo MobileNet}}                                                                                                  \\
            \midrule
            aleatorio                & 10                          & 00:29:29                & 72,31\%                 & 76,40\% & 72,31\% & 69,62\% & 100 \\
            aleatorio                & 20                          & 00:50:36                & 76,48\%                 & 78,82\% & 76,48\% & 75,58\% & 100 \\
            aleatorio                & 50                          & 01:54:09                & 75,56\%                 & 79,72\% & 75,56\% & 74,67\% & 100 \\
            aleatorio                & 100                         & 00:02:12                & 76,08\%                 & 79,97\% & 76,08\% & 75,61\% & 1   \\
            \bottomrule
        \end{tabular}
    }
    \caption{Comparativa de resultados de la generación inicial utilizando el algoritmo \textbf{aleatorio} con los modelos \textbf{ResNet50} y \textbf{MobileNet}.}
    \label{tab:initial-generation-comparison}
\end{table}


En las Tabla~\ref{tab:initial-generation-comparison} se muestran los resultados obtenidos para diferentes porcentajes iniciales de datos con ambos modelos.
Además, en la Figura X se incluye un boxplot donde se comparan los valores de accuracy por porcentaje de datos y por algoritmo.

\section{Comparativa entre Algoritmos}\label{sec:comparativa-entre-algoritmos}
A lo largo del desarrollo del proyecto, se diseñaron y ajustaron diversas versiones de algoritmos evolutivos y meméticos para seleccionar subconjuntos representativos de imágenes. Esta sección presenta una comparativa entre ellos, no solo desde una perspectiva cuantitativa (accuracy, precisión, etc.), sino también cualitativa, incorporando los aprendizajes progresivos que guiaron su evolución.

\subsection{Iteración inicial: comparación base}\label{sec:iteracion-inicial-comparacion-base}
La primera comparativa se centró en tres enfoques principales:

\begin{itemize}
    \item \textbf{Algoritmo aleatorio}, como línea base sin criterio de selección.
    \item \textbf{Búsqueda local}, ajustando iterativamente el conjunto seleccionado.
    \item \textbf{Algoritmo genético (v1)}, combinando subconjuntos mediante operadores clásicos de cruce y mutación.
    \item \textbf{Algoritmo memético}, que combinaba el genético con un proceso de mejora local adicional.
\end{itemize}

\begin{figure}[!h]
    \centering
    \includegraphics[width=0.95\textwidth]{imagenes/mobilenet-BOXPLOT-generacion-inicial}
    \caption{Boxplot comparando el \textit{accuracy} alcanzado por cada algoritmo de los iniciales.}
    \label{fig:boxplot-generacion-inicial}
\end{figure}

El gráfico de la Figura~\ref{fig:boxplot-generacion-inicial} ofrece una visión comparativa del rendimiento de los algoritmos evaluados en la etapa inicial del proyecto.
Se observa que el algoritmo aleatorio, como era de esperar, presenta una alta dispersión y una mediana de accuracy relativamente baja.
Este comportamiento se justifica por la ausencia de una estrategia que guíe la selección de datos, lo que da lugar a conjuntos de entrenamiento inconsistentes.


Por su parte, la búsqueda local mejora notablemente la mediana de accuracy, manteniendo una menor variabilidad que el enfoque aleatorio.
Esto indica que, pese a su simplicidad, la exploración guiada del espacio de soluciones permite alcanzar resultados más estables y competitivos.


El algoritmo genético (v1) representa un salto adicional en rendimiento.
La combinación de operadores evolutivos como la selección, el cruce y la mutación genera soluciones más refinadas,
lo que se traduce en una mediana superior y en una mayor compactación de los valores en torno a ella.


Finalmente, el algoritmo memético destaca como el más eficiente dentro de esta primera iteración.
Al incorporar una fase de mejora local sobre la evolución genética, logra afinar aún más las soluciones obtenidas.
Este enfoque híbrido permite reducir la dispersión y minimizar la probabilidad de obtener soluciones poco efectivas.


\subsection{Mejoras progresivas: evolución del algoritmo genético}\label{sec:mejoras-progresivas-evolucion-algoritmo-genetico}
Aunque el algoritmo memético alcanzó el mejor resultado puntual en la comparativa inicial, se decidió continuar el desarrollo
sobre el algoritmo genético por varias razones estratégicas.


En primer lugar, el análisis detallado mostró que el genético también presentaba una mediana de accuracy más alta,
lo que sugiere un rendimiento más consistente.
Además, incluso en sus ejecuciones menos favorables, el genético mostró mejores valores mínimos que el memético,
lo cual evidencia una mayor robustez frente a escenarios menos óptimos.


A nivel práctico, el algoritmo genético ofrecía una estructura más simple y modular, lo que facilitó la introducción progresiva de mejoras específicas.
Esta simplicidad también implicaba una menor carga computacional al prescindir de la búsqueda local en cada iteración,
permitiendo realizar pruebas más rápidas y escalables.
Por estos motivos, se consideró que centrar los esfuerzos en afinar el genético aportaría beneficios significativos
tanto en eficiencia como en estabilidad, sin renunciar a un rendimiento competitivo.


Por lo tanto, se optó por implementar varias versiones del algoritmo genético, cada una con mejoras específicas:


\begin{itemize}
    \item \textbf{Versión 2 (Genético v2)}: se introdujo un cruce ponderado que daba más peso al progenitor con mejor fitness,
          y se seleccionaba únicamente el mejor de los dos hijos generado.
          Esto evitaba la propagación de soluciones poco efectivas y mejoraba la velocidad de convergencia.
    \item \textbf{Versión 3 (Genético v3)}: se añadió una lógica de reinicio poblacional. Si después de varias generaciones
          no se observaba mejora, se mantenía el mejor individuo y se regeneraban aleatoriamente los demás.
          Esta estrategia ayudó a salir de óptimos locales sin reiniciar todo el proceso.
          % \item \textbf{Versión 4 (Genético v4)} [prototipo]: se exploró la posibilidad de conservar un histórico de las mejores
          %       soluciones y reutilizarlas en futuras generaciones como una forma de memoria evolutiva.
\end{itemize}

Estas versiones se probaron bajo las mismas condiciones que los algoritmos iniciales, lo que permitió compararlos en igualdad de condiciones.


Durante el desarrollo de la Versión 2, se evaluaron distintas estrategias para el operador de mutación.
Inicialmente, se aplicaba una regla fija que realizaba un 10\% de permutaciones sobre el subconjunto seleccionado:
\[
    \mathrm{num\_swaps} = \max\left(1,\ \mathrm{length} \times 0.1\right)
\]
Sin embargo, esta estrategia podía resultar demasiado conservadora o agresiva, según el tamaño de la solución.
En la versión final se adoptó una fórmula más flexible que adapta la magnitud de la mutación en función tanto del subconjunto mutado como del original:
\[
    \mathrm{num\_swaps} = \min\left(\mathrm{length} \times 0.15,\ \mathrm{length} \times 0.8\right)
\]
Esta modificación permite un equilibrio más eficaz entre exploración y preservación de estructura, limitando las perturbaciones excesivas sin eliminar la capacidad de escape de óptimos locales.

\colorbox{yellow}{Debe de comprobarse cómo funciona la fórmula con el ratio añadido:}
\[
    \mathrm{num\_swaps} = \min\left(\mathrm{length} \times 0.15,\ \mathrm{length} \times \mathrm{ratio} \times 0.8\right)
\]


\subsection{Evaluación de resultados}\label{sec:evaluacion-de-resultados}
A continuación, se presentan dos gráficos en forma de boxplot que comparan el rendimiento de los algoritmos con base en la métrica de \textit{accuracy}.
Estas visualizaciones permiten analizar la distribución de resultados, la variabilidad y la consistencia de cada enfoque:

\begin{figure}[!h]
    \centering
    \includegraphics[width=0.95\textwidth]{imagenes/mobilenet-BOXPLOT-accuracy-porcentaje}
    \caption{Boxplot del \textit{accuracy} alcanzado por los algoritmos en función del porcentaje inicial de imágenes seleccionadas.}
    \label{fig:boxplot-accuracy-porcentaje}
\end{figure}

\begin{figure}[!h]
    \centering
    \includegraphics[width=0.95\textwidth]{imagenes/mobilenet-BOXPLOT-accuracy-algoritmo}
    \caption{Boxplot comparando el \textit{accuracy} alcanzado por cada algoritmo, agrupando los resultados según la estrategia utilizada.}
    \label{fig:boxplot-accuracy-algoritmo}
\end{figure}

\subsubsection{Análisis de los resultados}\label{sec:analisis-de-los-resultados}

El primer boxplot (Figura~\ref{fig:boxplot-accuracy-porcentaje}) muestra que, como era esperable,
el \textit{accuracy} mejora progresivamente al incrementar el porcentaje de datos usados.
Sin embargo, también se observa que algunos algoritmos, como los genéticos mejorados, obtienen valores muy competitivos
incluso con bajos porcentajes iniciales, lo que refuerza su utilidad como técnica de reducción.


En el segundo boxplot (Figura~\ref{fig:boxplot-accuracy-algoritmo}), permite comparar directamente el impacto de cada estrategia.
Las versiones mejoradas del algoritmo genético (v2 y v3), así como el algoritmo memético,
no solo alcanzan medianas de accuracy más altas, sino que también muestran menor variabilidad.
Esta menor dispersión indica una mayor estabilidad entre ejecuciones, lo cual es deseable en procesos
de optimización con componentes estocásticos.


Además, se observa una clara reducción en los valores atípicos negativos en los algoritmos mejorados, lo que sugiere una menor
probabilidad de obtener resultados significativamente bajos.
Esto es especialmente relevante en contextos donde se busca fiabilidad en entornos con recursos limitados.


En conjunto, los resultados evidencian que las mejoras introducidas, como el cruce ponderado, la lógica de reinicio poblacional
o la combinación con búsqueda local, aportan beneficios sustanciales en términos de rendimiento y robustez.
Estos algoritmos superan sistemáticamente a las estrategias más simples (aleatorio y búsqueda local), tanto en
precisión como en consistencia, confirmando su idoneidad para tareas de selección de subconjuntos de datos en aprendizaje profundo.


Estas observaciones permiten concluir que las modificaciones introducidas en el algoritmo genético no solo mejoran la media de rendimiento,
sino que también aportan consistencia y robustez frente a la aleatoriedad inherente a este tipo de procesos evolutivos.


\section{Análisis del balance de clases}\label{sec:analisis-del-balance-de-clases}
Adicionalmente, se realizó un análisis del balance de clases en las soluciones finales generadas por cada algoritmo.
El objetivo era comprobar si los subconjuntos seleccionados mantenían una representación equilibrada entre las
distintas clases del dataset (Paper, Rock y Scissors), o si ciertos algoritmos tendían a favorecer algunas clases frente a otras.


\begin{figure}[htp]
    \centering
    \includegraphics[width=1\textwidth]{imagenes/mobilenet-BARPLOT-balance-de-clases-por-algoritmo}
    \caption{Distribución del porcentaje de imágenes por clase en las soluciones generadas por cada algoritmo.}
    \label{fig:balance-de-clases-por-algoritmo}
\end{figure}

Tal como se observa en la Figura~\ref{fig:balance-de-clases-por-algoritmo}, los algoritmos más simples como el aleatorio o la búsqueda local presentan ligeras desviaciones, mientras que las versiones más avanzadas, en especial los algoritmos genéticos mejorados y el memético, logran mantener una distribución más homogénea entre clases.
Este equilibrio es clave para evitar sesgos en el modelo entrenado y mejorar métricas como el F1-score.


Aunque si podemos observar una leve tendencia a que los algoritmos necesitan unos pocos datos más de la clase \textit{Paper}, a la vez que reduce el porcentaje de la clase \textit{Rock}.


\begin{figure}[htp]
    \centering
    \includegraphics[width=1\textwidth]{imagenes/mobilenet-BARPLOT-porcentaje-inical-vs-final-por-algoritmo}
    \caption{Comparación entre porcentaje inicial y final de imágenes seleccionadas por algoritmo.
    }
    \label{fig:comparation-initial-final}
\end{figure}

Por otro lado, se analizó cómo varía el porcentaje de datos seleccionados desde el inicio hasta el final del proceso en cada algoritmo.
Este análisis, reflejado en la Figura~\ref{fig:comparation-initial-final}, permite observar si los algoritmos tienden a mantener,
reducir o incluso aumentar la cantidad de imágenes utilizadas durante su evolución.


Se puede apreciar que los algoritmos con un porcentaje fijo mantienen estable la cantidad de datos seleccionados, como era de esperar debido a su implementación.
Sin embargo, en los algoritmos con comportamiento “libre”, como \textit{búsqueda local (libre)} o \textit{genético2 (libre)}, si se observa su variabilidad,
en particular, ambos algoritmos tienden a aumentar el porcentaje de imágenes seleccionadas respecto al inicial,
lo que vuelve a confirmar la tendencia de necesitar más imágenes para mejorar el rendimiento del modelo.


\section{Validación con el dataset PAINTING}\label{sec:validacion-con-el-dataset-painting}
Para comprobar la generalización de los resultados, se repitieron los experimentos con el dataset PAINTING, de mayor tamaño y complejidad.
% En este caso, los mejores resultados también se obtuvieron con las versiones mejoradas de los algoritmos genéticos, seguidos muy de cerca por el memético.
% En todos los casos, se logró mantener o incluso mejorar el rendimiento del modelo en comparación con el uso del 100\% de los datos, empleando únicamente
% un subconjunto optimizado.

\colorbox{yellow}{Falta añadir el análisis detallado de los resultados obtenidos con el dataset PAINTING.}