% !TeX root = ../proyecto.tex

\chapter{Resultados y Análisis}\label{ch:resultados-y-analisis}
En este capítulo se exponen los experimentos realizados y los resultados obtenidos en los distintos escenarios evaluados.
El objetivo principal fue analizar el rendimiento de los modelos entrenados con conjuntos de datos reducidos,
seleccionados mediante los distintos algoritmos desarrollados a lo largo del proyecto.



\section{Resultados con el conjunto completo}\label{sec:resultados-conjunto-completo}
Como punto de partida, se evalúa el rendimiento de los modelos convolucionales entrenados con el \textbf{100\%} del conjunto de datos.
Esta prueba sirve como referencia para contrastar los resultados obtenidos mediante las técnicas de reducción aplicadas posteriormente.
Se emplean los modelos \textbf{ResNet50} y \textbf{MobileNetV2}, y se miden las métricas de \textit{accuracy},
\textit{precision}, \textit{recall} y \textit{F1-score} sobre el conjunto de validación.

\begin{table}[htp]
    \centering
    \resizebox{\textwidth}{!}{
        \begin{tabular}{P{2cm} P{2.5cm} P{2.5cm} P{2.5cm} P{2cm}}
            \toprule
            \textbf{Duración por Eval.} & \textbf{Accuracy (Avg)} & \textbf{Precision (Avg)} & \textbf{Recall (Avg)} & \textbf{F1-score (Avg)} \\
            \midrule
            \multicolumn{5}{l}{\textbf{Modelo ResNet50}}                                                                                       \\
            \midrule
            00:02:42                    & 87,90\%                 & 88,96\%                  & 87,90\%               & 87,81\%                 \\
            \midrule
            \multicolumn{5}{l}{\textbf{Modelo MobileNet}}                                                                                      \\
            \midrule
            00:03:16                    & 78,60\%                 & 81,55\%                  & 78,60\%               & 77,68\%                 \\
            \bottomrule
        \end{tabular}
    }
    \caption{Comparativa de resultados del \textbf{100\%} con los modelos \texttt{ResNet50} y \texttt{MobileNet}.}
    \label{tab:resultados-100-resnet50-mobilenet}
\end{table}

\begin{figure}[htp]
    \centering
    \includegraphics[width=0.95\textwidth]{imagenes/evaluaciones/comparacion_modelos_100.png}
    \caption{Comparación de \textit{accuracy} de los modelos \texttt{ResNet50} y \texttt{MobileNet} usando el 100\% del conjunto de datos.}
    \label{fig:comparacion_modelos_100}
\end{figure}

Los resultados presentados en la Tabla~\ref{tab:resultados-100-resnet50-mobilenet}, y visualizados en la Figura~\ref{fig:comparacion_modelos_100},
representan el rendimiento máximo alcanzable en condiciones ideales,
es decir, utilizando la totalidad del conjunto de datos sin aplicar técnicas de reducción.
Este escenario establece un techo de rendimiento que se utiliza como referencia para evaluar la eficacia de
los algoritmos de reducción de datos implementados en el resto del estudio.

En particular, se observa que \textbf{ResNet50} obtiene mejores resultados en todas las métricas evaluadas,
destacando especialmente en \textit{accuracy} y \textit{precision}, donde supera por más de 9 puntos porcentuales a \textbf{MobileNetV2}.
Esta superioridad en el rendimiento viene acompañada de un tiempo por evaluación ligeramente menor (\textbf{00:02:42} frente a \textbf{00:03:16}),
lo que indica una mayor eficiencia en la etapa de predicción.

Esta comparativa inicial permite establecer las bases para el análisis de los resultados obtenidos mediante las técnicas de reducción de datos,
sirviendo como referencia para valorar el impacto de las estrategias aplicadas en las secciones posteriores.


\section{Comparativa inicial de modelos}\label{sec:comparativa-inicial-modelos}
Antes de aplicar los algoritmos propuestos de reducción de datos, se considera fundamental realizar una comparativa inicial entre distintos
modelos de redes neuronales convolucionales para determinar cuál de ellos es el más adecuado para los experimentos.
Esta comparación permite identificar la arquitectura que ofrece un mejor equilibrio entre rendimiento y eficiencia computacional,
estableciendo una base sólida sobre la que construir los siguientes análisis.

Para llevar a cabo esta comparativa, se utiliza el enfoque aleatorio (\texttt{RS}) como estrategia de referencia.
Al seleccionar subconjuntos de datos de manera aleatoria, sin ninguna optimización, se obtiene una línea base que permiteevaluar el
comportamiento de cada modelo en condiciones controladas.
Esta línea base es especialmente valiosa, ya que ofrece una perspectiva realista del rendimiento mínimo esperable sin aplicar técnicas
avanzadas de reducción de datos, sirviendo como punto de partida para comparar las mejoras introducidas por los algoritmos posteriores.


\begin{table}[htp]
    \centering
    \resizebox{\textwidth}{!}{
        \begin{tabular}{P{2cm} P{2.5cm} P{2.5cm} P{2.5cm} P{2cm} P{2cm} P{2cm} P{2cm}}
            \toprule
            \textbf{Porcentaje Inicial} & \textbf{Evaluaciones Realizadas} & \textbf{Duración Total} & \textbf{Duración por Eval.} &
            \textbf{Accuracy (Avg)}     & \textbf{Precision (Avg)}         & \textbf{Recall (Avg)}   & \textbf{F1-score (Avg)}                                             \\
            \midrule
            \multicolumn{8}{l}{\textbf{Modelo ResNet50}}                                                                                                                   \\
            \midrule
            10\%                        & 100                              & 01:21:37                & 00:00:48                    & 85,16\% & 86,30\% & 85,16\% & 85,02\% \\
            25\%                        & 100                              & 01:28:14                & 00:00:52                    & 87,26\% & 88,13\% & 87,26\% & 87,02\% \\
            50\%                        & 100                              & 02:27:17                & 00:01:28                    & 88,49\% & 89,51\% & 88,49\% & 88,30\% \\
            75\%                        & 100                              & 03:52:41                & 00:02:19                    & 89,89\% & 90,53\% & 89,89\% & 89,70\% \\
            100\%                       & 1                                & -                       & 00:02:55                    & 87,42\% & 88,73\% & 87,42\% & 87,18\% \\
            \midrule
            \multicolumn{8}{l}{\textbf{Modelo MobileNet}}                                                                                                                  \\
            \midrule
            10\%                        & 100                              & 00:38:16                & 00:00:22                    & 81,34\% & 82,46\% & 81,34\% & 80,52\% \\
            25\%                        & 100                              & 01:18:22                & 00:00:47                    & 80,70\% & 81,90\% & 80,70\% & 79,90\% \\
            50\%                        & 100                              & 02:26:40                & 00:01:28                    & 80,86\% & 82,65\% & 80,86\% & 80,22\% \\
            75\%                        & 100                              & 03:05:48                & 00:01:51                    & 82,26\% & 84,16\% & 82,26\% & 81,67\% \\
            100\%                       & 1                                & -                       & 00:03:16                    & 78,60\% & 81,55\% & 78,60\% & 77,68\% \\
            \bottomrule
        \end{tabular}
    }
    \caption{Comparativa de resultados de la generación inicial utilizando el \texttt{RS} y el \texttt{100\%} con los modelos \texttt{ResNet50} y \texttt{MobileNet}.}
    \label{tab:resnet50-vs-mobilenet}
\end{table}

\begin{figure}[htp]
    \centering
    \includegraphics[width=0.95\textwidth]{imagenes/evaluaciones/comparacion_modelos.png}
    \caption{Diagrama de barras para comparar los modelos usando el \textit{accuracy} alcanzado por cada porcentaje inicial.}
    \label{fig:comparacion_modelos}
\end{figure}

Se realizan pruebas con distintos porcentajes iniciales de datos seleccionados aleatoriamente (10\%, 25\%, 50\%, 75\% y 100\%), utilizando tanto \texttt{ResNet50} como \texttt{MobileNetV2}.
Los resultados obtenidos se presentan en la Tabla~\ref{tab:resnet50-vs-mobilenet}, que resume las métricas alcanzadas por cada modelo,
y en la Figura~\ref{fig:comparacion_modelos}, que muestra la comparación de los valores de \textit{accuracy} mediantes barras separados por los porcentajes iniciales.

Los resultados muestran que \texttt{ResNet50} logra un mejor rendimiento en términos de \textit{accuracy}, \textit{precision}, \textit{recall} y \textit{F1-score}.
Sin embargo, este mejor rendimiento viene acompañado de un tiempo de entrenamiento considerablemente mayor.
Por su parte, \texttt{MobileNetV2} ofrece una solución más eficiente en cuanto a tiempos de ejecución, a costa de una ligera pérdida de precisión,
lo que la convierte en una opción atractiva para entornos con recursos computacionales limitados.

Aunque \texttt{ResNet50} alcanza la mayor precisión con el 100\% de los datos, su huella computacional (aproximádamente 25M parámetros) encarece la ejecución de los cientos de entrenamientos requeridos en los experimentos de reducción.
\texttt{MobileNetV2}, con solo 3,5 M parámetros, reduce entre un 45\% y un 60\% el tiempo total de entrenamiento y
permite reproducir el estudio en hardware modesto sin sacrificar más de 5-6 puntos porcentuales de precisión en los subconjuntos evaluados.
Por tanto, se adopta \texttt{MobileNetV2} como modelo principal, priorizando la eficiencia sobre la ligera ventaja de \texttt{ResNet50}.
El \texttt{RS} queda como línea base para cuantificar las mejoras aportadas por los algoritmos de reducción propuestos

Aunque \texttt{ResNet50} obtiene los mejores resultados cuando se utiliza 100\% de los datos,
su elevado coste computacional hace que no sea práctico para este trabajo.
Dado que los experimentos requieren entrenar el modelo cientos de veces con diferentes subconjuntos,
usar una red tan pesada como \texttt{ResNet50} resultaría demasiado costoso en tiempo y recursos.
En cambio, \texttt{MobileNetV2} ofrece una alternativa mucho más ligera,
reduciendo notablemente los tiempos de entrenamiento y permitiendo realizar todos los experimentos incluso en equipos con menos capacidad.
Además, la pérdida de precisión respecto a \texttt{ResNet50} es moderada y aceptable para los objetivos del estudio.
Por ello, se elige \texttt{MobileNetV2} como modelo principal, priorizando la eficiencia.
Así, el \texttt{RS} se utiliza como punto de referencia para evaluar el impacto de las estrategias de reducción de datos.


\section{Resultados de la búsqueda local}\label{sec:resultados-busqueda-local}
Como se describe en el apartado correspondiente (ver \hyperref[sec:algoritmo-busqueda-local]{Sección~\ref*{sec:algoritmo-busqueda-local}}),
la búsqueda local permite mejorar progresivamente una solución inicial mediante pequeñas modificaciones guiadas por el rendimiento.
En este apartado se evalúa su efectividad como alternativa más estructurada frente al enfoque aleatorio, pero sin llegar a la complejidad de los algoritmos evolutivos.
Su inclusión busca analizar hasta qué punto una estrategia simple pero guiada puede generar subconjuntos de datos más representativos y consistentes.

\begin{figure}[htp]
    \centering
    \includegraphics[width=1\textwidth]{imagenes/evaluaciones/comparacion_rs-ls.png}
    \caption{Boxplot comparando el \texttt{RS} con la \texttt{LS} usando \textit{accuracy}.}
    \label{fig:aleatorio-vs-busqueda-local}
\end{figure}
En la Figura~\ref{fig:aleatorio-vs-busqueda-local} se muestran los resultados mediante un boxplot que permite observar
la distribución completa de valores obtenidos en las distintas ejecuciones.

Se puede apreciar que el algoritmo de búsqueda local (\texttt{LS}) mejora claramente la mediana del \textit{accuracy} respecto al enfoque aleatorio (\texttt{RS}).
Mientras que el \texttt{RS} se sitúa en torno a una mediana de \textbf{0.787}, la \texttt{LS} alcanza una mediana superior, próxima a \textbf{0.818}.
Esta diferencia refleja una mayor capacidad del algoritmo local para generar subconjuntos más representativos y eficaces.

Además, los valores máximos que alcanzan ambos algoritmos son similares (en torno a \textbf{0.858}-\textbf{0.860}),
pero la \texttt{LS} muestra una dispersión más acotada hacia valores altos, lo que sugiere mayor estabilidad en sus resultados.
En cambio, el \texttt{RS} presenta una mayor dispersión hacia valores bajos y aunque presente una mayor sensibilidad a la aleatoriedad de las selecciones,
puede llegar a tener mejores valores mínimos, como se evidencia en su menor valor mínimo (\textbf{0.785} frente a \textbf{0.769} en \texttt{LS}),
pero siendo el de la \texttt{LS} un valor atípico.

Esto pone de manifiesto que, aunque el \texttt{RS} puede ocasionalmente alcanzar buenos resultados,
la \texttt{LS} ofrece una mejor consistencia y fiabilidad, con menos varianza entre ejecuciones y una tendencia general a obtener subconjuntos de entrenamiento más efectivos.


\section{Resultados del Algoritmo Genético}\label{sec:resultados-algoritmo-genetico}
Con el objetivo de superar las limitaciones observadas (como el riesgo de estancamiento o la exploración poco estructurada del espacio de soluciones)
se incorpora un enfoque evolutivo más completo: el algoritmo genético (\texttt{GA}), descrito en la Sección~\ref{sec:genetico-v1}, el cual sirve como punto de partida para explorar la
aplicación de estrategias metaheurísticas en la selección de subconjuntos representativos de imágenes.
Su estructura evolutiva, basada en selección por torneo, cruce e incorporación de mutación,
ofrece ya desde sus primeras versiones una capacidad superior para generalizar, en comparación con métodos más simples como la \texttt{LS}.

\begin{figure}[htp]
    \centering
    \includegraphics[width=1\textwidth]{imagenes/evaluaciones/comparacion_ls-ga.png}
    \caption{Boxplot comparando \texttt{LS} con \texttt{GA} usando \textit{accuracy}.}
    \label{fig:lr-vs-gen-v1}
\end{figure}

Tal como se observa en la Figura~\ref{fig:lr-vs-gen-v1}, el algoritmo genético (\texttt{GA}) consigue una \textbf{mediana de \textit{accuracy}} más alta que la búsqueda local (\texttt{LS}),
reflejando un rendimiento medio más consistente.
Además, presenta un valor máximo superior (alcanza hasta \textbf{0.876}), lo que evidencia su mayor potencial para encontrar soluciones de alta calidad.

No obstante, también se aprecia una ligera mayor dispersión en los resultados del \texttt{GA}, particularmente hacia los valores bajos.
Esto indica que, pese a su capacidad exploratoria, el \texttt{GA} puede generar soluciones poco efectivas si no se controlan adecuadamente ciertos operadores como el cruce o la mutación.
De hecho, su valor mínimo (\textbf{0.796}) es superior al de la \texttt{LS} en esta comparativa,
pero deja margen para mejoras en la presión selectiva o en mecanismos que eviten estancamientos.

La \texttt{LS}, por su parte, mantiene un comportamiento más estable, aunque con una mediana ligeramente inferior.
Su distribución es más concentrada y limitada en el extremo superior, lo que evidencia su carácter más explotador pero con menor capacidad para alcanzar soluciones óptimas globales.

Estos resultados sirven como evidencia empírica para continuar desarrollando nuevas versiones del \texttt{GA},
incorporando mejoras específicas en sus operadores con el fin de aprovechar su capacidad exploratoria y, al mismo tiempo, mitigar sus limitaciones.

\section{Mejorando el operador de cruce}\label{sec:incorporacion-cruce}
La primera mejora introducida al \texttt{GA} consiste en reemplazar el cruce aleatorio por un cruce ponderado,
donde se prioriza la contribución del progenitor con mayor \textit{fitness}.
Además, se incorpora una estrategia selectiva que conserva únicamente el mejor de los dos hijos generados en cada cruce.
Ambos cambios, explicados en detalle en la \hyperref[sec:genetico-v2]{Sección~\ref*{sec:genetico-v2}},
buscan aumentar la presión evolutiva y acelerar la convergencia hacia soluciones de mayor calidad,
evitando así que soluciones mediocres se propaguen innecesariamente en la población.

\begin{figure}[htp]
    \centering
    \includegraphics[width=1\textwidth]{imagenes/evaluaciones/operador-de-cruce.png}
    \caption{Boxplot de \textit{accuracy} comparando el \texttt{GA} y el \texttt{GA-WC}.}
    \label{fig:cruce_ponderado}
\end{figure}

Tal como se observa en la Figura~\ref{fig:cruce_ponderado}, esta modificación produce una mejora clara en la calidad y estabilidad de los resultados.
La versión con cruce ponderado (\texttt{GA-WC}) alcanza un valor máximo atípico superior (\textbf{0.887}),
aunque presenta una mediana más baja que la versión básica (\texttt{GA}).

Pero por otra parte, se aprecia una ligera reducción en la dispersión de los valores inferiores,
con un mínimo de \textbf{0.798} frente al \textbf{0.796} en la versión anterior, lo que sugiere una mayor consistencia.
Aunque el IQR (\hyperref[subsec:visualizacion-de-resultados]{Rango Intercuartílico}) sigue siendo amplio,
la acumulación de valores más cercanos al rango superior refleja una convergencia evolutiva más enfocada y menos dependiente del azar.

En conjunto, esta mejora en el cruce no solo permite una transferencia más eficiente de características ventajosas,
sino que también incrementa la presión selectiva sobre la calidad de las soluciones.
Esto se traduce en un comportamiento más robusto, menos propenso a resultados erráticos y con una mayor capacidad de exploración dirigida del espacio de soluciones.


\section{Mejorando el operador de mutación}\label{sec:mejorando-mutacion}
A partir de los resultados obtenidos con el \texttt{GA-WC}, se evalua una nueva versión en la que se introdujo una \textbf{mutación adaptativa}
(ver \hyperref[sec:genetico-mutacion]{Sección~\ref*{sec:genetico-mutacion}}).
A diferencia de la versión original con tasa fija, esta estrategia ajusta el número de intercambios en función del tamaño del subconjunto mutado,
lo que permite una mayor flexibilidad en escenarios de diferente escala.

\begin{figure}[htp]
    \centering
    \includegraphics[width=1\textwidth]{imagenes/evaluaciones/mutacion-adaptativa.png}
    \caption{Comparación de \textit{accuracy} entre el \texttt{GA-WC} y el \texttt{GA-AM}.}
    \label{fig:mutacion-adaptativa}
\end{figure}

\begin{figure}[htp]
    \centering
    \includegraphics[width=1\textwidth]{imagenes/evaluaciones/mutacion-adaptativa_por_porcentaje.png}
    \caption{Diagrama de comparación usando el \textit{accuracy} entre el \texttt{GA-WC} Y el \texttt{GA-AM}, separados por porcentaje inicial.}
    \label{fig:mutacion-adaptativa-porcentaje}
\end{figure}

Como se aprecia en la Figura~\ref{fig:mutacion-adaptativa},
ambos algoritmos presentan distribuciones de \textit{accuracy} muy similares en términos de mediana e IQR.
El \texttt{GA-WC} mantiene un ligero máximo superior, alcanzando un \textbf{0.887} frente al \textbf{0.876} del \texttt{GA-AM}.
Sin embargo, el algoritmo con mutación adaptativa muestra una distribución más compacta en la parte central,
con menor dispersión hacia valores bajos, lo que sugiere una mayor consistencia entre ejecuciones.

La principal diferencia se observa en la robustez de los resultados: el \texttt{GA-AM} presenta una distribución más estable,
con menos valores atípicos y una mayor concentración de ejecuciones cerca del cuartil superior.
Este comportamiento indica una menor propensión a caídas abruptas de rendimiento y refuerza la idea de que la mutación
adaptativa mejora la estabilidad del proceso evolutivo.

Al analizar los resultados por porcentaje inicial (Figura~\ref{fig:mutacion-adaptativa-porcentaje}),
se confirma que el \texttt{GA-AM} mantiene una estabilidad más uniforme en todos los escenarios, mientras que el \texttt{GA-WC} muestra una mayor variabilidad,
especialmente en configuraciones con menor cantidad de datos.
Aunque las diferencias en \textit{accuracy} medio no son significativas, la menor dispersión y la reducción de valores extremos justifican la
preferencia por la versión adaptativa en contextos donde la fiabilidad es prioritaria.

En conjunto, la mutación adaptativa no genera una mejora radical en precisión media,
pero sí contribuye a una evolución más controlada y menos susceptible a degradaciones, favoreciendo una mayor consistencia en los resultados.
Además, al adaptarse al tamaño del subconjunto, evita configuraciones subóptimas que podrían surgir con una tasa fija de mutación,
lo que la hace especialmente adecuada para escenarios con escalas variables.

Esta mejora consolida la capacidad del algoritmo para equilibrar exploración y explotación,
y lo posiciona como una alternativa más robusta y estable en tareas de reducción de datos para aprendizaje profundo.


\section{Resultados del reinicio poblacional}\label{sec:resultados-reinicio-poblacional}
La última mejora realizada de los algoritmos genéticos (ver \hyperref[sec:genetico-v3]{Sección~\ref*{sec:genetico-v3}})
introduce una lógica de reinicio poblacional diseñada para evitar estancamientos evolutivos.
El algoritmo monitoriza el rendimiento del segundo mejor individuo, y si este no mejora durante dos generaciones consecutivas,
se aplica un reinicio parcial que conserva únicamente al mejor individuo de la población.

\begin{figure}[htp]
    \centering
    \includegraphics[width=1\textwidth]{imagenes/evaluaciones/reinicio-poblacional.png}
    \caption{Comparación de \textit{accuracy} entre el \texttt{GA-AM} y con \texttt{GA-PR}.}
    \label{fig:reinicio_poblacional}
\end{figure}
\begin{table}[htp]
    \centering
    \resizebox{\textwidth}{!}{
        \begin{tabular}{P{2cm} P{2cm} P{2.5cm} P{2.5cm} P{2.5cm} P{2.5cm} P{2cm} P{2cm} P{2cm} P{2cm} P{2cm} P{2.5cm} P{2.5cm}}
            \toprule
            \textbf{Algoritmo}    & \textbf{Duración Total} & \textbf{Duración por Eval.} & \textbf{Accuracy (Avg)} & \textbf{Precision (Avg)} &
            \textbf{Recall (Avg)} & \textbf{F1-score (Avg)} & \textbf{Evaluaciones}       & \textbf{Porc.Paper}     & \textbf{Porc.Rock}       & \textbf{Porc.Scissors}                                               \\
            \midrule
            \multicolumn{11}{l}{\textbf{10\%}}                                                                                                                                                                        \\
            \midrule
            GA-AM                 & 00:27:39                & 00:00:17                    & 83,66\%                 & 84,24\%                  & 83,66\%                & 83,09\% & 100 & 35,64\% & 30,87\% & 33,49\% \\
            GA-PR                 & 00:29:02                & 00:00:17                    & 81,93\%                 & 83,07\%                  & 81,93\%                & 81,19\% & 100 & 34,52\% & 32,14\% & 33,33\% \\
            \midrule
            \multicolumn{11}{l}{\textbf{25\%}}                                                                                                                                                                        \\
            \midrule
            GA-AM                 & 00:57:29                & 00:00:34                    & 82,42\%                 & 83,68\%                  & 82,42\%                & 81,78\% & 100 & 33,33\% & 32,54\% & 34,13\% \\
            GA-PR                 & 01:00:22                & 00:00:36                    & 82,64\%                 & 83,78\%                  & 82,64\%                & 81,96\% & 100 & 33,62\% & 33,27\% & 33,11\% \\
            \midrule
            \multicolumn{11}{l}{\textbf{50\%}}                                                                                                                                                                        \\
            \midrule
            GA-AM                 & 01:47:31                & 00:01:05                    & 81,13\%                 & 82,62\%                  & 81,13\%                & 80,41\% & 100 & 33,94\% & 32,76\% & 33,30\% \\
            GA-PR                 & 01:53:13                & 00:01:08                    & 81,24\%                 & 83,09\%                  & 81,24\%                & 80,59\% & 100 & 33,21\% & 33,20\% & 33,59\% \\
            \midrule
            \multicolumn{11}{l}{\textbf{75\%}}                                                                                                                                                                        \\
            \midrule
            GA-AM                 & 02:36:41                & 00:01:34                    & 82,42\%                 & 83,94\%                  & 82,42\%                & 81,85\% & 100 & 33,67\% & 33,11\% & 33,22\% \\
            GA-PR                 & 02:41:24                & 00:01:37                    & 82,69\%                 & 84,33\%                  & 82,69\%                & 82,06\% & 100 & 33,52\% & 33,44\% & 33,04\% \\
            \midrule
            \multicolumn{11}{l}{\textbf{100\%}}                                                                                                                                                                       \\
            \midrule
            100\%                 & --                      & 00:03:16                    & 78,60\%                 & 81,55\%                  & 78,60\%                & 77,68\% & 1   & 33,33\% & 33,33\% & 33,33\% \\
            \bottomrule
        \end{tabular}
    }
    \caption{Resultados de los algoritmos \texttt{GA-AM} y \texttt{GA-PR} por porcentaje inicial.}
    \label{tab:resultados-am-pr-porcentaje}
\end{table}

Los resultados, mostrados en la Figura~\ref{fig:reinicio_poblacional} y la Tabla~\ref{tab:resultados-am-pr-porcentaje},
permiten extraer varias conclusiones relevantes.
En primer lugar, el \textbf{valor máximo de accuracy} es idéntico para \texttt{GA-AM} y \texttt{GA-PR}, alcanzando ambos un \textbf{0.876},
lo que sugiere que el reinicio no favorece la aparición de soluciones de mayor calidad.
Por el contrario, el \textbf{valor mínimo de accuracy} observado en \texttt{GA-PR} (\textbf{0.766}) es notablemente más bajo que el de \texttt{GA-AM} (\textbf{0.798}),
lo que indica una mayor propensión a obtener ejecuciones de bajo rendimiento cuando se utiliza la estrategia de reinicio.

La distribución general de resultados revela que el \texttt{GA-PR} no logra una reducción significativa en la dispersión de los valores:
aunque los cuartiles y medianas de \texttt{GA-AM} y \texttt{GA-PR} son similares, se observa un leve desplazamiento hacia valores ligeramente más bajos en \texttt{GA-PR},
lo cual es especialmente evidente en el escenario de menor porcentaje inicial (10\%).
Esta tendencia sugiere que el reinicio poblacional, al reintroducir diversidad de forma abrupta,
puede generar soluciones subóptimas que no contribuyen de manera sustancial a mejorar la población.

La Tabla de resultados~\ref{tab:resultados-am-pr-porcentaje} respalda estas observaciones: en promedio,
el algoritmo \texttt{GA-AM} presenta una ligera superioridad en precisión media (\textbf{83,66\%} frente a \textbf{81,93\%} en el 10\%), así como en F1-score y recall.
Estas diferencias, aunque no drásticas, son consistentes en la mayoría de los escenarios.
Además, las métricas de distribución de clases y las duraciones de ejecución son prácticamente equivalentes entre \texttt{GA-AM} y \texttt{GA-PR},
lo que refuerza la idea de que el impacto del reinicio poblacional no justifica su complejidad añadida.

En resumen, aunque el reinicio poblacional busca aumentar la exploración y evitar el estancamiento,
en esta implementación concreta no aporta mejoras tangibles en términos de precisión ni estabilidad,
e incluso puede introducir soluciones más erráticas en ciertas ejecuciones.

\section{Resultados con versiones libres}\label{sec:resultados-versiones-libres}
Como parte de la evolución de los algoritmos desarrollados, se propone la creación de \textbf{versiones libres},
en las que el tamaño del subconjunto seleccionado no permanece fijo durante la ejecución, sino que puede ajustarse de forma dinámica en función de las decisiones evolutivas.
Esta flexibilidad permite que los algoritmos modifiquen el número de datos utilizados a medida que avanzan las generaciones,
adaptándose de manera más natural a las características del problema.

Para evaluar el impacto de esta modificación, se generan versiones libres tanto para el algoritmo de búsqueda local (\texttt{LS})
como para el algoritmo con mutación adaptativa (\texttt{GA-AM}).
En el caso de la \texttt{LS}, además, se incorpora una variación adicional: el porcentaje inicial de imágenes no es fijo,
sino que se selecciona aleatoriamente en cada ejecución, introduciendo así un mayor grado de aleatoriedad y diversidad en el proceso.


\begin{figure}[htp]
    \centering
    \includegraphics[width=0.9\textwidth]{imagenes/evaluaciones/libres/barplot_por_algoritmo.png}
    \caption{Comparación de \textit{accuracy} entre el \texttt{LS}, \texttt{GA-AM} y sus versiones Libres (formato BARPLOT).}
    \label{fig:barplot_por_algoritmo-libres}
\end{figure}

En la Figura~\ref{fig:barplot_por_algoritmo-libres} se observa que las versiones libres mantienen un rendimiento promedio muy similar al de las versiones originales.
La precisión media (\textit{accuracy}) de las versiones libres es ligeramente superior en algunos casos,
lo que indica que la capacidad de adaptación no introduce pérdidas de rendimiento e incluso puede aportar pequeñas mejoras en escenarios específicos.
Las barras de error sugieren que la variabilidad entre ejecuciones se mantiene controlada,
lo que refuerza la idea de que la flexibilidad no compromete la estabilidad de los resultados.


\begin{figure}[htp]
    \centering
    \includegraphics[width=0.9\textwidth]{imagenes/evaluaciones/libres/boxplot_por_algoritmo.png}
    \caption{Comparación de \textit{accuracy} entre el \texttt{LS}, \texttt{GA-AM} y sus versiones Libres (formato BOXPLOT).}
    \label{fig:boxplot_por_algoritmo-libres}
\end{figure}

Los diagramas de caja en la Figura~\ref{fig:boxplot_por_algoritmo-libres} permiten una comparación más detallada de la dispersión de los resultados.
En el caso de la \texttt{LS}, la versión libre muestra una ligera mejora en la mediana y un rango intercuartílico (IQR) más compacto, lo que indica una mayor estabilidad.
Además, se observan valores atípicos superiores que sugieren la posibilidad de obtener ejecuciones con precisión especialmente alta.
En el caso del \texttt{GA-AM}, las diferencias entre las versiones fija y libre son más sutiles: aunque las medianas son prácticamente idénticas,
la versión libre presenta una ligera reducción en los valores mínimos, lo que podría reflejar una mayor adaptabilidad frente a escenarios más difíciles.


\begin{figure}[htp]
    \centering
    \includegraphics[width=0.9\textwidth]{imagenes/evaluaciones/libres/am_por_pi.png}
    \caption{Comparación de \textit{accuracy} en función del porcentaje inicial para el algoritmo \texttt{GA-AM}.}
    \label{fig:am_por_pi}
\end{figure}

La Figura~\ref{fig:am_por_pi} muestra la evolución del \textit{accuracy} en función del porcentaje inicial de datos utilizados en el algoritmo \texttt{GA-AM}.
Se observa que la precisión se mantiene estable a lo largo de los distintos puntos de partida,
con ligeras variaciones que no afectan de manera significativa al comportamiento general del algoritmo.
Este resultado confirma que la flexibilidad en el tamaño del subconjunto no introduce un sesgo negativo en la calidad de las soluciones encontradas,
sino que permite adaptarse a diferentes condiciones iniciales sin comprometer el rendimiento.


\begin{figure}[htp]
    \centering
    \includegraphics[width=0.9\textwidth]{imagenes/evaluaciones/libres/distribucion-clases.png}
    \caption{Distribución de clases en los subconjuntos generados por los algoritmos estándar y libres.}
    \label{fig:distribucion_libres}
\end{figure}

Por otro lado, la Figura~\ref{fig:distribucion_libres} demuestra que tanto las versiones fijas como las libres preservan una
distribución equilibrada de las clases \texttt{Rock}, \texttt{Paper} y \texttt{Scissors}.
Las proporciones entre clases se mantienen estables, sin que la flexibilidad en el tamaño del subconjunto genere desbalances significativos.
Este aspecto es crucial, ya que garantiza la validez de los experimentos y asegura que las mejoras observadas no son producto de un sesgo de clase inadvertido.


\begin{figure}[htp]
    \centering
    \includegraphics[width=0.9\textwidth]{imagenes/evaluaciones/libres/scatter_ls-f.png}
    \caption{Relación entre \textit{accuracy} y porcentaje final de datos seleccionados por el algoritmo \texttt{LS-F}.}
    \label{fig:scatter_bl_f}
\end{figure}

\begin{figure}[htp]
    \centering
    \includegraphics[width=0.9\textwidth]{imagenes/evaluaciones/libres/scatter_ga-am-f.png}
    \caption{Relación entre \textit{accuracy} y porcentaje final de datos seleccionados por el algoritmo \texttt{GA-AM-F}.}
    \label{fig:scatter_gen_v2}
\end{figure}

\begin{table}[htp]
    \centering
    \resizebox{\textwidth}{!}{
        \begin{tabular}{P{2cm} P{2cm} P{2.5cm} P{2.5cm} P{2.5cm} P{2.5cm} P{2cm} P{2cm} P{2cm} P{2cm} P{2cm}}
            \toprule
            \textbf{Algoritmo} & \textbf{Duración Total} & \textbf{Duración por Eval.} & \textbf{Accuracy (Avg)} & \textbf{Precision (Avg)} & \textbf{Recall (Avg)} & \textbf{F1-score (Avg)} & \textbf{Evaluaciones} \\
            \midrule
            \multicolumn{8}{l}{\textbf{10\%}}                                                                                                                                                                         \\
            \midrule
            GA-AM              & 00:27:39                & 00:00:16                    & 83.66\%                 & 84.24\%                  & 83.66\%               & 83.09\%                 & 100                   \\
            GA-AM-F            & 00:34:32                & 00:00:20                    & 82.96\%                 & 84.07\%                  & 82.96\%               & 82.12\%                 & 100                   \\
            \midrule
            \multicolumn{8}{l}{\textbf{25\%}}                                                                                                                                                                         \\
            \midrule
            GA-AM              & 00:57:28                & 00:00:34                    & 82.42\%                 & 83.68\%                  & 82.42\%               & 81.78\%                 & 100                   \\
            GA-AM-F            & 01:07:13                & 00:00:40                    & 81.77\%                 & 82.66\%                  & 81.77\%               & 81.18\%                 & 100                   \\
            \midrule
            \multicolumn{8}{l}{\textbf{50\%}}                                                                                                                                                                         \\
            \midrule
            GA-AM              & 01:47:31                & 00:01:04                    & 81.13\%                 & 82.62\%                  & 81.13\%               & 80.41\%                 & 100                   \\
            GA-AM-F            & 01:51:24                & 00:01:06                    & 83.60\%                 & 85.22\%                  & 83.60\%               & 83.03\%                 & 100                   \\
            \midrule
            \multicolumn{8}{l}{\textbf{75\%}}                                                                                                                                                                         \\
            \midrule
            GA-AM              & 02:36:40                & 00:01:34                    & 82.42\%                 & 83.94\%                  & 82.42\%               & 81.85\%                 & 100                   \\
            GA-AM-F            & 02:33:25                & 00:01:32                    & 82.20\%                 & 83.94\%                  & 82.20\%               & 81.47\%                 & 100                   \\
            \midrule
            \multicolumn{8}{l}{\textbf{100\%}}                                                                                                                                                                        \\
            \midrule
            100\%              & --                      & 00:03:15                    & 78.60\%                 & 81.55\%                  & 78.6\%                & 77.68\%                 & 1                     \\
            \bottomrule
        \end{tabular}}
    \caption{Resultados de los algoritmos \texttt{GA-AM} y \texttt{GA-AM-F} por porcentaje inicial.}
    \label{tab:resultados-am-f-porcentaje}
\end{table}

\begin{table}[htp]
    \centering
    \resizebox{\textwidth}{!}{
        \begin{tabular}{P{2cm} P{1.8cm} P{2cm} P{2.3cm} P{2.5cm} P{2.5cm} P{2.5cm} P{2.5cm}}
            \toprule
            \textbf{Algoritmo} & \textbf{Porc. Inicial} & \textbf{Duración Total} & \textbf{Duración por Eval.} & \textbf{Accuracy (Avg)} & \textbf{Precision (Avg)} & \textbf{Recall (Avg)} & \textbf{F1-score (Avg)} \\
            \midrule
            \multicolumn{8}{l}{\textbf{Porcentaje inicial $\leq$10\%}}                                                                                                                                                 \\
            \midrule
            LS                 & 10\%                   & 00:33                   & 00:00                       & 81.24\%                 & 81.81\%                  & 81.24\%               & 80.61\%                 \\
            LS-F               & 6\%                    & 00:38                   & 00:00                       & 83.23\%                 & 83.7\%                   & 83.23\%               & 82.71\%                 \\
            \midrule
            \multicolumn{8}{l}{\textbf{Porcentaje inicial $\leq$25\%}}                                                                                                                                                 \\
            \midrule
            LS                 & 25\%                   & 01:07                   & 00:00                       & 82.47\%                 & 83.95\%                  & 82.47\%               & 81.76\%                 \\
            LS-F               & 12\%                   & 01:00                   & 00:00                       & 80.91\%                 & 82.73\%                  & 80.91\%               & 79.82\%                 \\
            \midrule
            \multicolumn{8}{l}{\textbf{Porcentaje inicial $\leq$50\%}}                                                                                                                                                 \\
            \midrule
            LS                 & 50\%                   & 02:08                   & 00:01                       & 80.76\%                 & 82.77\%                  & 80.76\%               & 80.09\%                 \\
            LS-F               & 35\%                   & 01:36                   & 00:00                       & 80.91\%                 & 83.12\%                  & 80.91\%               & 79.76\%                 \\
            LS-F               & 37\%                   & 01:57                   & 00:01                       & 83.6\%                  & 83.96\%                  & 83.6\%                & 83.19\%                 \\
            LS-F               & 40\%                   & 02:20                   & 00:01                       & 82.12\%                 & 83.3\%                   & 82.12\%               & 81.34\%                 \\
            LS-F               & 50\%                   & 03:13                   & 00:01                       & 81.45\%                 & 82.6\%                   & 81.45\%               & 80.61\%                 \\
            \midrule
            \multicolumn{8}{l}{\textbf{Porcentaje inicial $\leq$75\%}}                                                                                                                                                 \\
            \midrule
            LS                 & 75\%                   & 03:06                   & 00:01                       & 82.37\%                 & 83.97\%                  & 82.37\%               & 81.67\%                 \\
            LS-F               & 57\%                   & 03:54                   & 00:02                       & 82.53\%                 & 83.72\%                  & 82.53\%               & 81.77\%                 \\
            LS-F               & 58\%                   & 04:02                   & 00:02                       & 81.45\%                 & 82.37\%                  & 81.45\%               & 80.76\%                 \\
            LS-F               & 61\%                   & 04:12                   & 00:02                       & 81.72\%                 & 83.3\%                   & 81.72\%               & 81.29\%                 \\
            LS-F               & 63\%                   & 02:36                   & 00:01                       & 83.6\%                  & 84.43\%                  & 83.6\%                & 82.77\%                 \\
            LS-F               & 67\%                   & 03:02                   & 00:01                       & 83.87\%                 & 84.59\%                  & 83.87\%               & 83.4\%                  \\
            LS-F               & 71\%                   & 03:18                   & 00:01                       & 83.06\%                 & 84.93\%                  & 83.06\%               & 82.42\%                 \\
            \midrule
            \multicolumn{8}{l}{\textbf{Porcentaje inicial $\leq$100\%}}                                                                                                                                                \\
            \midrule
            LS-F               & 85\%                   & 03:49                   & 00:02                       & 81.99\%                 & 83.71\%                  & 81.99\%               & 81.56\%                 \\
            LS-F               & 87\%                   & 04:32                   & 00:02                       & 82.26\%                 & 84.22\%                  & 82.26\%               & 81.76\%                 \\
            100\%              & 100\%                  & --                      & 00:03                       & 78.6\%                  & 81.55\%                  & 78.6\%                & 77.68\%                 \\
            \bottomrule
        \end{tabular}}
    \caption{Resultados de los algoritmos \texttt{LS} y \texttt{LS-F} agrupados por franjas de porcentaje inicial.}
    \label{tab:resultados-ls-franjas-porcentaje}
\end{table}


Finalmente, las Figuras~\ref{fig:scatter_bl_f} y~\ref{fig:scatter_gen_v2},
junto con las Tablas~\ref{tab:resultados-am-f-porcentaje} y~\ref{tab:resultados-ls-franjas-porcentaje},
permiten visualizar y comparar con mayor profundidad la relación entre el \textit{accuracy},
el porcentaje final de datos seleccionados y otros aspectos clave como la duración y estabilidad en las versiones libres.

En el caso de la \texttt{LS-F},
la Figura~\ref{fig:scatter_bl_f} muestra una tendencia a incrementar ligeramente el tamaño del subconjunto final en las ejecuciones con mayor precisión,
lo que sugiere una adaptación flexible en función del rendimiento alcanzado.
Esta observación se refuerza al revisar los datos de la Tabla~\ref{tab:resultados-ls-franjas-porcentaje},
donde se evidencia que \texttt{LS-F} mantiene una precisión elevada incluso con porcentajes iniciales relativamente bajos,
y logra hacerlo con duraciones contenidas por evaluación, especialmente en las franjas hasta el 50\%.

Por su parte, el \texttt{GA-AM-F} muestra en la Figura~\ref{fig:scatter_gen_v2} una mayor dispersión en los tamaños finales seleccionados,
lo que refleja su capacidad adaptativa para ajustarse a diferentes condiciones de búsqueda.
Esta flexibilidad también se ve reflejada en la Tabla~\ref{tab:resultados-am-f-porcentaje},
donde se observa que \texttt{GA-AM-F} alcanza una precisión superior a la versión fija \texttt{GA-AM}, especialmente en porcentajes iniciales del 50\%,
manteniendo una duración por evaluación razonable.

Este comportamiento evidencia que los algoritmos libres no solo adaptan la composición del subconjunto, sino también su escala,
ajustando dinámicamente la cantidad de datos utilizados según las necesidades del proceso evolutivo.
En general, estas versiones demuestran ser una alternativa más versátil y robusta,
capaz de mantener la calidad del rendimiento incluso en escenarios con alta variabilidad en los datos o restricciones de tamaño no conocidas de antemano.
Además, las tablas permiten apreciar esta versatilidad en detalle, al mostrar cómo se comportan métricas clave como \textit{Precision},
\textit{Recall} y \textit{F1-score} a lo largo de las distintas franjas de porcentaje inicial.


\section{Resultados del algoritmo memético}\label{sec:resultados-algoritmo-memetico}
Finalmente, se evalua el algoritmo memético (\texttt{MA}) (ver \hyperref[sec:algoritmo-memetico]{Sección~\ref*{sec:algoritmo-memetico}}),
el cual combina la evolución genética con una búsqueda local aplicada de forma probabilística sobre ciertos individuos seleccionados.
Este enfoque híbrido busca equilibrar la exploración del espacio de soluciones con una intensificación localizada,
ofreciendo mejoras tanto en precisión como en estabilidad.


\begin{figure}[htp]
    \centering
    \includegraphics[width=0.9\textwidth]{imagenes/evaluaciones/comparacion-ma.png}
    \caption{Comparación de \textit{accuracy} entre el algoritmo memético (\texttt{MA}) y su versión libre.}
    \label{fig:memetico_comparacion}
\end{figure}

Tal como se aprecia en la Figura~\ref{fig:memetico_comparacion}, el algoritmo memético (\texttt{MA}) supera claramente al mejor de los enfoques genéticos
(\texttt{GA-AM}), alcanzando una mediana más elevada y un valor máximo de \textit{accuracy} de hasta \textbf{0.903}.
Su distribución es más compacta, con menor dispersión hacia los valores bajos, lo que refleja una mayor consistencia entre ejecuciones.

La versión libre del memético, que incorpora también un ajuste dinámico del tamaño del subconjunto
(ver \hyperref[subsec:memetico-libre]{Apartado~\ref*{subsec:memetico-libre}}), muestra un rendimiento muy similar al estándar,
con una ligera reducción en el valor máximo pero una estabilidad comparable.
Esto sugiere que el componente adaptativo no penaliza la calidad de las soluciones y puede incluso aportar mayor flexibilidad en entornos más inciertos.


\begin{table}[htp]
    \centering
    \resizebox{\textwidth}{!}{
        \begin{tabular}{P{2.2cm} P{2.2cm} P{2.2cm} P{2.2cm} P{2.2cm} P{2.2cm} P{2cm} P{2.5cm} P{2.5cm}}
            \toprule
            \textbf{Algoritmo}    & \textbf{Duración Total} & \textbf{Duración por Eval.} & \textbf{Porc.
            Final}                & \textbf{Accuracy (Avg)} & \textbf{Precision (Avg)}    &
            \textbf{Recall (Avg)} & \textbf{F1-score (Avg)} & \textbf{Evaluaciones}                                                                     \\
            \midrule
            \multicolumn{9}{l}{\textbf{10\%}}                                                                                                           \\
            \midrule
            GA-AM-F               & 00:34:33                & 00:00:21                    & 14,94\%       & 82,96\% & 84,07\% & 82,96\% & 82,12\% & 100 \\
            MA                    & 00:27:43                & 00:00:17                    & 10,00\%       & 82,85\% & 84,44\% & 82,85\% & 81,91\% & 100 \\
            MA-F                  & 00:32:39                & 00:00:20                    & 9,62\%        & 83,28\% & 84,70\% & 83,28\% & 82,46\% & 100 \\
            \midrule
            \multicolumn{9}{l}{\textbf{25\%}}                                                                                                           \\
            \midrule
            GA-AM-F               & 01:07:13                & 00:00:40                    & 26,68\%       & 81,77\% & 82,66\% & 81,77\% & 81,18\% & 100 \\
            MA                    & 00:57:12                & 00:00:34                    & 25,00\%       & 81,99\% & 83,11\% & 81,99\% & 81,21\% & 100 \\
            MA-F                  & 01:08:54                & 00:00:41                    & 27,18\%       & 82,80\% & 83,77\% & 82,80\% & 82,17\% & 100 \\
            \midrule
            \multicolumn{9}{l}{\textbf{50\%}}                                                                                                           \\
            \midrule
            GA-AM-F               & 01:51:24                & 00:01:07                    & 46,36\%       & 83,60\% & 85,22\% & 83,60\% & 83,03\% & 100 \\
            MA                    & 01:47:32                & 00:01:05                    & 50,00\%       & 81,51\% & 83,36\% & 81,51\% & 80,89\% & 100 \\
            MA-F                  & 02:16:51                & 00:01:22                    & 67,07\%       & 81,61\% & 83,09\% & 81,61\% & 80,96\% & 100 \\
            \midrule
            \multicolumn{9}{l}{\textbf{75\%}}                                                                                                           \\
            \midrule
            GA-AM-F               & 02:33:25                & 00:01:32                    & 72,72\%       & 82,20\% & 83,94\% & 82,20\% & 81,47\% & 100 \\
            MA                    & 02:37:43                & 00:01:35                    & 75,00\%       & 82,74\% & 84,33\% & 82,74\% & 82,14\% & 100 \\
            MA-F                  & 03:08:21                & 00:01:53                    & 71,16\%       & 82,69\% & 84,18\% & 82,69\% & 82,10\% & 100 \\
            \midrule
            \multicolumn{9}{l}{\textbf{100\%}}                                                                                                          \\
            \midrule
            100\%                 & --                      & 00:03:16                    & 100,00\%      & 78,60\% & 81,55\% & 78,60\% & 77,68\% & 1   \\
            \bottomrule
        \end{tabular}
    }
    \caption{Resultados de los \texttt{MA} y del \texttt{GA-AM-F} por porcentaje inicial.}
    \label{tab:resultados-memetico-genetico-libres}
\end{table}

El análisis detallado de la Tabla~\ref{tab:resultados-memetico-genetico-libres} confirma estas observaciones y permite extraer
conclusiones más matizadas sobre el comportamiento de los algoritmos.
En primer lugar, el \texttt{MA-F} mantiene una precisión media comparable o superior a la del \texttt{MA} en la mayoría de los escenarios,
especialmente en configuraciones con porcentajes iniciales más bajos (10\% y 25\%).
Esto indica que la capacidad de ajuste dinámico no solo no perjudica el rendimiento, sino que puede ofrecer ventajas en términos de adaptabilidad,
permitiendo al algoritmo optimizar la selección de datos sin depender de un tamaño fijo preestablecido.

Además, el porcentaje final alcanzado por \texttt{MA-F} muestra una notable variabilidad según el escenario:
en los casos con menor porcentaje inicial, tiende a mantenerse cercano al valor de partida (por ejemplo, un 9,62\% en el escenario del 10\%),
mientras que en configuraciones más amplias (50\% y 75\%), el tamaño final puede incrementarse significativamente (hasta un 67,07\% en el caso del 50\%).
Este comportamiento adaptativo refleja que el algoritmo es capaz de ajustar la escala de la solución en función de las características del espacio de búsqueda,
evitando tanto la sobrerrepresentación como la selección excesiva de datos innecesarios.

Por otro lado, la duración total y el tiempo medio por evaluación aumentan con el porcentaje inicial, pero esto no compromete la eficiencia general del \texttt{MA-F},
que mantiene un equilibrio adecuado entre precisión y coste computacional

De esta forma, tanto el \texttt{MA} como su variante libre, \texttt{MA-F}, se consolidan como las estrategias más eficaces del estudio,
ofreciendo una combinación robusta de rendimiento, adaptabilidad y estabilidad.


\section{Resultados finales entre enfoques}\label{sec:comparacion-final-enfoques}
Tras analizar el rendimiento individual de cada enfoque a lo largo de las secciones anteriores,
en esta sección se realiza una síntesis comparativa entre los principales algoritmos desarrollados: el algoritmo genético con cruce ponderado (\texttt{GA-WC}), el algoritmo genético con mutación adaptativa (\texttt{GA-AM}), el algoritmo memético (\texttt{MA}) y su versión libre (\texttt{MA-F}).
Se incluyen además las referencias al 100\% del conjunto de datos y a la selección aleatoria (\texttt{RS})
como líneas base para contextualizar los resultados.

El objetivo es identificar cuál de estos enfoques logra el mejor compromiso entre precisión (\textit{accuracy}),
estabilidad de resultados, y eficiencia en la reducción de datos, así como validar la hipótesis de que una selección inteligente de ejemplos puede superar incluso al uso del 100\% del conjunto de datos.

\subsection{Análisis comparativo de accuracy}\label{sec:comparacion-final-accuracy}
\begin{figure}[htp]
    \centering
    \includegraphics[width=0.95\textwidth]{imagenes/evaluaciones/final/boxplot-por-algoritmo.png}
    \caption{Boxplot de \textit{accuracy} para los algoritmos \texttt{RS}, \texttt{GA-WC}, \texttt{GA-AM}, \texttt{GA-AM-F}, \texttt{MA} y \texttt{MA-F}.}
    \label{fig:boxplot-comparacion-final}
\end{figure}
El análisis comparativo de \textit{accuracy} entre algoritmos confirma de manera general las conclusiones obtenidas en los análisis de las secciones anteriores.
Como muestra el boxplot~\ref{fig:boxplot-comparacion-final}, los algoritmos meméticos, especialmente la versión libre \texttt{MA-F},
presentan las mejores métricas globales en términos de precisión y estabilidad.
\texttt{MA-F} alcanza las medianas más altas y muestra una distribución más compacta, lo que refleja una mayor consistencia entre ejecuciones.
Además, la dispersión de resultados en \texttt{MA-F} es menor, lo que sugiere una robustez notable frente a la aleatoriedad de las ejecuciones y una mayor fiabilidad en su rendimiento.

Por otro lado, el algoritmo memético estándar \texttt{MA} también destaca por su rendimiento superior,
aunque presenta una ligera mayor variabilidad en algunos casos.
Aun así, se posiciona como una solución altamente efectiva,
confirmando que la combinación de evolución global y búsqueda local permite generar subconjuntos más representativos y eficientes.

En contraste, los algoritmos \texttt{RS} y \texttt{GA-WC} muestran una mayor dispersión en los resultados,
lo que indica una menor estabilidad y una mayor dependencia de la aleatoriedad en la selección de datos.
\texttt{RS}, como se ha observado a lo largo de todo el análisis, tiende a obtener resultados más erráticos, mientras que \texttt{GA-WC},
aunque introduce mejoras respecto al \texttt{RS}, sigue sin alcanzar la solidez de los algoritmos meméticos.

Por su parte, el algoritmo \texttt{GA-AM} presenta un rendimiento intermedio: mejora las métricas obtenidas por \texttt{RS} y \texttt{GA-WC},
pero no logra superar la precisión ni la estabilidad de los algoritmos meméticos.
Esto refuerza la hipótesis planteada a lo largo del análisis de los resultados: los enfoques basados en algoritmos meméticos,
especialmente en su versión libre, permiten alcanzar una combinación óptima de precisión,
reducción de datos y estabilidad en problemas de selección de instancias para modelos de aprendizaje profundo.

\subsection{Porcentaje final de datos seleccionados}\label{sec:porcentaje-final-datos-seleccionados}
\begin{figure}[htp]
    \centering
    \includegraphics[width=0.95\textwidth]{imagenes/evaluaciones/final/barplot-por-porcentaje.png}
    \caption{Porcentaje final de datos seleccionados por cada algoritmo.}
    \label{fig:barplot-por-porcentaje}
\end{figure}
\begin{figure}[htp]
    \centering
    \includegraphics[width=0.95\textwidth]{imagenes/evaluaciones/final/scatter.png}
    \caption{Relación entre \textit{accuracy} y porcentaje final de datos seleccionados por cada algoritmo.}
    \label{fig:scatter-final}
\end{figure}El análisis del porcentaje final de datos seleccionados permite evaluar la eficiencia de cada algoritmo,
entendida como la capacidad para mantener una precisión elevada reduciendo el volumen de datos utilizados.
Como se observa en la Figura~\ref{fig:barplot-por-porcentaje}, los algoritmos meméticos, y en particular \texttt{MA-F},
logran mantener o incluso reducir de forma significativa el tamaño del subconjunto de entrenamiento en comparación con
los enfoques basados en algoritmos genéticos como \texttt{GA-WC} y \texttt{GA-AM}.

En concreto, \texttt{MA-F} destaca por su capacidad para obtener subconjuntos más compactos sin comprometer la calidad del modelo,
seleccionando en promedio un 70\% o menos del conjunto original de datos.
Este comportamiento es consistente a lo largo de las distintas configuraciones,
y sugiere que \texttt{MA-F} es capaz de adaptarse dinámicamente al tamaño de la solución en función de la complejidad del problema.
Esta flexibilidad resulta especialmente valiosa, ya que permite optimizar el uso de recursos computacionales sin sacrificar rendimiento.

Por otro lado, los algoritmos \texttt{GA-WC} y \texttt{GA-AM} requieren porcentajes más elevados para alcanzar resultados competitivos,
lo que refleja una menor eficiencia en la reducción de datos.
El algoritmo \texttt{RS}, al no aplicar estrategias de optimización específicas,
presenta una mayor dispersión y una dependencia directa de la aleatoriedad en la selección de instancias.

La Figura~\ref{fig:scatter-final} complementa este análisis mostrando la relación entre \textit{accuracy} y el porcentaje final de datos seleccionados.
Se observa que \texttt{MA} y \texttt{MA-F} consiguen altos valores de \textit{accuracy} utilizando porcentajes de datos más reducidos,
validando la hipótesis de que es posible mejorar la calidad del modelo mediante una selección optimizada.
En contraste, los enfoques menos sofisticados requieren porcentajes más altos para alcanzar precisiones similares,
lo que demuestra la ventaja competitiva de los algoritmos meméticos.

\subsection{Estabilidad en la distribución de clases}\label{sec:distribucion-clases-final}
\begin{figure}[htp]
    \centering
    \includegraphics[width=0.95\textwidth]{imagenes/evaluaciones/final/distribucion-de-clases.png}
    \caption{Distribución de clases seleccionadas por cada algoritmo en el conjunto reducido final.}
    \label{fig:distribucion-de-clases-final}
\end{figure}

El análisis de la distribución de clases en los subconjuntos finales permite evaluar si los algoritmos de reducción mantienen
un equilibrio adecuado entre las diferentes clases presentes en el dataset original.
Como se observa en la Figura~\ref{fig:distribucion-de-clases-final}, todos los algoritmos tienden a preservar un reparto
razonablemente balanceado entre las clases \textit{Paper}, \textit{Rock} y \textit{Scissors}, con porcentajes cercanos al 33\%.

Sin embargo, se aprecian ligeras variaciones en la estabilidad de la distribución según el algoritmo utilizado.
Los algoritmos \texttt{MA} y \texttt{MA-F} presentan una mayor consistencia en el reparto de clases,
mostrando menores desviaciones y barras de error más reducidas.
Esto indica que las soluciones generadas por los enfoques meméticos tienden a conservar de forma más homogénea la diversidad del conjunto original,
evitando la infrarepresentación o sobrecarga de clases específicas.

En contraste, los algoritmos \texttt{RS}, \texttt{GA-WC} y \texttt{GA-AM} muestran una mayor dispersión en los porcentajes de las clases,
especialmente en el caso de \texttt{RS}, lo que refleja una mayor influencia de la aleatoriedad y una menor capacidad para garantizar la estabilidad en la distribución.
Esta variabilidad podría generar sesgos no deseados en el modelo final, afectando su rendimiento en tareas de clasificación.

En resumen, los resultados confirman que los algoritmos meméticos, y en particular \texttt{MA-F},
no solo son eficaces en términos de precisión y reducción de datos, sino que también logran mantener una distribución de clases equilibrada.
Esta propiedad es fundamental para asegurar la generalización del modelo, ya que evita la introducción de desequilibrios que podrían comprometer la calidad del aprendizaje.

\subsection{Síntesis final}\label{sec:sintesis-final}
\begin{table}[htp]
    \centering
    \resizebox{\textwidth}{!}{
        \begin{tabular}{P{2.2cm} P{2cm} P{2cm} P{2cm} P{2cm} P{2cm} P{2cm} P{2cm} P{2cm} P{2.5cm}}
            \toprule
            \textbf{Algoritmo} & \textbf{Duración Total} & \textbf{Duración por Eval.} & \textbf{Accuracy (Avg)} & \textbf{Precision (Avg)} & \textbf{Recall (Avg)} & \textbf{F1-score (Avg)} & \textbf{Evaluaciones} & \textbf{Porc.
            Final}                                                                                                                                                                                                                    \\
            \midrule
            \multicolumn{9}{l}{\textbf{10\%}}                                                                                                                                                                                         \\
            \midrule
            RS                 & 00:38:17                & 00:00:22                    & 81,34\%                 & 82,46\%                  & 81,34\%               & 80,52\%                 & 100                   & 10,00\%       \\
            GA-WC              & 00:34:03                & 00:00:21                    & 83,60\%                 & 84,38\%                  & 83,60\%               & 82,82\%                 & 100                   & 10,00\%       \\
            GA-AM              & 00:27:39                & 00:00:17                    & 83,66\%                 & 84,24\%                  & 83,66\%               & 83,09\%                 & 100                   & 10,00\%       \\
            GA-AM-F            & 00:34:32                & 00:00:21                    & 82,96\%                 & 84,07\%                  & 82,96\%               & 82,12\%                 & 100                   & 14,94\%       \\
            MA                 & 00:27:43                & 00:00:17                    & 82,85\%                 & 84,44\%                  & 82,85\%               & 81,91\%                 & 100                   & 10,00\%       \\
            MA-F               & 00:32:39                & 00:00:20                    & 83,28\%                 & 84,70\%                  & 83,28\%               & 82,46\%                 & 100                   & 9,62\%        \\
            \midrule
            \multicolumn{9}{l}{\textbf{25\%}}                                                                                                                                                                                         \\
            \midrule
            RS                 & 01:18:22                & 00:00:47                    & 80,70\%                 & 81,90\%                  & 80,70\%               & 79,90\%                 & 100                   & 25,00\%       \\
            GA-WC              & 01:10:21                & 00:00:43                    & 82,37\%                 & 83,86\%                  & 82,37\%               & 81,57\%                 & 100                   & 25,00\%       \\
            GA-AM              & 00:57:29                & 00:00:34                    & 82,42\%                 & 83,68\%                  & 82,42\%               & 81,78\%                 & 100                   & 25,00\%       \\
            GA-AM-F            & 01:07:13                & 00:00:40                    & 81,77\%                 & 82,66\%                  & 81,77\%               & 81,18\%                 & 100                   & 26,68\%       \\
            MA                 & 00:57:12                & 00:00:34                    & 81,99\%                 & 83,11\%                  & 81,99\%               & 81,21\%                 & 100                   & 25,00\%       \\
            MA-F               & 01:08:54                & 00:00:41                    & 82,80\%                 & 83,77\%                  & 82,80\%               & 82,17\%                 & 100                   & 27,18\%       \\
            \midrule
            \multicolumn{9}{l}{\textbf{50\%}}                                                                                                                                                                                         \\
            \midrule
            RS                 & 02:26:40                & 00:01:28                    & 80,86\%                 & 82,65\%                  & 80,86\%               & 80,22\%                 & 100                   & 50,00\%       \\
            GA-WC              & 02:15:43                & 00:01:21                    & 81,61\%                 & 83,78\%                  & 81,61\%               & 81,06\%                 & 100                   & 50,00\%       \\
            GA-AM              & 01:47:31                & 00:01:05                    & 81,13\%                 & 82,62\%                  & 81,13\%               & 80,41\%                 & 100                   & 50,00\%       \\
            GA-AM-F            & 01:51:24                & 00:01:07                    & 83,60\%                 & 85,22\%                  & 83,60\%               & 83,03\%                 & 100                   & 46,36\%       \\
            MA                 & 01:47:32                & 00:01:05                    & 81,51\%                 & 83,36\%                  & 81,51\%               & 80,89\%                 & 100                   & 50,00\%       \\
            MA-F               & 02:16:51                & 00:01:22                    & 81,61\%                 & 83,09\%                  & 81,61\%               & 80,96\%                 & 100                   & 67,07\%       \\
            \midrule
            \multicolumn{9}{l}{\textbf{75\%}}                                                                                                                                                                                         \\
            \midrule
            RS                 & 03:05:49                & 00:01:51                    & 82,26\%                 & 84,16\%                  & 82,26\%               & 81,67\%                 & 100                   & 75,00\%       \\
            GA-WC              & 03:33:06                & 00:02:08                    & 82,31\%                 & 83,91\%                  & 82,31\%               & 81,59\%                 & 100                   & 75,00\%       \\
            GA-AM              & 02:36:41                & 00:01:34                    & 82,42\%                 & 83,94\%                  & 82,42\%               & 81,85\%                 & 100                   & 75,00\%       \\
            GA-AM-F            & 02:33:25                & 00:01:32                    & 82,20\%                 & 83,94\%                  & 82,20\%               & 81,47\%                 & 100                   & 72,72\%       \\
            MA                 & 02:37:43                & 00:01:35                    & 82,74\%                 & 84,33\%                  & 82,74\%               & 82,14\%                 & 100                   & 75,00\%       \\
            MA-F               & 03:08:21                & 00:01:53                    & 82,69\%                 & 84,18\%                  & 82,69\%               & 82,10\%                 & 100                   & 71,16\%       \\
            \midrule
            \multicolumn{9}{l}{\textbf{100\%}}                                                                                                                                                                                        \\
            \midrule
            100\%              & --                      & 00:03:15                    & 78,60\%                 & 81,55\%                  & 78,60\%               & 77,68\%                 & 1                     & 100,00\%      \\
            \bottomrule
        \end{tabular}
    }
    \caption{Resultados detallados por porcentaje inicial.}
    \label{tab:resultados-generales-por-porcentaje-inicial}
\end{table}

Los resultados globales obtenidos para el conjunto de datos Rock, Paper, Scissors (véase Tabla~\ref{tab:resultados-generales-por-porcentaje-inicial})
permiten extraer conclusiones claras sobre el rendimiento de los algoritmos evaluados.

En primer lugar, el \texttt{MA-F} se consolida como la solución más robusta y eficaz: no solo alcanza la mayor precisión media en múltiples porcentajes iniciales,
sino que también logra mantener un porcentaje reducido de datos seleccionados, optimizando la eficiencia sin comprometer el rendimiento.
Además, \texttt{MA-F} presenta una notable estabilidad entre ejecuciones,
reflejada en su baja dispersión y en la conservación de una distribución equilibrada de clases en los subconjuntos generados.
Estos resultados demuestran su capacidad para superar incluso a la referencia del 100\% de datos, logrando un mejor rendimiento con menos instancias.

El algoritmo \texttt{GA-AM-F}, incorporado como mejora del genético adaptativo, muestra una evolución destacable.
Aunque en algunos escenarios no supera completamente al enfoque memético, sí logra mejores precisiones que otros algoritmos genéticos,
manteniendo además un porcentaje de datos intermedio, lo cual refuerza su potencial como alternativa más estable y eficaz dentro del grupo genético.

El enfoque \texttt{MA} también ofrece resultados sobresalientes, manteniendo altos niveles de precisión y una considerable reducción de datos.
Sin embargo, presenta una ligera mayor variabilidad en comparación con \texttt{MA-F}, lo que sugiere una estabilidad algo inferior en ciertos escenarios.

En cuanto a \texttt{GA-AM}, aunque mejora claramente respecto a \texttt{RS} y \texttt{GA-WC}, no alcanza los niveles de rendimiento ni estabilidad de los enfoques meméticos.
El algoritmo \texttt{GA-WC}, por su parte, obtiene ligeras mejoras frente a la selección aleatoria,
pero requiere un mayor porcentaje de datos para lograr precisión competitiva, y sufre mayor dispersión entre ejecuciones.

Finalmente, la referencia aleatoria \texttt{RS} queda superada por todos los enfoques evolutivos y meméticos.
Sus resultados muestran una alta variabilidad, menor precisión global y una propensión a generar subconjuntos con distribuciones de clases desbalanceadas,
evidenciando la importancia de emplear estrategias de selección informadas.

Además de los análisis cuantitativos, las Figuras~\ref{fig:barplot-por-pi-10},~\ref{fig:barplot-por-pi-25},~\ref{fig:barplot-por-pi-50},~\ref{fig:barplot-por-pi-75} y~\ref{fig:barplot-por-pi-100}
refuerzan visualmente estas conclusiones mediante barplots comparativos para cada porcentaje inicial.
Estos gráficos permiten observar cómo varía el rendimiento de cada algoritmo manteniendo fijo el punto de partida,
destacando especialmente la superioridad de los enfoques meméticos (\texttt{MA}, \texttt{MA-F}) y la mejora progresiva introducida por \texttt{GA-AM-F}.

\begin{figure}[htp]
    \centering
    \includegraphics[width=0.7\textwidth]{imagenes/evaluaciones/final/barplot-por-pi/pi-10.png}
    \caption{Comparativa de \textit{accuracy} para cada algoritmo con porcentaje inicial \texttt{10\%}.}
    \label{fig:barplot-por-pi-10}
\end{figure}

\begin{figure}[htp]
    \centering
    \includegraphics[width=0.7\textwidth]{imagenes/evaluaciones/final/barplot-por-pi/pi-25.png}
    \caption{Comparativa de \textit{accuracy} para cada algoritmo con porcentaje inicial \texttt{25\%}.}
    \label{fig:barplot-por-pi-25}
\end{figure}

\begin{figure}[htp]
    \centering
    \includegraphics[width=0.7\textwidth]{imagenes/evaluaciones/final/barplot-por-pi/pi-50.png}
    \caption{Comparativa de \textit{accuracy} para cada algoritmo con porcentaje inicial \texttt{50\%}.}
    \label{fig:barplot-por-pi-50}
\end{figure}

\begin{figure}[htp]
    \centering
    \includegraphics[width=0.7\textwidth]{imagenes/evaluaciones/final/barplot-por-pi/pi-75.png}
    \caption{Comparativa de \textit{accuracy} para cada algoritmo con porcentaje inicial \texttt{75\%}.}
    \label{fig:barplot-por-pi-75}
\end{figure}

\begin{figure}[htp]
    \centering
    \includegraphics[width=0.7\textwidth]{imagenes/evaluaciones/final/barplot-por-pi/pi-100.png}
    \caption{Comparativa de \textit{accuracy} para cada algoritmo con porcentaje inicial \texttt{100\%}.}
    \label{fig:barplot-por-pi-100}
\end{figure}

En conjunto, este análisis confirma que los algoritmos meméticos, especialmente en su versión libre (\texttt{MA-F}),
representan la mejor alternativa para abordar la selección de subconjuntos de datos en tareas de aprendizaje profundo.
Logran un equilibrio óptimo entre precisión, eficiencia y estabilidad, superando ampliamente a las técnicas basadas en selección aleatoria o enfoques genéticos más sencillos.


\section{Validación con el dataset \texttt{PAINTING}}\label{sec:validacion-con-painting}
Para validar la robustez de los algoritmos propuestos, se emplea el dataset \texttt{PAINTING},
caracterizado por una elevada complejidad visual y un número superior de clases respecto al conjunto \texttt{RPS}.
Los experimentos se realizan con los porcentajes iniciales del 25\%, 50\% y 75\%, utilizando los algoritmos \texttt{MA} y \texttt{MA-F}, identificados previamente como los más efectivos.
Como referencia comparativa, se incluyen también resultados con el 100\% del dataset y la \texttt{RS}.

\subsection{Comparación entre algoritmos}

\begin{figure}[htp]
    \centering
    \includegraphics[width=1\textwidth]{imagenes/evaluaciones/painting/comparacion-por-algoritmo.png}
    \caption{Boxplot comparando resultados con el dataset \texttt{PAINTING} usando \textit{accuracy}.}
    \label{fig:comparacion-por-algoritmo}
\end{figure}

En la Figura~\ref{fig:comparacion-por-algoritmo} se muestran los valores de \textit{accuracy} obtenidos por cada algoritmo.
El algoritmo \texttt{MA-F} sigue destacando, alcanzando una mediana de \textbf{0.933} cuando se parte del 50\% de los datos,
superando al modelo entrenado con el 100\% del conjunto (mediana \textbf{0.924}).
El algoritmo \texttt{MA} obtiene también resultados sólidos, especialmente con el 75\%.

La \texttt{RS} presenta de nuevo mayor variabilidad, y aunque alcanza un valor máximo competitivo, su rendimiento es menos estable.
Estos resultados refuerzan que la representatividad del subconjunto es un factor más relevante que su tamaño absoluto.

\subsection{Evolución del tamaño del subconjunto}

\begin{figure}[htp]
    \centering
    \includegraphics[width=0.9\textwidth]{imagenes/evaluaciones/painting/scatter-por-porcentaje.png}
    \caption{Comparacion del tamaño del subconjunto seleccionado por los \texttt{MA} y \texttt{RS} según el porcentaje inicial y mostrando \texttt{accuracy}.}
    \label{fig:scatter-painting}
\end{figure}

La Figura~\ref{fig:scatter-painting} muestra cómo evoluciona dinámicamente el tamaño final de los subconjuntos generados por \texttt{MA-F}.
El algoritmo incrementa el número de instancias cuando detecta beneficios significativos en la precisión, siendo más agresivo con puntos de partida bajos.
Por ejemplo, al iniciar con el 25\% y 50\%, el tamaño final aumenta hasta un 41,35\% y 68,69\% respectivamente.
En cambio, con un 75\% inicial, el incremento es más moderado, alcanzando un 78,29\%.

\subsection{Impacto del porcentaje inicial}

\begin{table}[htp]
    \centering
    \resizebox{\textwidth}{!}{
        \begin{tabular}{P{2.5cm} P{2cm} P{2.5cm} P{2.5cm} P{2.5cm} P{2.5cm} P{2cm}}
            \toprule
            \textbf{Algoritmo} & \textbf{Accuracy (Avg)} & \textbf{Precision (Avg)} & \textbf{Recall (Avg)} & \textbf{F1-score (Avg)} & \textbf{Evaluaciones} & \textbf{Porc.
            Final}                                                                                                                                                            \\
            \midrule
            \multicolumn{7}{l}{\textbf{25\%}}                                                                                                                                 \\
            \midrule
            RS                 & 91,80\%                 & 91,49\%                  & 91,80\%               & 91,57\%                 & 100                   & 25,00\%       \\
            MA                 & 91,89\%                 & 91,63\%                  & 91,89\%               & 91,67\%                 & 100                   & 25,00\%       \\
            MA-F               & 92,24\%                 & 92,06\%                  & 92,24\%               & 92,09\%                 & 100                   & 41,35\%       \\
            \midrule
            \multicolumn{7}{l}{\textbf{50\%}}                                                                                                                                 \\
            \midrule
            RS                 & 92,69\%                 & 92,52\%                  & 92,69\%               & 92,55\%                 & 100                   & 50,00\%       \\
            MA                 & 92,73\%                 & 92,61\%                  & 92,73\%               & 92,63\%                 & 100                   & 50,00\%       \\
            MA-F               & 93,11\%                 & 93,00\%                  & 93,11\%               & 93,01\%                 & 100                   & 68,69\%       \\
            \midrule
            \multicolumn{7}{l}{\textbf{75\%}}                                                                                                                                 \\
            \midrule
            RS                 & 93,14\%                 & 92,97\%                  & 93,14\%               & 93,01\%                 & 100                   & 75,00\%       \\
            MA                 & 93,11\%                 & 92,93\%                  & 93,11\%               & 92,99\%                 & 100                   & 75,29\%       \\
            MA-F               & 93,02\%                 & 92,88\%                  & 93,02\%               & 92,92\%                 & 100                   & 78,29\%       \\
            \midrule
            \multicolumn{7}{l}{\textbf{100\%}}                                                                                                                                \\
            \midrule
            100\%              & 92,24\%                 & 92,05\%                  & 92,24\%               & 92,06\%                 & 1                     & 100,00\%      \\
            \bottomrule
        \end{tabular}
    }
    \caption{Resultados de los algoritmos meméticos, aleatorio y uso del 100\% en el dataset \texttt{PAINTING}.}
    \label{tab:resultados-memetico-painting}
\end{table}

\begin{figure}[htp]
    \centering
    \includegraphics[width=1\textwidth]{imagenes/evaluaciones/painting/comparacion-por-porcentaje.png}
    \caption{Diagrama de barras de \textit{accuracy} según el porcentaje inicial de datos del \texttt{MA}, \texttt{MA-F} y el 100\%.}
    \label{fig:accuracy_porcentaje_painting}
\end{figure}

Los resultados de las Figuras~\ref{fig:accuracy_porcentaje_painting} y~\ref{fig:scatter-painting}, junto con la Tabla~\ref{tab:resultados-memetico-painting},
refuerzan la conclusión de que no es necesario utilizar el 100\% del conjunto para lograr rendimientos óptimos.
Los porcentajes iniciales del 50\% y 75\% muestran una excelente capacidad para generalizar,
superando al conjunto completo en estabilidad y máxima precisión alcanzada (\textbf{0.933}).
El 25\% inicial, si bien logra resultados competitivos, evidencia una mayor sensibilidad a la variabilidad del subconjunto,
una tendencia que también se manifestó en los resultados del dataset \texttt{RPS} cuando se empleaban subconjuntos muy reducidos.


\subsection{Distribución y equilibrio de clases}

\begin{figure}[htp]
    \centering
    \includegraphics[width=0.9\textwidth]{imagenes/evaluaciones/painting/balance-de-clases-por-algoritmo.png}
    \caption{Distribución de clases seleccionadas por cada algoritmo.}
    \label{fig:balance_clases_painting}
\end{figure}

La Figura~\ref{fig:balance_clases_painting} muestra cómo se preserva la proporción entre clases,
un aspecto que también se respetó en experimentos anteriores.
La capacidad de mantener esta diversidad es clave para evitar sesgos de entrenamiento,
como se discutió al comparar los métodos evolutivos frente a selección aleatoria.

\subsection{Síntesis final}
La validación con el dataset \texttt{PAINTING} confirma la efectividad y generalización de los algoritmos meméticos, especialmente la variante libre.
Los resultados subrayan que seleccionar datos representativos es más beneficioso que usar el conjunto completo, ofreciendo precisión elevada con menor coste computacional.

Este análisis complementa y confirma las conclusiones obtenidas con \texttt{RPS}, incluso en un entorno más complejo y multiclase,
demuestra que el enfoque evolutivo no solo es eficaz, sino también robusto y transferible,
reforzando el valor práctico de los enfoques evolutivos propuestos.


\section{Validación con el dataset \texttt{CIFAR10}}
Esta sección tiene como objetivo comprobar si las conclusiones obtenidas en los apartados anteriores,
especialmente sobre el comportamiento de los algoritmos meméticos y las versiones libres, se mantienen en un entorno más desafiante como \texttt{CIFAR10}.
A diferencia de los datasets anteriores, este conjunto presenta una mayor complejidad visual,
baja resolución y una fuerte variabilidad intraclase, lo que lo convierte en un entorno ideal para validar la generalización y robustez de las soluciones propuestas.
Asimismo, se busca identificar posibles contradicciones o limitaciones que pudieran no haber sido evidentes en entornos más simples.

Para ello, se evalúan los algoritmos que han mostrado mejor rendimiento hasta ahora (\texttt{GA-AM}, \texttt{GA-AM-F}, \texttt{MA}, \texttt{MA-F})
y se incluye como referencia el modelo entrenado con el \texttt{100\%} del conjunto de datos.

En este caso, se han realizado únicamente 4 ejecuciones por configuración en lugar de las 5 habituales en el resto del proyecto,
debido a una nueva limitación impuesta en el servidor utilizado, su elevada saturación y la falta de tiempo disponible para completar todas las evaluaciones con un dataset de este tamaño.

\subsection{Evaluación general del rendimiento}
La Figura~\ref{fig:cifar10_boxplot} presenta la distribución de precisión obtenida por cada algoritmo.
Las cuatro variantes superan la mediana del modelo de referencia (\textbf{0.778}), confirmando que una selección informada puede ser más eficaz que el uso completo e indiscriminado de datos.
Destacan \texttt{GA-AM-F} y \texttt{MA-F}, que alcanzan máximos de \textbf{0.805} y \textbf{0.808}, respectivamente.

Más allá de la precisión máxima, se observa que las versiones libres reducen la dispersión entre ejecuciones, reforzando su estabilidad en condiciones más adversas.
Este comportamiento respalda lo ya observado en los datasets \texttt{RPS} y \texttt{PAINTING}.

\begin{figure}[htp]
    \centering
    \includegraphics[width=0.9\textwidth]{imagenes/evaluaciones/cifar10/boxplot-por-algoritmo.png}
    \caption{Distribución de \texttt{accuracy} obtenida para cada algoritmo sobre \texttt{CIFAR10}.}
    \label{fig:cifar10_boxplot}
\end{figure}

\subsection{Comparativa con las versiones libres}
La Figura~\ref{fig:cifar10_fija_vs_libre} compara directamente las versiones \texttt{FIJA} y \texttt{LIBRE} para los algoritmos \texttt{GA-AM} y \texttt{MA}.
Al igual que en datasets anteriores, las versiones libres muestran una ligera mejora en precisión media y una reducción de la varianza.

No obstante, en este entorno más exigente, la diferencia entre versiones fijas y libres se reduce,
lo que sugiere que la ventaja del ajuste dinámico del tamaño del subconjunto puede verse atenuada por la complejidad del conjunto de datos.
Aun así, las versiones libres mantienen una ligera superioridad, lo que valida su utilidad incluso en escenarios de mayor dificultad.

\begin{figure}[htp]
    \centering
    \includegraphics[width=0.9\textwidth]{imagenes/evaluaciones/cifar10/boxplot-libres.png}
    \caption{Comparativa de precisión entre versiones fijas y libres para los algoritmos evolutivos.}
    \label{fig:cifar10_fija_vs_libre}
\end{figure}

\subsection{Consistencia frente al porcentaje inicial}
La Figura~\ref{fig:cifar10_porcentaje} muestra cómo evoluciona la precisión media en función del porcentaje de datos iniciales.
Se observa que los valores iniciales del 50\% y 75\% destacan de forma consistente, logrando precisiones iguales o superiores al entrenamiento completo.
Este comportamiento reafirma la validez de los resultados obtenidos en \texttt{PAINTING}, donde el 50\% superaba al 100\%,
y sugiere que estos porcentajes representan puntos de partida especialmente eficientes para la selección de subconjuntos informativos.

\begin{figure}[htp]
    \centering
    \includegraphics[width=0.9\textwidth]{imagenes/evaluaciones/cifar10/comparacion_por_porcentaje.png}
    \caption{Precisión media alcanzada según el porcentaje inicial de datos.}
    \label{fig:cifar10_porcentaje}
\end{figure}

\begin{figure}[htp]
    \centering
    \includegraphics[width=0.8\textwidth]{imagenes/evaluaciones/cifar10/scatter.png}
    \caption{Porcentaje inicial vs porcentaje final de datos seleccionados.}
    \label{fig:cifar10_scatter}
\end{figure}

Para comprender mejor el comportamiento de los algoritmos, se muestra la Figura~\ref{fig:cifar10_scatter}.
A partir de esta visualización, se pueden extraer varios patrones relevantes:

\begin{enumerate}
    \item \textbf{Trayectorias eficientes desde valores intermedios.} Los puntos de partida del 50\% y el 75\% no solo conducen a altas precisiones,
          sino que lo hacen manteniendo tamaños de subconjuntos finales relativamente estables, lo que sugiere que permiten al algoritmo trabajar con confianza desde etapas tempranas.
    \item \textbf{Zona de alta precisión estabilizada.} Existe una franja de subconjuntos finales, en torno al 60--80\%,
          donde se concentra la mayor parte de las ejecuciones con mejor precisión.
          Esto sugiere que este rango podría representar un punto de equilibrio entre información útil y redundancia.
    \item \textbf{Tendencia a expandir el subconjunto de datos.} A diferencia de experimentos anteriores,
          donde algunos algoritmos reducían el número de ejemplos seleccionados respecto al conjunto inicial,
          en este caso se observa una inclinación general a ampliar el tamaño del subconjunto final.
          Esta conducta sugiere una estrategia orientada a reforzar la representatividad del conjunto,
          priorizando una cobertura más amplia cuando la información inicial es limitada.
    \item \textbf{Baja rentabilidad en extremos.} Las ejecuciones que parten de 10\% o 100\% (teniendo el contexto de las otras figuras)
          rara vez conducen a los mejores resultados: en el primer caso, la falta de diversidad inicial limita la exploración efectiva; en el segundo,
          la inclusión de datos redundantes parece perjudicar la optimización.
\end{enumerate}

En conjunto, los resultados demuestran que los algoritmos son capaces de ajustar dinámicamente el tamaño de los subconjuntos, maximizando la precisión con una fracción moderada de datos.
Especialmente, los arranques desde el 50\% y 75\% ofrecen un equilibrio favorable entre diversidad y eficiencia,
lo que los convierte en opciones recomendables en escenarios de selección activa de datos.

\subsection{Distribución de clases}
La Figura~\ref{fig:cifar10_balance} presenta la distribución de clases en los subconjuntos generados por los algoritmos.
Todos mantienen una proporción equilibrada, lo que garantiza que las mejoras de rendimiento no provienen de sesgos estructurales, sino de un proceso evolutivo eficaz.

Este equilibrio ya fue observado en \texttt{RPS} y \texttt{PAINTING}, lo que refuerza la consistencia de los algoritmos propuestos en distintos contextos.

\begin{figure}[htp]
    \centering
    \includegraphics[width=0.9\textwidth]{imagenes/evaluaciones/cifar10/balance-de-clases.png}
    \caption{
        Porcentaje de instancias por clase tras aplicar cada algoritmo.
        La distribución permanece prácticamente uniforme.
    }
    \label{fig:cifar10_balance}
\end{figure}

\subsection{Síntesis final de la validación con \texttt{CIFAR10}}
Los resultados obtenidos con \texttt{CIFAR10} permiten validar las principales conclusiones extraídas en las fases anteriores del estudio:

\begin{itemize}
    \item Todos los algoritmos evaluados superan el rendimiento del modelo entrenado con el 100\% del dataset,
          confirmando que una selección óptima puede ser más eficaz que el uso de datos sin filtrar.
    \item Las variantes libres (\texttt{-F}) mantienen su superioridad frente a las versiones fijas, aunque en este caso la ventaja es más moderada.
    \item A partir del 50\% de datos, los algoritmos alcanzan rendimientos muy competitivos, llegando incluso a superar bastante al entrenamiento completo.
    \item La distribución de clases permanece estable en todos los casos, descartando que las mejoras observadas se deban a sesgos de clase.
    \item En contraste con datasets anteriores, en \texttt{CIFAR10} los algoritmos tienden a expandir los subconjuntos seleccionados,
          lo que sugiere una adaptación activa frente a la mayor complejidad visual del entorno.
\end{itemize}

En resumen, \texttt{MA-F} vuelve a consolidarse como la opción más prometedora, combinando precisión elevada,
baja varianza y adaptabilidad ante diferentes porcentajes iniciales.
Además, esta validación refuerza que las conclusiones alcanzadas en entornos más simples son extrapolables a escenarios de mayor dificultad,
demostrando la solidez general del enfoque propuesto.
