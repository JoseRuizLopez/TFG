% !TeX root = ../proyecto.tex

\chapter{Repaso Bibliográfico}\label{ch:repaso-bibliografico}
En este capítulo se revisa la literatura relevante en torno a los modelos de \textbf{aprendizaje profundo}, la
necesidad de reducir los conjuntos de \textbf{datos de entrenamiento} y el rol de los \textbf{algoritmos meméticos} en
esta tarea.
A lo largo de este capítulo, se analizan estudios previos que aplican técnicas de \textbf{optimización} para reducir
los datos necesarios en el entrenamiento de \textbf{redes neuronales convolucionales} (CNN) y se exploran las
soluciones existentes en la selección de instancias mediante algoritmos \textbf{evolutivos} y \textbf{meméticos}.


\section{Reducción de Conjuntos de Datos en Aprendizaje Profundo}
\label{sec:reduccion-de-conjuntos-de-datos-en-aprendizaje-profundo}
El \textbf{aprendizaje profundo}, especialmente en el campo de la \textbf{visión por computadora}, se ha basado en
grandes volúmenes de \textbf{datos} para alcanzar su éxito.
En particular, las redes neuronales convolucionales (CNN) han demostrado un rendimiento notable en tareas de
\textbf{clasificación}, \textbf{detección} y \textbf{segmentación} de imágenes~\cite{goodfellowDeepLearning2016}.
Sin embargo, el volumen de datos necesario para entrenar estos modelos supone retos en cuanto a la disponibilidad de
\textbf{almacenamiento}, el \textbf{tiempo de entrenamiento} y el \textbf{costo computacional}, especialmente en
sistemas con \textbf{recursos limitados}~\cite{lecunDeepLearning2015}.


La necesidad de reducir estos volúmenes de datos sin comprometer la \textbf{precisión} del modelo ha impulsado
investigaciones en torno a técnicas de \textbf{selección de instancias}, donde el objetivo es identificar y mantener
solo las instancias de datos que aportan valor al modelo.
A este respecto, las técnicas de selección de instancias combinadas con estrategias de
\textbf{aumento de datos}~\cite{shortenSurveyImageData2019} permiten reducir el tamaño de los conjuntos de datos
manteniendo una \textbf{diversidad} adecuada, lo cual es crucial para evitar el \textbf{sobreajuste} y la pérdida de
\textbf{generalización} en las redes neuronales.
Además, otros enfoques, como el \textbf{submuestreo de datos} y la generación de \textbf{conjuntos sintéticos}, se han
propuesto como soluciones complementarias en el contexto de aprendizaje profundo, siendo útiles en escenarios con
conjuntos de datos desequilibrados o insuficientes.


\section{Selección de Instancias mediante Algoritmos Evolutivos y Meméticos}
\label{sec:seleccion-de-instancias-mediante-algoritmos-evolutivos-y-memeticos}
La \textbf{selección de instancias} es una técnica de reducción de datos que se centra en eliminar aquellas instancias
\textbf{redundantes} o \textbf{irrelevantes}, mejorando la \textbf{eficiencia} y manteniendo la \textbf{precisión} en
el modelo entrenado.
Los \textbf{algoritmos evolutivos}, como los \textbf{algoritmos genéticos}, han demostrado eficacia en la selección de
instancias al simular procesos de \textbf{selección natural}, con la ventaja de que pueden explorar un espacio de
\textbf{soluciones} de manera eficiente a través de \textbf{operadores genéticos} como la \textbf{selección}, el
\textbf{cruce} y la \textbf{mutación}~\cite{hollandAdaptationNaturalArtificial1975}.


Dentro de este campo, los \textbf{algoritmos meméticos} ofrecen un avance significativo al combinar los principios de
\textbf{optimización evolutiva} con técnicas de \textbf{búsqueda local}, lo que permite una adaptación más precisa de
las instancias seleccionadas~\cite{moscatoEvolutionSearchOptimization2000}.
Estos algoritmos representan un enfoque \textbf{híbrido}, ya que aprovechan la capacidad \textbf{exploratoria} de los
algoritmos evolutivos con la capacidad \textbf{explotatoria} de las búsquedas locales, lo cual es crucial para alcanzar
una \textbf{convergencia óptima}.
Al introducir esta \textbf{dualidad}, los algoritmos meméticos pueden enfocarse en subconjuntos de datos más
representativos y potencialmente relevantes para el modelo, maximizando la reducción sin comprometer la calidad del
aprendizaje.


\section{Aplicación de Algoritmos Meméticos en Modelos Convolucionales}\label{sec:algoritmos-memeticos-en-modelos-convolucionales}
La capacidad de los \textbf{algoritmos meméticos} para optimizar la selección de datos se ha aplicado a
\textbf{modelos convolucionales}, en particular en contextos de redes profundas que requieren grandes cantidades de
datos para alcanzar un rendimiento óptimo~\cite{neriHandbookMemeticAlgorithms2012}.
La selección de instancias mediante algoritmos meméticos permite mantener la precisión del modelo, pero con un volumen
de datos significativamente reducido, lo cual es especialmente útil en aplicaciones donde los recursos computacionales
y el tiempo son
limitados~\cite{dongMemeticAlgorithmEvolving2020}.


Algunos estudios han demostrado que los algoritmos meméticos no solo reducen los tiempos de \textbf{entrenamiento} y el
tamaño de los conjuntos de \textbf{datos}, sino que también ayudan a minimizar el riesgo de \textbf{sobreajuste}.
Al reducir los datos redundantes, el modelo \textbf{convolucional} puede centrarse en \textbf{patrones} más
específicos, potenciando su capacidad de \textbf{generalización} y \textbf{robustez}.
Estas ventajas son especialmente valiosas en escenarios como el \textbf{reconocimiento facial} o la \textbf{medicina},
donde la precisión y la eficiencia en el procesamiento de imágenes son esenciales.


\section{Desafíos y Adaptabilidad de los Algoritmos Meméticos en Aprendizaje Profundo}
\label{sec:desafios-y-adaptabilidad-algoritmos-memeticos}
A pesar de sus beneficios, la aplicación de \textbf{algoritmos meméticos} en modelos de aprendizaje profundo plantea
desafíos.
Uno de los principales es la necesidad de ajustar \textbf{parámetros complejos}, como el tamaño de la
\textbf{población}, las tasas de \textbf{mutación} y los métodos de \textbf{selección} y \textbf{cruce}.
Estos parámetros afectan de manera directa la \textbf{velocidad de convergencia} y la capacidad del algoritmo para
encontrar soluciones \textbf{óptimas}.
Además, los algoritmos meméticos suelen ser computacionalmente \textbf{exigentes}~\cite{goldbergGeneticAlgorithmsSearch1989}.

Para abordar estos desafíos, investigaciones recientes han explorado el desarrollo de
\textbf{variantes de algoritmos meméticos adaptativos}, que ajustan automáticamente sus parámetros en función del
desempeño durante el proceso de \textbf{entrenamiento}~\cite{molinaMASWChainsMemeticAlgorithm2010}.
Un ejemplo destacado es el algoritmo F-MAD, que incorpora lógica difusa para ajustar dinámicamente
parámetros clave como la tasa de cruce y el factor de escala en problemas multiobjetivo~\cite{subburajFuzzySystemBased2025}.
Esta \textbf{adaptabilidad} representa una vía prometedora, ya que permite que los algoritmos se adapten dinámicamente
a los cambios en el entorno de datos y en las necesidades del modelo, facilitando así su implementación en aplicaciones
prácticas.


\section{Perspectivas Futuras en la Optimización de CNN con Algoritmos Meméticos}
\label{sec:perspectivas-futuras-en-la-optimizacion-de-cnn-con-algoritmos-memeticos}
La combinación de \textbf{CNN} y \textbf{algoritmos meméticos} sigue siendo un área emergente y prometedora en la
investigación de \textbf{redes neuronales profundas}.
A medida que la tecnología y la \textbf{capacidad de procesamiento} evolucionan, es probable que los algoritmos
meméticos se integren de manera más fluida en \textbf{arquitecturas de aprendizaje profundo}, no solo para la selección
de instancias, sino también para la \textbf{optimización de hiperparámetros} y \textbf{estructuras de red}.
Asimismo, se anticipa que la investigación futura también explore la combinación de estos métodos con otros enfoques
avanzados de \textbf{reducción de datos}, como las técnicas de \textbf{distilación de modelos} y la
\textbf{poda de redes neuronales}.


En conclusión, este capítulo ha revisado las bases teóricas que sustentan la selección de
\textbf{algoritmos meméticos} para reducir conjuntos de datos en \textbf{redes profundas}, resaltando su relevancia en
la optimización de \textbf{modelos CNN} en entornos de recursos limitados.
