@article{albelwiFrameworkDesigningArchitectures2017,
  title = {A Framework for Designing the Architectures of Deep Convolutional Neural Networks},
  author = {Albelwi, Saleh and Mahmood, Ausif},
  year = {2017},
  month = jun,
  journal = {Entropy},
  volume = {19},
  number = {6},
  pages = {242},
  publisher = {Multidisciplinary Digital Publishing Institute},
  issn = {1099-4300},
  doi = {10.3390/e19060242},
  urldate = {2025-06-14},
  abstract = {Recent advances in Convolutional Neural Networks (CNNs) have obtained promising results in difficult deep learning tasks. However, the success of a CNN depends on finding an architecture to fit a given problem. A hand-crafted architecture is a challenging, time-consuming process that requires expert knowledge and effort, due to a large number of architectural design choices. In this article, we present an efficient framework that automatically designs a high-performing CNN architecture for a given problem. In this framework, we introduce a new optimization objective function that combines the error rate and the information learnt by a set of feature maps using deconvolutional networks (deconvnet). The new objective function allows the hyperparameters of the CNN architecture to be optimized in a way that enhances the performance by guiding the CNN through better visualization of learnt features via deconvnet. The actual optimization of the objective function is carried out via the Nelder-Mead Method (NMM). Further, our new objective function results in much faster convergence towards a better architecture. The proposed framework has the ability to explore a CNN architecture's numerous design choices in an efficient way and also allows effective, distributed execution and synchronization via web services. Empirically, we demonstrate that the CNN architecture designed with our approach outperforms several existing approaches in terms of its error rate. Our results are also competitive with state-of-the-art results on the MNIST dataset and perform reasonably against the state-of-the-art results on CIFAR-10 and CIFAR-100 datasets. Our approach has a significant role in increasing the depth, reducing the size of strides, and constraining some convolutional layers not followed by pooling layers in order to find a CNN architecture that produces a high recognition performance.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  langid = {english},
  keywords = {CNN architecture design,convolutional neural networks (CNNs),correlation coefficient (Corr),deconvolutional networks (deconvnet),deep learning,Nelder-Mead method (NMM),objective function},
  note = {\url{https://www.mdpi.com/1099-4300/19/6/242}}
}

@article{alnuaimHumanComputerInteractionHand2022,
  title = {Human-Computer Interaction with Hand Gesture Recognition Using ResNet and MobileNet},
  author = {Alnuaim, Abeer and Zakariah, Mohammed and Hatamleh, Wesam Atef and Tarazi, Hussam and Tripathi, Vikas and Amoatey, Enoch Tetteh},
  year = {2022},
  journal = {Computational Intelligence and Neuroscience},
  volume = {2022},
  number = {1},
  pages = {8777355},
  issn = {1687-5273},
  doi = {10.1155/2022/8777355},
  abstract = {Sign language is the native language of deaf people, which they use in their daily life, and it facilitates the communication process between deaf people. The problem faced by deaf people is targeted using sign language technique. Sign language refers to the use of the arms and hands to communicate, particularly among those who are deaf. This varies depending on the person and the location from which they come. As a result, there is no standardization about the sign language to be used; for example, American, British, Chinese, and Arab sign languages are all distinct. Here, in this study we trained a model, which will be able to classify the Arabic sign language, which consists of 32 Arabic alphabet sign classes. In images, sign language is detected through the pose of the hand. In this study, we proposed a framework, which consists of two CNN models, and each of them is individually trained on the training set. The final predictions of the two models were ensembled to achieve higher results. The dataset used in this study is released in 2019 and is called as ArSL2018. It is launched at the Prince Mohammad Bin Fahd University, Al Khobar, Saudi Arabia. The main contribution in this study is resizing the images to 64 {$\ast$} 64 pixels, converting from grayscale images to three-channel images, and then applying the median filter to the images, which acts as lowpass filtering in order to smooth the images and reduce noise and to make the model more robust to avoid overfitting. Then, the preprocessed image is fed into two different models, which are ResNet50 and MobileNetV2. ResNet50 and MobileNetV2 architectures were implemented together. The results we achieved on the test set for the whole data are with an accuracy of about 97\% after applying many preprocessing techniques and different hyperparameters for each model, and also different data augmentation techniques.},
  langid = {english},
  note = {\url{https://onlinelibrary.wiley.com/doi/abs/10.1155/2022/8777355}}
}

@misc{arazziSecureFederatedData2025,
  title = {Secure Federated Data Distillation},
  author = {Arazzi, Marco and Cihangiroglu, Mert and Nicolazzo, Serena and Nocera, Antonino},
  year = {2025},
  month = mar,
  number = {arXiv:2502.13728},
  eprint = {2502.13728},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2502.13728},
  urldate = {2025-06-14},
  abstract = {Dataset Distillation (DD) is a powerful technique for reducing large datasets into compact, representative synthetic datasets, accelerating Machine Learning training. However, traditional DD methods operate in a centralized manner, which poses significant privacy threats and reduces its applicability. To mitigate these risks, we propose a Secure Federated Data Distillation (SFDD) framework to decentralize the distillation process while preserving privacy. Unlike existing Federated Distillation techniques that focus on training global models with distilled knowledge, our approach aims to produce a distilled dataset without exposing local contributions. We leverage the gradient-matching-based distillation method, adapting it for a distributed setting where clients contribute to the distillation process without sharing raw data. The central aggregator iteratively refines a synthetic dataset by integrating client-side updates while ensuring data confidentiality. To make our approach resilient to inference attacks perpetrated by the server that could exploit gradient updates to reconstruct private data, we create an optimized Local Differential Privacy approach, called LDPO-RLD. Furthermore, we assess the framework's resilience against malicious clients executing backdoor attacks (such as Doorping) and demonstrate robustness under the assumption of a sufficient number of participating clients. Our experimental results demonstrate the effectiveness of SFDD and that the proposed defense concretely mitigates the identified vulnerabilities, with minimal impact on the performance of the distilled dataset. By addressing the interplay between privacy and federation in dataset distillation, this work advances the field of privacy-preserving Machine Learning making our SFDD framework a viable solution for sensitive data-sharing applications.},
  archiveprefix = {arXiv},
  howpublished = {\url{http://arxiv.org/abs/2502.13728}},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Cryptography and Security}
}

@book{chaconProGit2014,
  title = {Pro Git},
  author = {Chacon, Scott and Straub, Ben},
  year = {2014},
  edition = {Second},
  publisher = {Apress},
  address = {New York},
  doi = {10.1007/978-1-4842-0076-6},
  abstract = {Pro Git (Second Edition) is your fully-updated guide to Git and its usage in the modern world. Git has come a long way since it was first developed by Linus Torvalds for Linux kernel development. It has taken the open source world by storm since its inception in 2005, and this book teaches you how to use it like a pro. Effective and well-implemented version control is a necessity for successful web projects, whether large or small. With this book you'll learn how to master the world of distributed version workflow, use the distributed features of Git to the full, and extend Git to meet your every need. Written by Git pros Scott Chacon and Ben Straub, Pro Git (Second Edition) builds on the hugely successful first edition, and is now fully updated for Git version 2.0, as well as including an indispensable chapter on GitHub. It's the best book for all your Git needs.},
  isbn = {978-1-4842-0076-6},
  langid = {english},
  keywords = {git version-control},
  note = {\url{https://git-scm.com/book/en/v2}}
}

@misc{chenClusteringBasedSubsetSelection2021,
  title = {Clustering-Based Subset Selection in Evolutionary Multiobjective Optimization},
  author = {Chen, Weiyu and Ishibuchi, Hisao and Shang, Ke},
  year = {2021},
  month = aug,
  number = {arXiv:2108.08453},
  eprint = {2108.08453},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2108.08453},
  urldate = {2025-06-14},
  abstract = {Subset selection is an important component in evolutionary multiobjective optimization (EMO) algorithms. Clustering, as a classic method to group similar data points together, has been used for subset selection in some fields. However, clustering-based methods have not been evaluated in the context of subset selection from solution sets obtained by EMO algorithms. In this paper, we first review some classic clustering algorithms. We also point out that another popular subset selection method, i.e., inverted generational distance (IGD)-based subset selection, can be viewed as clustering. Then, we perform a comprehensive experimental study to evaluate the performance of various clustering algorithms in different scenarios. Experimental results are analyzed in detail, and some suggestions about the use of clustering algorithms for subset selection are derived. Additionally, we demonstrate that decision maker's preference can be introduced to clustering-based subset selection.},
  archiveprefix = {arXiv},
  howpublished = {\url{http://arxiv.org/abs/2108.08453}},
  keywords = {Computer Science - Neural and Evolutionary Computing}
}

@misc{CIFAR10Dataset,
  title = {CIFAR-10 dataset},
  urldate = {2025-06-12},
  howpublished = {\url{https://www.cs.toronto.edu/~kriz/cifar.html}}
}

@misc{Cifar10PyTorch,
  title = {Cifar10-PyTorch},
  urldate = {2025-06-12},
  howpublished = {\url{https://docs.pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html}}
}

@misc{CleanedArtImages,
  title = {(Cleaned) Art Images: Drawing/Painting/Sculptures/Engravings},
  shorttitle = {Art Images},
  urldate = {2025-02-24},
  abstract = {Dataset with about 9000 images containing 5 types of arts},
  howpublished = {\url{https://www.kaggle.com/datasets/moosecat/art-images-drawings-painting-sculpture-engraving}},
  langid = {english}
}

@misc{CondaDocumentation,
  title = {Conda Documentation},
  urldate = {2025-02-24},
  howpublished = {\url{https://docs.conda.io/en/latest/}}
}

@misc{CreationVirtualEnvironments,
  title = {Creation of virtual environments},
  journal = {Python documentation},
  urldate = {2025-02-24},
  abstract = {C{\'o}digo fuente: Lib/venv/ El m{\'o}dulo venv admite la creaci{\'o}n de <<entornos virtuales>> ligeros, cada uno con su propio conjunto independiente de paquetes de Python instalados en sus directorios site. S...},
  howpublished = {\url{https://docs.python.org/3/library/venv.html}},
  langid = {spanish}
}

@misc{CuBLASDeterministicAlgorithms,
  title = {cuBLAS Deterministic Algorithms},
  urldate = {2025-02-24},
  howpublished = {\url{https://docs.nvidia.com/cuda/cublas/index.html\#cublasApi_reproducibility}}
}

@incollection{derracSurveyEvolutionaryInstance2012,
  title = {A Survey on Evolutionary Instance Selection and Generation},
  booktitle = {Modeling, Analysis, and Applications in Metaheuristic Computing: Advancements and Trends},
  author = {Derrac, Joaqu{\'i}n and Garc{\'i}a, Salvador and Herrera, Francisco},
  year = {2012},
  pages = {233--266},
  publisher = {IGI Global Scientific Publishing},
  doi = {10.4018/978-1-4666-0270-0.ch014},
  urldate = {2025-06-14},
  abstract = {The use of Evolutionary Algorithms to perform data reduction tasks has become an effective approach to improve the performance of data mining algorithms. Many proposals in the literature have shown that Evolutionary Algorithms obtain excellent results in their application as Instance Selection and I...},
  copyright = {Access limited to members},
  isbn = {978-1-4666-0270-0},
  langid = {english},
  note = {\url{https://www.igi-global.com/gateway/chapter/63814}}
}

@inproceedings{dongMemeticAlgorithmEvolving2020,
  title = {A Memetic Algorithm for Evolving Deep Convolutional Neural Network in Image Classification},
  author = {Dong, Junwei and Zhang, Liangjie and Hou, Boyu and Feng, Liang},
  year = {2020},
  month = dec,
  pages = {2663--2669},
  doi = {10.1109/SSCI47803.2020.9308162}
}

@misc{EnhancingCollaborationProjectbased,
  title = {Enhancing collaboration in project-based organizations with IT},
  urldate = {2025-05-11},
  abstract = {The force driving a project-based organization (PBO) is the performance and completion of project tasks. Because of this, PBOs rely on collaborative relationships and innovative technologies to accomplish their goals. This paper examines a framework that could help PBOs improve their collaborations and a multi-level strategy that may help PBOs use innovation technologies more effectively. In doing so, it defines the concept of collaboration and overviews the field's literature on collaboration and on using technology to improve inter-organization collaboration; it identifies the barriers to collaboration that PBOs often face and the organizational typologies common to PBOs. It looks at how collaborative information technologies can support PBOs, noting how--via a proposed framework--these functionalities relate to the process of managing projects. It then outlines the proposed strategy, explaining the concept of e-collaboration and its relationship to collaboration in PBOs. It discusses the process of impleme},
  howpublished = {\url{https://www.pmi.org/learning/library/enhancing-collaboration-project-based-organizations-7141}},
  langid = {english}
}

@misc{euAIAct2024,
  title = {Article 10: Data and Data Governance, EU Artificial Intelligence Act},
  author = {{European Parliament and Council}},
  year = {2024},
  howpublished = {\url{https://artificialintelligenceact.eu/article/10/}}
}

@article{garciaMemeticAlgorithmEvolutionary2008,
  title = {A memetic algorithm for evolutionary prototype selection: A scaling up approach},
  shorttitle = {A memetic algorithm for evolutionary prototype selection},
  author = {Garc{\'i}a, Salvador and Cano, Jos{\'e} Ram{\'o}n and Herrera, Francisco},
  year = {2008},
  month = aug,
  journal = {Pattern Recognition},
  volume = {41},
  number = {8},
  pages = {2693--2709},
  issn = {0031-3203},
  doi = {10.1016/j.patcog.2008.02.006},
  urldate = {2025-06-14},
  abstract = {Prototype selection problem consists of reducing the size of databases by removing samples that are considered noisy or not influential on nearest neighbour classification tasks. Evolutionary algorithms have been used recently for prototype selection showing good results. However, due to the complexity of this problem when the size of the databases increases, the behaviour of evolutionary algorithms could deteriorate considerably because of a lack of convergence. This additional problem is known as the scaling up problem. Memetic algorithms are approaches for heuristic searches in optimization problems that combine a population-based algorithm with a local search. In this paper, we propose a model of memetic algorithm that incorporates an ad hoc local search specifically designed for optimizing the properties of prototype selection problem with the aim of tackling the scaling up problem. In order to check its performance, we have carried out an empirical study including a comparison between our proposal and previous evolutionary and non-evolutionary approaches studied in the literature. The results have been contrasted with the use of non-parametric statistical procedures and show that our approach outperforms previously studied methods, especially when the database scales up.},
  keywords = {Data mining,Data reduction,Evolutionary algorithms,Memetic algorithms,Nearest neighbour rule,Prototype selection,Scaling up},
  note = {\url{https://www.sciencedirect.com/science/article/pii/S0031320308000770}}
}

@book{goldbergGeneticAlgorithmsSearch1989,
  title = {Genetic Algorithms in Search, Optimization, and Machine Learning},
  author = {Goldberg, D.E.},
  year = {1989},
  series = {Addison Wesley series in artificial intelligence},
  publisher = {Addison-Wesley},
  isbn = {978-0-201-15767-3},
  lccn = {lc88006276},
  note = {\url{https://books.google.es/books?id=2IIJAAAACAAJ}}
}

@book{goodfellowDeepLearning2016,
  title = {Deep Learning},
  author = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
  year = {2016},
  publisher = {MIT Press}
}

@article{hartCondensedNearestNeighbor1968,
  title = {The condensed nearest neighbor rule (Corresp.)},
  author = {Hart, P.},
  year = {1968},
  month = may,
  journal = {IEEE Transactions on Information Theory},
  volume = {14},
  number = {3},
  pages = {515--516},
  issn = {1557-9654},
  doi = {10.1109/TIT.1968.1054155},
  urldate = {2025-06-14},
  note = {\url{https://ieeexplore.ieee.org/document/1054155}}
}

@inproceedings{heDeepResidualLearning2016,
  title = {Deep Residual Learning for Image Recognition},
  booktitle = {2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  year = {2016},
  pages = {770--778},
  doi = {10.1109/CVPR.2016.90},
  keywords = {Complexity theory,Degradation,Image recognition,Image segmentation,Neural networks,Training,Visualization}
}

@misc{hintonDistillingKnowledgeNeural2015,
  title = {Distilling the Knowledge in a Neural Network},
  author = {Hinton, Geoffrey and Vinyals, Oriol and Dean, Jeff},
  year = {2015},
  month = mar,
  number = {arXiv:1503.02531},
  eprint = {1503.02531},
  primaryclass = {stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1503.02531},
  urldate = {2025-06-14},
  abstract = {A very simple way to improve the performance of almost any machine learning algorithm is to train many different models on the same data and then to average their predictions. Unfortunately, making predictions using a whole ensemble of models is cumbersome and may be too computationally expensive to allow deployment to a large number of users, especially if the individual models are large neural nets. Caruana and his collaborators have shown that it is possible to compress the knowledge in an ensemble into a single model which is much easier to deploy and we develop this approach further using a different compression technique. We achieve some surprising results on MNIST and we show that we can significantly improve the acoustic model of a heavily used commercial system by distilling the knowledge in an ensemble of models into a single model. We also introduce a new type of ensemble composed of one or more full models and many specialist models which learn to distinguish fine-grained classes that the full models confuse. Unlike a mixture of experts, these specialist models can be trained rapidly and in parallel.},
  archiveprefix = {arXiv},
  howpublished = {\url{http://arxiv.org/abs/1503.02531}},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning}
}

@book{hollandAdaptationNaturalArtificial1975,
  title = {Adaptation in Natural and Artificial Systems: An Introductory Analysis with Applications to Biology, Control, and Artificial Intelligence},
  author = {Holland, J.H.},
  year = {1975},
  publisher = {University of Michigan Press},
  isbn = {978-0-472-08460-9},
  lccn = {lc74078988},
  note = {\url{https://books.google.es/books?id=YE5RAAAAMAAJ}}
}

@misc{howardMobileNetsEfficientConvolutional2017,
  title = {MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications},
  shorttitle = {MobileNets},
  author = {Howard, Andrew G. and Zhu, Menglong and Chen, Bo and Kalenichenko, Dmitry and Wang, Weijun and Weyand, Tobias and Andreetto, Marco and Adam, Hartwig},
  year = {2017},
  month = apr,
  number = {arXiv:1704.04861},
  eprint = {1704.04861},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1704.04861},
  abstract = {We present a class of efficient models called MobileNets for mobile and embedded vision applications. MobileNets are based on a streamlined architecture that uses depth-wise separable convolutions to build light weight deep neural networks. We introduce two simple global hyper-parameters that efficiently trade off between latency and accuracy. These hyper-parameters allow the model builder to choose the right sized model for their application based on the constraints of the problem. We present extensive experiments on resource and accuracy tradeoffs and show strong performance compared to other popular models on ImageNet classification. We then demonstrate the effectiveness of MobileNets across a wide range of applications and use cases including object detection, finegrain classification, face attributes and large scale geo-localization.},
  archiveprefix = {arXiv},
  howpublished = {\url{http://arxiv.org/abs/1704.04861}},
  keywords = {Computer Science - Computer Vision and Pattern Recognition}
}

@misc{jiaCertifiedRobustnessNearest2021,
  title = {Certified Robustness of Nearest Neighbors against Data Poisoning and Backdoor Attacks},
  author = {Jia, Jinyuan and Liu, Yupei and Cao, Xiaoyu and Gong, Neil Zhenqiang},
  year = {2021},
  month = dec,
  number = {arXiv:2012.03765},
  eprint = {2012.03765},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2012.03765},
  urldate = {2025-06-14},
  abstract = {Data poisoning attacks and backdoor attacks aim to corrupt a machine learning classifier via modifying, adding, and/or removing some carefully selected training examples, such that the corrupted classifier makes incorrect predictions as the attacker desires. The key idea of state-of-the-art certified defenses against data poisoning attacks and backdoor attacks is to create a majority vote mechanism to predict the label of a testing example. Moreover, each voter is a base classifier trained on a subset of the training dataset. Classical simple learning algorithms such as k nearest neighbors (kNN) and radius nearest neighbors (rNN) have intrinsic majority vote mechanisms. In this work, we show that the intrinsic majority vote mechanisms in kNN and rNN already provide certified robustness guarantees against data poisoning attacks and backdoor attacks. Moreover, our evaluation results on MNIST and CIFAR10 show that the intrinsic certified robustness guarantees of kNN and rNN outperform those provided by state-of-the-art certified defenses. Our results serve as standard baselines for future certified defenses against data poisoning attacks and backdoor attacks.},
  archiveprefix = {arXiv},
  howpublished = {\url{http://arxiv.org/abs/2012.03765}},
  keywords = {Computer Science - Cryptography and Security,Computer Science - Machine Learning}
}

@incollection{ketkarIntroductionPyTorch2021,
  title = {Introduction to PyTorch},
  booktitle = {Deep Learning with Python: Learn Best Practices of Deep Learning Models with PyTorch},
  author = {Ketkar, Nikhil and Moolayil, Jojo},
  year = {2021},
  pages = {27--91},
  publisher = {Apress},
  address = {Berkeley, CA},
  doi = {10.1007/978-1-4842-5364-9_2},
  abstract = {The recent years have witnessed major releases of frameworks and tools to democratize deep learning to the masses. Today, we have a plethora of options at our disposal. A few popular names include PyTorch, TensorFlow, Keras, and MXNet---the list is never-ending. This chapter aims to provide an overview of PyTorch. We will be using PyTorch extensively throughout the book for implementing deep learning examples. Note that this chapter is not a comprehensive guide for PyTorch, so you should consult the additional materials suggested in the chapter for a deeper understanding of the framework. A basic overview will be offered and the necessary additions to the topic will be provided in the course of the examples implemented later in the book.},
  isbn = {978-1-4842-5364-9},
  note = {\url{https://doi.org/10.1007/978-1-4842-5364-9_2}}
}

@incollection{kramerScikitLearn2016,
  title = {Scikit-Learn},
  booktitle = {Machine Learning for Evolution Strategies},
  author = {Kramer, Oliver},
  year = {2016},
  pages = {45--53},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-319-33383-0_5},
  abstract = {scikit-learn is an open source machine learning library written in Python.},
  isbn = {978-3-319-33383-0},
  note = {\url{https://doi.org/10.1007/978-3-319-33383-0_5}}
}

@misc{LargescaleDatasetPruning,
  title = {Large-scale Dataset Pruning with Dynamic Uncertainty},
  urldate = {2025-06-14},
  howpublished = {\url{https://arxiv.org/html/2306.05175v3}}
}

@article{lecunDeepLearning2015,
  title = {Deep learning},
  author = {LeCun, Yann and Bengio, Yoshua and Hinton, Geoffrey},
  year = {2015},
  month = may,
  journal = {Nature},
  volume = {521},
  number = {7553},
  pages = {436--444},
  issn = {1476-4687},
  doi = {10.1038/nature14539},
  abstract = {Deep learning allows computational models that are composed of multiple processing layers to learn representations of data with multiple levels of abstraction. These methods have dramatically improved the state-of-the-art in speech recognition, visual object recognition, object detection and many other domains such as drug discovery and genomics. Deep learning discovers intricate structure in large data sets by using the backpropagation algorithm to indicate how a machine should change its internal parameters that are used to compute the representation in each layer from the representation in the previous layer. Deep convolutional nets have brought about breakthroughs in processing images, video, speech and audio, whereas recurrent nets have shone light on sequential data such as text and speech.},
  note = {\url{https://doi.org/10.1038/nature14539}}
}

@article{leiComprehensiveSurveyDataset2024,
  title = {A Comprehensive Survey of Dataset Distillation},
  author = {Lei, Shiye and Tao, Dacheng},
  year = {2024},
  month = jan,
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume = {46},
  number = {1},
  eprint = {2301.05603},
  primaryclass = {cs},
  pages = {17--32},
  issn = {0162-8828, 2160-9292, 1939-3539},
  doi = {10.1109/TPAMI.2023.3322540},
  urldate = {2025-06-14},
  abstract = {Deep learning technology has developed unprecedentedly in the last decade and has become the primary choice in many application domains. This progress is mainly attributed to a systematic collaboration in which rapidly growing computing resources encourage advanced algorithms to deal with massive data. However, it has gradually become challenging to handle the unlimited growth of data with limited computing power. To this end, diverse approaches are proposed to improve data processing efficiency. Dataset distillation, a dataset reduction method, addresses this problem by synthesizing a small typical dataset from substantial data and has attracted much attention from the deep learning community. Existing dataset distillation methods can be taxonomized into meta-learning and data matching frameworks according to whether they explicitly mimic the performance of target data. Although dataset distillation has shown surprising performance in compressing datasets, there are still several limitations such as distilling high-resolution data or data with complex label spaces. This paper provides a holistic understanding of dataset distillation from multiple aspects, including distillation frameworks and algorithms, factorized dataset distillation, performance comparison, and applications. Finally, we discuss challenges and promising directions to further promote future studies on dataset distillation.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  note = {\url{http://arxiv.org/abs/2301.05603}}
}

@inproceedings{liGenerativeDatasetDistillation2024,
  title = {Generative Dataset Distillation: Balancing Global Structure and Local Details},
  shorttitle = {Generative Dataset Distillation},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  author = {Li, Longzhen and Li, Guang and Togo, Ren and Maeda, Keisuke and Ogawa, Takahiro and Haseyama, Miki},
  year = {2024},
  pages = {7664--7671},
  urldate = {2025-06-14},
  langid = {english},
  note = {\url{https://openaccess.thecvf.com/content/CVPR2024W/DDCV/html/Li_Generative_Dataset_Distillation_Balancing_Global_Structure_and_Local_Details_CVPRW_2024_paper.html}}
}

@misc{Matplotlib393Documentation,
  title = {Matplotlib 3.9.3 documentation},
  urldate = {2025-02-24},
  howpublished = {\url{https://matplotlib.org/3.9.3/index.html}}
}

@misc{MNISTDataset,
  title = {MNIST Dataset},
  urldate = {2025-02-24},
  abstract = {The MNIST database of handwritten digits (http://yann.lecun.com)},
  howpublished = {\url{https://www.kaggle.com/datasets/hojjatk/mnist-dataset}},
  langid = {english}
}

@inproceedings{molinaMASWChainsMemeticAlgorithm2010,
  title = {MA-SW-Chains: Memetic algorithm based on local search chains for large scale continuous global optimization},
  shorttitle = {MA-SW-Chains},
  booktitle = {IEEE Congress on Evolutionary Computation},
  author = {Molina, Daniel and Lozano, Manuel and Herrera, Francisco},
  year = {2010},
  month = jul,
  pages = {1--8},
  issn = {1941-0026},
  doi = {10.1109/CEC.2010.5586034},
  urldate = {2025-06-12},
  abstract = {Memetic algorithms are effective algorithms to obtain reliable and accurate solutions for complex continuous optimization problems. Nowadays, high dimensional optimization problems are an interesting field of research. The high dimensionality introduces new problems for the optimization process, requiring more scalable algorithms that, at the same time, could explore better the higher domain space around each solution. In this work, we proposed a memetic algorithm, MA-SW-Chains, for large scale global optimization. This algorithm assigns to each individual a local search intensity that depends on its features, by chaining different local search applications. MA-SW-Chains is an adaptation to large scale optimization of a previous algorithm, MA-CMA-Chains, to improve its performance on high-dimensional problems. Finally, we present the results obtained by our proposal using the benchmark problems defined in the Special Session of Large Scale Global Optimization on the IEEE Congress on Evolutionary Computation in 2010.},
  keywords = {Biological cells,Convergence,Evolutionary computation,Memetics,Optimization,Proposals,Steady-state},
  note = {\url{https://ieeexplore.ieee.org/abstract/document/5586034}}
}

@article{moscatoEvolutionSearchOptimization2000,
  title = {On Evolution, Search, Optimization, Genetic Algorithms and Martial Arts - Towards Memetic Algorithms},
  author = {Moscato, Pablo},
  year = {2000},
  month = oct,
  journal = {Caltech Concurrent Computation Program}
}

@book{neriHandbookMemeticAlgorithms2012,
  title = {Handbook of Memetic Algorithms},
  editor = {Neri, Ferrante and Cotta, Carlos and Moscato, Pablo and Kacprzyk, Janusz},
  year = {2012},
  series = {Studies in Computational Intelligence},
  volume = {379},
  publisher = {Springer},
  address = {Berlin, Heidelberg},
  doi = {10.1007/978-3-642-23247-3},
  copyright = {http://www.springer.com/tdm},
  isbn = {978-3-642-23246-6 978-3-642-23247-3},
  langid = {english},
  keywords = {Computational Intelligence,Memetic Algorithms,Memetic Computing},
  note = {\url{http://link.springer.com/10.1007/978-3-642-23247-3}}
}

@misc{NotionGestionTareas,
  title = {Notion - Gesti{\'o}n de Tareas},
  journal = {Notion},
  urldate = {2025-02-27},
  abstract = {Crea un panel personalizado para gestionar todas tus tareas de trabajo personales, a la vez que aportas informaci{\'o}n relevante de todo tu espacio de trabajo de Notion.},
  howpublished = {\url{https://www.notion.com/es-es/help/guides/personal-work-dashboard}},
  langid = {spanish}
}

@misc{NumPyV20Manual,
  title = {NumPy v2.0 Manual},
  urldate = {2025-02-24},
  howpublished = {\url{https://numpy.org/doc/2.0/index.html}}
}

@misc{Openpyxl313Documentation,
  title = {Openpyxl 3.1.3 documentation},
  urldate = {2025-05-03},
  howpublished = {\url{https://openpyxl.readthedocs.io/en/stable/}}
}

@misc{OriginalArtImages,
  title = {(Original) Art Images: Drawing/Painting/Sculptures/Engravings},
  shorttitle = {Art Images},
  urldate = {2025-02-24},
  abstract = {Dataset with about 9000 images containing 5 types of arts},
  howpublished = {\url{https://www.kaggle.com/datasets/thedownhill/art-images-drawings-painting-sculpture-engraving}},
  langid = {english}
}

@misc{OverviewGoogleCloud,
  title = {Overview Google Cloud},
  urldate = {2025-02-25},
  abstract = {An overview of Google Cloud Platform.},
  howpublished = {\url{https://cloud.google.com/docs/overview?hl=es-419}},
  langid = {spanish}
}

@misc{Pandas223Documentation,
  title = {Pandas 2.2.3 documentation},
  urldate = {2025-06-08},
  howpublished = {\url{https://pandas.pydata.org/pandas-docs/version/2.2/index.html}}
}

@misc{pattersonCarbonEmissionsLarge2021,
  title = {Carbon Emissions and Large Neural Network Training},
  author = {Patterson, David and Gonzalez, Joseph and Le, Quoc and Liang, Chen and Munguia, Lluis-Miquel and Rothchild, Daniel and So, David and Texier, Maud and Dean, Jeff},
  year = {2021},
  month = apr,
  number = {arXiv:2104.10350},
  eprint = {2104.10350},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2104.10350},
  urldate = {2025-06-14},
  abstract = {The computation demand for machine learning (ML) has grown rapidly recently, which comes with a number of costs. Estimating the energy cost helps measure its environmental impact and finding greener strategies, yet it is challenging without detailed information. We calculate the energy use and carbon footprint of several recent large models-T5, Meena, GShard, Switch Transformer, and GPT-3-and refine earlier estimates for the neural architecture search that found Evolved Transformer. We highlight the following opportunities to improve energy efficiency and CO2 equivalent emissions (CO2e): Large but sparsely activated DNNs can consume {$<$}1/10th the energy of large, dense DNNs without sacrificing accuracy despite using as many or even more parameters. Geographic location matters for ML workload scheduling since the fraction of carbon-free energy and resulting CO2e vary {\textasciitilde}5X-10X, even within the same country and the same organization. We are now optimizing where and when large models are trained. Specific datacenter infrastructure matters, as Cloud datacenters can be {\textasciitilde}1.4-2X more energy efficient than typical datacenters, and the ML-oriented accelerators inside them can be {\textasciitilde}2-5X more effective than off-the-shelf systems. Remarkably, the choice of DNN, datacenter, and processor can reduce the carbon footprint up to {\textasciitilde}100-1000X. These large factors also make retroactive estimates of energy cost difficult. To avoid miscalculations, we believe ML papers requiring large computational resources should make energy consumption and CO2e explicit when practical. We are working to be more transparent about energy use and CO2e in our future research. To help reduce the carbon footprint of ML, we believe energy usage and CO2e should be a key metric in evaluating models, and we are collaborating with MLPerf developers to include energy usage during training and inference in this industry standard benchmark.},
  archiveprefix = {arXiv},
  howpublished = {\url{http://arxiv.org/abs/2104.10350}},
  keywords = {Computer Science - Computers and Society,Computer Science - Machine Learning}
}

@inproceedings{pmlr-v81-buolamwini18a,
  title = {Gender Shades: Intersectional Accuracy Disparities in Commercial Gender Classification},
  booktitle = {Proceedings of the 1st Conference on Fairness, Accountability and Transparency},
  author = {Buolamwini, Joy and Gebru, Timnit},
  editor = {Friedler, Sorelle A. and Wilson, Christo},
  year = {2018},
  month = feb,
  series = {Proceedings of Machine Learning Research},
  volume = {81},
  pages = {77--91},
  publisher = {PMLR},
  abstract = {Recent studies demonstrate that machine learning algorithms can discriminate based on classes like race and gender. In this work, we present an approach to evaluate bias present in automated facial analysis algorithms and datasets with respect to phenotypic subgroups. Using the dermatologist approved Fitzpatrick Skin Type classification system, we characterize the gender and skin type distribution of two facial analysis benchmarks, IJB-A and Adience. We find that these datasets are overwhelmingly composed of lighter-skinned subjects (79.6},
  note = {\url{https://proceedings.mlr.press/v81/buolamwini18a.html}}
}

@misc{PolarsPythonAPI,
  title = {Polars --- Python API reference},
  urldate = {2025-02-24},
  howpublished = {\url{https://docs.pola.rs/api/python/stable/reference/}}
}

@misc{qmeeusEarlyStoppingDisscussion2018,
  type = {Forum post},
  title = {Early Stopping Disscussion},
  author = {{qmeeus}},
  year = {2018},
  month = aug,
  journal = {Data Science Stack Exchange},
  urldate = {2025-06-14},
  howpublished = {\url{https://datascience.stackexchange.com/q/37186}}
}

@misc{realRegularizedEvolutionImage2019,
  title = {Regularized Evolution for Image Classifier Architecture Search},
  author = {Real, Esteban and Aggarwal, Alok and Huang, Yanping and Le, Quoc V.},
  year = {2019},
  month = feb,
  number = {arXiv:1802.01548},
  eprint = {1802.01548},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1802.01548},
  urldate = {2025-06-14},
  abstract = {The effort devoted to hand-crafting neural network image classifiers has motivated the use of architecture search to discover them automatically. Although evolutionary algorithms have been repeatedly applied to neural network topologies, the image classifiers thus discovered have remained inferior to human-crafted ones. Here, we evolve an image classifier---AmoebaNet-A---that surpasses hand-designs for the first time. To do this, we modify the tournament selection evolutionary algorithm by introducing an age property to favor the younger genotypes. Matching size, AmoebaNet-A has comparable accuracy to current state-of-the-art ImageNet models discovered with more complex architecture-search methods. Scaled to larger size, AmoebaNet-A sets a new state-of-the-art 83.9\% / 96.6\% top-5 ImageNet accuracy. In a controlled comparison against a well known reinforcement learning algorithm, we give evidence that evolution can obtain results faster with the same hardware, especially at the earlier stages of the search. This is relevant when fewer compute resources are available. Evolution is, thus, a simple method to effectively discover high-quality architectures.},
  archiveprefix = {arXiv},
  howpublished = {\url{http://arxiv.org/abs/1802.01548}},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Distributed Parallel and Cluster Computing,Computer Science - Neural and Evolutionary Computing}
}

@misc{ReglamentoIA2024,
  title = {Reglamento (UE) 2024/1689 del Parlamento Europeo y del Consejo, de 13 de junio de 2024, sobre normas armonizadas en materia de inteligencia artificial},
  author = {{Parlamento Europeo y Consejo de la Uni{\'o}n Europea}},
  year = {2024},
  month = jul,
  howpublished = {\url{https://eur-lex.europa.eu/legal-content/ES/TXT/?uri=CELEX:32024R1689}},
  langid = {spanish}
}

@misc{RegulationEU20242024,
  title = {Regulation (EU) 2024/1689 of the European Parliament and of the Council of 13 June 2024 laying down harmonised rules on artificial intelligence and amending Regulations (EC) No 300/2008, (EU) No 167/2013, (EU) No 168/2013, (EU) 2018/858, (EU) 2018/1139 and (EU) 2019/2144 and Directives 2014/90/EU, (EU) 2016/797 and (EU) 2020/1828 (Artificial Intelligence Act) (Text with EEA relevance)},
  year = {2024},
  month = jun,
  urldate = {2025-06-14},
  howpublished = {\url{http://data.europa.eu/eli/reg/2024/1689/oj/eng}},
  langid = {english}
}

@misc{ResNet50,
  title = {ResNet50},
  journal = {PyTorch},
  urldate = {2025-02-24},
  abstract = {Model Description},
  howpublished = {\url{https://pytorch.org/hub/nvidia_deeplearningexamples_resnet50/}},
  langid = {english}
}

@misc{RockPaperScissors,
  title = {Rock Paper Scissors Dataset},
  journal = {Roboflow},
  urldate = {2025-02-24},
  abstract = {Download 2925 free images labeled for classification.},
  howpublished = {\url{https://public.roboflow.com/classification/rock-paper-scissors}}
}

@misc{rolnickDeepLearningRobust2018,
  title = {Deep Learning is Robust to Massive Label Noise},
  author = {Rolnick, David and Veit, Andreas and Belongie, Serge and Shavit, Nir},
  year = {2018},
  month = feb,
  number = {arXiv:1705.10694},
  eprint = {1705.10694},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1705.10694},
  urldate = {2025-06-14},
  abstract = {Deep neural networks trained on large supervised datasets have led to impressive results in image classification and other tasks. However, well-annotated datasets can be time-consuming and expensive to collect, lending increased interest to larger but noisy datasets that are more easily obtained. In this paper, we show that deep neural networks are capable of generalizing from training data for which true labels are massively outnumbered by incorrect labels. We demonstrate remarkably high test performance after training on corrupted data from MNIST, CIFAR, and ImageNet. For example, on MNIST we obtain test accuracy above 90 percent even after each clean training example has been diluted with 100 randomly-labeled examples. Such behavior holds across multiple patterns of label noise, even when erroneous labels are biased towards confusing classes. We show that training in this regime requires a significant but manageable increase in dataset size that is related to the factor by which correct labels have been diluted. Finally, we provide an analysis of our results that shows how increasing noise decreases the effective batch size.},
  archiveprefix = {arXiv},
  howpublished = {\url{http://arxiv.org/abs/1705.10694}},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing}
}

@misc{SalarioParaData,
  title = {Salario para Data Scientist en Espa{\~n}a - Salario Medio},
  journal = {Talent.com},
  urldate = {2025-02-24},
  abstract = {Data Scientist en Espa{\~n}a perciben un salario medio mensual {\texteuro} 3.389. Prueba la herramienta de salarios de Talent.com y descubre cu{\'a}l es el salario medio de los profesionales de la industria.},
  howpublished = {\url{https://es.talent.com/salary}},
  langid = {spanish}
}

@article{salman2023data,
  title = {Data selection for efficient model training: an overview},
  author = {Salman, Hisham and Zhan, Xingchao and Krishnan, Ranjit and Qi, Yingyu and Li, Yingya and Wang, Xing and Wang, Zhangyang},
  year = {2023},
  journal = {arXiv preprint arXiv:2306.05175},
  eprint = {2306.05175},
  archiveprefix = {arXiv}
}

@misc{sandlerMobileNetV2InvertedResiduals2019,
  title = {MobileNetV2: Inverted Residuals and Linear Bottlenecks},
  shorttitle = {MobileNetV2},
  author = {Sandler, Mark and Howard, Andrew and Zhu, Menglong and Zhmoginov, Andrey and Chen, Liang-Chieh},
  year = {2019},
  month = mar,
  number = {arXiv:1801.04381},
  eprint = {1801.04381},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1801.04381},
  abstract = {In this paper we describe a new mobile architecture, MobileNetV2, that improves the state of the art performance of mobile models on multiple tasks and benchmarks as well as across a spectrum of different model sizes. We also describe efficient ways of applying these mobile models to object detection in a novel framework we call SSDLite. Additionally, we demonstrate how to build mobile semantic segmentation models through a reduced form of DeepLabv3 which we call Mobile DeepLabv3. The MobileNetV2 architecture is based on an inverted residual structure where the input and output of the residual block are thin bottleneck layers opposite to traditional residual models which use expanded representations in the input an MobileNetV2 uses lightweight depthwise convolutions to filter features in the intermediate expansion layer. Additionally, we find that it is important to remove non-linearities in the narrow layers in order to maintain representational power. We demonstrate that this improves performance and provide an intuition that led to this design. Finally, our approach allows decoupling of the input/output domains from the expressiveness of the transformation, which provides a convenient framework for further analysis. We measure our performance on Imagenet classification, COCO object detection, VOC image segmentation. We evaluate the trade-offs between accuracy, and number of operations measured by multiply-adds (MAdd), as well as the number of parameters},
  archiveprefix = {arXiv},
  howpublished = {\url{http://arxiv.org/abs/1801.04381}},
  keywords = {Computer Science - Computer Vision and Pattern Recognition}
}

@misc{ScrumGuide,
  title = {Scrum Guide},
  urldate = {2025-02-25},
  howpublished = {\url{https://scrumguides.org/scrum-guide.html}}
}

@misc{Seaborn0132Documentation,
  title = {Seaborn 0.13.2 documentation},
  urldate = {2025-05-03},
  howpublished = {\url{https://seaborn.pydata.org/tutorial.html}}
}

@article{shortenSurveyImageData2019,
  title = {A survey on Image Data Augmentation for Deep Learning},
  author = {Shorten, Connor and Khoshgoftaar, Taghi M.},
  year = {2019},
  month = jul,
  journal = {Journal of Big Data},
  volume = {6},
  number = {1},
  pages = {60},
  issn = {2196-1115},
  doi = {10.1186/s40537-019-0197-0},
  abstract = {Deep convolutional neural networks have performed remarkably well on many Computer Vision tasks. However, these networks are heavily reliant on big data to avoid overfitting. Overfitting refers to the phenomenon when a network learns a function with very high variance such as to perfectly model the training data. Unfortunately, many application domains do not have access to big data, such as medical image analysis. This survey focuses on Data Augmentation, a data-space solution to the problem of limited data. Data Augmentation encompasses a suite of techniques that enhance the size and quality of training datasets such that better Deep Learning models can be built using them. The image augmentation algorithms discussed in this survey include geometric transformations, color space augmentations, kernel filters, mixing images, random erasing, feature space augmentation, adversarial training, generative adversarial networks, neural style transfer, and meta-learning. The application of augmentation methods based on GANs are heavily covered in this survey. In addition to augmentation techniques, this paper will briefly discuss other characteristics of Data Augmentation such as test-time augmentation, resolution impact, final dataset size, and curriculum learning. This survey will present existing methods for Data Augmentation, promising developments, and meta-level decisions for implementing Data Augmentation. Readers will understand how Data Augmentation can improve the performance of their models and expand limited datasets to take advantage of the capabilities of big data.},
  note = {\url{https://doi.org/10.1186/s40537-019-0197-0}}
}

@misc{sinhaTeSTTesttimeSelfTraining2022,
  title = {TeST: Test-time Self-Training under Distribution Shift},
  shorttitle = {TeST},
  author = {Sinha, Samarth and Gehler, Peter and Locatello, Francesco and Schiele, Bernt},
  year = {2022},
  month = sep,
  number = {arXiv:2209.11459},
  eprint = {2209.11459},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2209.11459},
  urldate = {2025-06-14},
  abstract = {Despite their recent success, deep neural networks continue to perform poorly when they encounter distribution shifts at test time. Many recently proposed approaches try to counter this by aligning the model to the new distribution prior to inference. With no labels available this requires unsupervised objectives to adapt the model on the observed test data. In this paper, we propose Test-Time Self-Training (TeST): a technique that takes as input a model trained on some source data and a novel data distribution at test time, and learns invariant and robust representations using a student-teacher framework. We find that models adapted using TeST significantly improve over baseline test-time adaptation algorithms. TeST achieves competitive performance to modern domain adaptation algorithms, while having access to 5-10x less data at time of adaption. We thoroughly evaluate a variety of baselines on two tasks: object detection and image segmentation and find that models adapted with TeST. We find that TeST sets the new state-of-the art for test-time domain adaptation algorithms.},
  archiveprefix = {arXiv},
  howpublished = {\url{http://arxiv.org/abs/2209.11459}},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning}
}

@misc{sorscherNeuralScalingLaws2023,
  title = {Beyond neural scaling laws: beating power law scaling via data pruning},
  shorttitle = {Beyond neural scaling laws},
  author = {Sorscher, Ben and Geirhos, Robert and Shekhar, Shashank and Ganguli, Surya and Morcos, Ari S.},
  year = {2023},
  month = apr,
  number = {arXiv:2206.14486},
  eprint = {2206.14486},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2206.14486},
  urldate = {2025-06-14},
  abstract = {Widely observed neural scaling laws, in which error falls off as a power of the training set size, model size, or both, have driven substantial performance improvements in deep learning. However, these improvements through scaling alone require considerable costs in compute and energy. Here we focus on the scaling of error with dataset size and show how in theory we can break beyond power law scaling and potentially even reduce it to exponential scaling instead if we have access to a high-quality data pruning metric that ranks the order in which training examples should be discarded to achieve any pruned dataset size. We then test this improved scaling prediction with pruned dataset size empirically, and indeed observe better than power law scaling in practice on ResNets trained on CIFAR-10, SVHN, and ImageNet. Next, given the importance of finding high-quality pruning metrics, we perform the first large-scale benchmarking study of ten different data pruning metrics on ImageNet. We find most existing high performing metrics scale poorly to ImageNet, while the best are computationally intensive and require labels for every image. We therefore developed a new simple, cheap and scalable self-supervised pruning metric that demonstrates comparable performance to the best supervised metrics. Overall, our work suggests that the discovery of good data-pruning metrics may provide a viable path forward to substantially improved neural scaling laws, thereby reducing the resource costs of modern deep learning.},
  archiveprefix = {arXiv},
  howpublished = {\url{http://arxiv.org/abs/2206.14486}},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning}
}

@inproceedings{strubellEnergyPolicyConsiderations2019,
  title = {Energy and Policy Considerations for Deep Learning in NLP},
  booktitle = {Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
  author = {Strubell, Emma and Ganesh, Ananya and McCallum, Andrew},
  editor = {Korhonen, Anna and Traum, David and M{\`a}rquez, Llu{\'i}s},
  year = {2019},
  month = jul,
  pages = {3645--3650},
  publisher = {Association for Computational Linguistics},
  address = {Florence, Italy},
  doi = {10.18653/v1/P19-1355},
  urldate = {2025-06-14},
  abstract = {Recent progress in hardware and methodology for training neural networks has ushered in a new generation of large networks trained on abundant data. These models have obtained notable gains in accuracy across many NLP tasks. However, these accuracy improvements depend on the availability of exceptionally large computational resources that necessitate similarly substantial energy consumption. As a result these models are costly to train and develop, both financially, due to the cost of hardware and electricity or cloud compute time, and environmentally, due to the carbon footprint required to fuel modern tensor processing hardware. In this paper we bring this issue to the attention of NLP researchers by quantifying the approximate financial and environmental costs of training a variety of recently successful neural network models for NLP. Based on these findings, we propose actionable recommendations to reduce costs and improve equity in NLP research and practice.},
  note = {\url{https://aclanthology.org/P19-1355/}}
}

@article{subburajFuzzySystemBased2025,
  title = {A fuzzy system based self-adaptive memetic algorithm using population diversity control for evolutionary multi-objective optimization},
  author = {Subburaj, Brindha and Miruna Joe Amali, S.},
  year = {2025},
  month = feb,
  journal = {Scientific Reports},
  volume = {15},
  number = {1},
  pages = {5735},
  publisher = {Nature Publishing Group},
  issn = {2045-2322},
  doi = {10.1038/s41598-025-89289-2},
  urldate = {2025-06-12},
  abstract = {Simulated by nature's evolution, numerous evolutionary algorithms had been proposed. These algorithms perform better for a particular problem domain and extensive parameter fine tuning and adaptations are required in optimizing problems of varied domain. This paper aims to develop robust and self-adaptive memetic algorithm by combining Differential Evolution based algorithm, a popular population based global search method with the Controlled Local search procedure to solve multi-objective optimization problems. Memetic Algorithm is an enhanced evolutionary algorithm, it combines global search method with local search techniques for faster convergence. Memetic algorithm improves both exploration and exploitation, preventing premature convergence and also refines the current best solutions efficiently. Proposed algorithm is named as Fuzzy based Memetic Algorithm using Diversity control (F-MAD). In F-MAD, population diversity is controlled through the control parameters self-adaptation of Differential Evolution algorithm (DE) such as, crossover rate and scaling factor by using two fuzzy systems. A controlled local search procedure is adapted for guiding convergence process thus balancing explore-exploit cycle. The control parameter self-adaptation and enhanced selection method with controlled local search method aid population diversity control in decision space and attaining optimal solutions with uniform distribution in terms of diversity and convergence metrics in objective space. These characteristics help the proposed method suitable to be extended to different application domain without the need of trial-and-error fine tuning of the parameters. The performance is tested through standard benchmark test problems-CEC 2009 test problems and DTLZ test problem and further validated through performance metrics and statistical test. It is compared with popular optimization algorithms and experiment results indicate that F-MAD perform well than State of-The-Art (SOTA) algorithms taken for comparison. F-MAD algorithm attains better results for 8 out of 10 CEC 2009 test problems (UF1-UF10) when compared to 20 other algorithms taken for comparison. For DTLZ problems, F-MAD attains better results for ALL 7 problems (DTLZ 1-DTLZ7) when compared to 8 other SOTA algorithms. The performance is further evaluated using Friedman rank test and the proposed F-MAD significantly outperformed other algorithms.},
  copyright = {2025 The Author(s)},
  langid = {english},
  keywords = {Computer science,Engineering},
  note = {\url{https://www.nature.com/articles/s41598-025-89289-2}}
}

@book{sydenhamHandbookMeasuringSystem2005,
  title = {Handbook of measuring system design},
  editor = {Sydenham, Peter H. and Thorn, Richard},
  year = {2005},
  publisher = {Wiley},
  address = {Chichester},
  isbn = {978-0-470-02143-9},
  langid = {english}
}

@misc{TorchcudaPyTorch24,
  title = {torch.cuda --- PyTorch 2.4 documentation},
  urldate = {2024-10-08},
  howpublished = {\url{https://pytorch.org/docs/stable/cuda.html}}
}

@book{vanderplasPythonDataScience2016,
  title = {Python Data Science Handbook: Essential Tools for Working with Data},
  shorttitle = {Python Data Science Handbook},
  author = {VanderPlas, Jake},
  year = {2016},
  month = nov,
  publisher = {"O'Reilly Media, Inc."},
  abstract = {For many researchers, Python is a first-class tool mainly because of its libraries for storing, manipulating, and gaining insight from data. Several resources exist for individual pieces of this data science stack, but only with the Python Data Science Handbook do you get them all---IPython, NumPy, Pandas, Matplotlib, Scikit-Learn, and other related tools.Working scientists and data crunchers familiar with reading and writing Python code will find this comprehensive desk reference ideal for tackling day-to-day issues: manipulating, transforming, and cleaning data; visualizing different types of data; and using data to build statistical or machine learning models. Quite simply, this is the must-have reference for scientific computing in Python.With this handbook, you'll learn how to use:IPython and Jupyter: provide computational environments for data scientists using PythonNumPy: includes the ndarray for efficient storage and manipulation of dense data arrays in PythonPandas: features the DataFrame for efficient storage and manipulation of labeled/columnar data in PythonMatplotlib: includes capabilities for a flexible range of data visualizations in PythonScikit-Learn: for efficient and clean Python implementations of the most important and established machine learning algorithms},
  googlebooks = {xYmNDQAAQBAJ},
  isbn = {978-1-4919-1214-0},
  langid = {english},
  keywords = {Computers / Data Science / Data Modeling & Design,Computers / Data Science / Data Visualization,Computers / Languages / General,Computers / Languages / Python,Computers / Programming / Open Source,Science / Research & Methodology},
  note = {\url{https://books.google.es/books?hl=es&lr=&id=xYmNDQAAQBAJ&oi=fnd&pg=PR2&dq=Python+Data+Science+Handbook&ots=Xs9Rj3qj-M&sig=f5K5ixzKjH7pc2Uo2IYW9jrPNI8\#v=onepage&q=Python\%20Data\%20Science\%20Handbook&f=false}}
}

@misc{wangDatasetDistillation2020,
  title = {Dataset Distillation},
  author = {Wang, Tongzhou and Zhu, Jun-Yan and Torralba, Antonio and Efros, Alexei A.},
  year = {2020},
  month = feb,
  number = {arXiv:1811.10959},
  eprint = {1811.10959},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1811.10959},
  urldate = {2025-06-14},
  abstract = {Model distillation aims to distill the knowledge of a complex model into a simpler one. In this paper, we consider an alternative formulation called dataset distillation: we keep the model fixed and instead attempt to distill the knowledge from a large training dataset into a small one. The idea is to synthesize a small number of data points that do not need to come from the correct data distribution, but will, when given to the learning algorithm as training data, approximate the model trained on the original data. For example, we show that it is possible to compress 60,000 MNIST training images into just 10 synthetic distilled images (one per class) and achieve close to original performance with only a few gradient descent steps, given a fixed network initialization. We evaluate our method in various initialization settings and with different learning objectives. Experiments on multiple datasets show the advantage of our approach compared to alternative methods.},
  archiveprefix = {arXiv},
  howpublished = {\url{http://arxiv.org/abs/1811.10959}},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning}
}

@misc{wangImprovedCertifiedDefenses2022,
  title = {Improved Certified Defenses against Data Poisoning with (Deterministic) Finite Aggregation},
  author = {Wang, Wenxiao and Levine, Alexander and Feizi, Soheil},
  year = {2022},
  month = jul,
  number = {arXiv:2202.02628},
  eprint = {2202.02628},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2202.02628},
  urldate = {2025-06-14},
  abstract = {Data poisoning attacks aim at manipulating model behaviors through distorting training data. Previously, an aggregation-based certified defense, Deep Partition Aggregation (DPA), was proposed to mitigate this threat. DPA predicts through an aggregation of base classifiers trained on disjoint subsets of data, thus restricting its sensitivity to dataset distortions. In this work, we propose an improved certified defense against general poisoning attacks, namely Finite Aggregation. In contrast to DPA, which directly splits the training set into disjoint subsets, our method first splits the training set into smaller disjoint subsets and then combines duplicates of them to build larger (but not disjoint) subsets for training base classifiers. This reduces the worst-case impacts of poison samples and thus improves certified robustness bounds. In addition, we offer an alternative view of our method, bridging the designs of deterministic and stochastic aggregation-based certified defenses. Empirically, our proposed Finite Aggregation consistently improves certificates on MNIST, CIFAR-10, and GTSRB, boosting certified fractions by up to 3.05\%, 3.87\% and 4.77\%, respectively, while keeping the same clean accuracies as DPA's, effectively establishing a new state of the art in (pointwise) certified robustness against data poisoning.},
  archiveprefix = {arXiv},
  howpublished = {\url{http://arxiv.org/abs/2202.02628}},
  keywords = {Computer Science - Cryptography and Security,Computer Science - Machine Learning,Statistics - Machine Learning}
}

@book{weidmanDeepLearningScratch2019,
  title = {Deep Learning from Scratch: Building with Python from First Principles},
  author = {Weidman, S.},
  year = {2019},
  publisher = {O'Reilly Media, Incorporated},
  isbn = {978-1-4920-4141-2},
  lccn = {2020301331},
  note = {\url{https://books.google.es/books?id=PRSCwwEACAAJ}}
}

@misc{WhatCloudRun,
  title = {What is Cloud Run {\textbar} Cloud Run Documentation},
  urldate = {2025-02-25},
  howpublished = {\url{https://cloud.google.com/run/docs/overview/what-is-cloud-run}},
  langid = {english}
}

@article{wilsonAsymptoticPropertiesNearest1972,
  title = {Asymptotic Properties of Nearest Neighbor Rules Using Edited Data},
  author = {Wilson, Dennis L.},
  year = {1972},
  month = jul,
  journal = {IEEE Transactions on Systems, Man, and Cybernetics},
  volume = {SMC-2},
  number = {3},
  pages = {408--421},
  issn = {2168-2909},
  doi = {10.1109/TSMC.1972.4309137},
  urldate = {2025-06-14},
  abstract = {The convergence properties of a nearest neighbor rule that uses an editing procedure to reduce the number of preclassified samples and to improve the performance of the rule are developed. Editing of the preclassified samples using the three-nearest neighbor rule followed by classification using the single-nearest neighbor rule with the remaining preclassified samples appears to produce a decision procedure whose risk approaches the Bayes' risk quite closely in many problems with only a few preclassified samples. The asymptotic risk of the nearest neighbor rules and the nearest neighbor rules using edited preclassified samples is calculated for several problems.},
  keywords = {Character recognition,Convergence,Decoding,Nearest neighbor searches,Pattern recognition,Random variables},
  note = {\url{https://ieeexplore.ieee.org/document/4309137}}
}

@article{woernerComprehensiveEasytouseMultidomain2025,
  title = {A comprehensive and easy-to-use multi-domain multi-task medical imaging meta-dataset},
  author = {Woerner, Stefano and Jaques, Arthur and Baumgartner, Christian F.},
  year = {2025},
  month = apr,
  journal = {Scientific Data},
  volume = {12},
  number = {1},
  pages = {666},
  publisher = {Nature Publishing Group},
  issn = {2052-4463},
  doi = {10.1038/s41597-025-04866-4},
  urldate = {2025-06-14},
  abstract = {While the field of medical image analysis has undergone a transformative shift with the integration of machine learning techniques, the main challenge of these techniques is often the scarcity of large, diverse, and well-annotated datasets. Medical images vary in format, size, and other parameters and therefore require extensive preprocessing and standardization, for usage in machine learning. Addressing these challenges, we introduce the Medical Imaging Meta-Dataset (MedIMeta), a novel multi-domain, multi-task meta-dataset. MedIMeta contains 19 medical imaging datasets spanning 10 different domains and encompassing 54 distinct medical tasks, all of which are standardized to the same format and readily usable in PyTorch or other ML frameworks. We perform a technical validation of MedIMeta, demonstrating its utility through fully supervised and cross-domain few-shot learning baselines.},
  copyright = {2025 The Author(s)},
  langid = {english},
  keywords = {Medical research,Scientific data},
  note = {\url{https://www.nature.com/articles/s41597-025-04866-4}}
}

@misc{zhaDatacentricArtificialIntelligence2023,
  title = {Data-centric Artificial Intelligence: A Survey},
  shorttitle = {Data-centric Artificial Intelligence},
  author = {Zha, Daochen and Bhat, Zaid Pervaiz and Lai, Kwei-Herng and Yang, Fan and Jiang, Zhimeng and Zhong, Shaochen and Hu, Xia},
  year = {2023},
  month = jun,
  number = {arXiv:2303.10158},
  eprint = {2303.10158},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2303.10158},
  urldate = {2025-06-14},
  abstract = {Artificial Intelligence (AI) is making a profound impact in almost every domain. A vital enabler of its great success is the availability of abundant and high-quality data for building machine learning models. Recently, the role of data in AI has been significantly magnified, giving rise to the emerging concept of data-centric AI. The attention of researchers and practitioners has gradually shifted from advancing model design to enhancing the quality and quantity of the data. In this survey, we discuss the necessity of data-centric AI, followed by a holistic view of three general data-centric goals (training data development, inference data development, and data maintenance) and the representative methods. We also organize the existing literature from automation and collaboration perspectives, discuss the challenges, and tabulate the benchmarks for various tasks. We believe this is the first comprehensive survey that provides a global view of a spectrum of tasks across various stages of the data lifecycle. We hope it can help the readers efficiently grasp a broad picture of this field, and equip them with the techniques and further research ideas to systematically engineer data for building AI systems. A companion list of data-centric AI resources will be regularly updated on https://github.com/daochenzha/data-centric-AI},
  archiveprefix = {arXiv},
  howpublished = {\url{http://arxiv.org/abs/2303.10158}},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Databases,Computer Science - Machine Learning}
}

@misc{zhaoDatasetCondensationGradient2021,
  title = {Dataset Condensation with Gradient Matching},
  author = {Zhao, Bo and Mopuri, Konda Reddy and Bilen, Hakan},
  year = {2021},
  month = mar,
  number = {arXiv:2006.05929},
  eprint = {2006.05929},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2006.05929},
  urldate = {2025-06-14},
  abstract = {As the state-of-the-art machine learning methods in many fields rely on larger datasets, storing datasets and training models on them become significantly more expensive. This paper proposes a training set synthesis technique for data-efficient learning, called Dataset Condensation, that learns to condense large dataset into a small set of informative synthetic samples for training deep neural networks from scratch. We formulate this goal as a gradient matching problem between the gradients of deep neural network weights that are trained on the original and our synthetic data. We rigorously evaluate its performance in several computer vision benchmarks and demonstrate that it significantly outperforms the state-of-the-art methods. Finally we explore the use of our method in continual learning and neural architecture search and report promising gains when limited memory and computations are available.},
  archiveprefix = {arXiv},
  howpublished = {\url{http://arxiv.org/abs/2006.05929}},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning}
}

@article{zhaoReviewConvolutionalNeural2024,
  title = {A review of convolutional neural networks in computer vision},
  author = {Zhao, Xia and Wang, Limin and Zhang, Yufei and Han, Xuming and Deveci, Muhammet and Parmar, Milan},
  year = {2024},
  month = mar,
  journal = {Artificial Intelligence Review},
  volume = {57},
  number = {4},
  pages = {99},
  issn = {1573-7462},
  doi = {10.1007/s10462-024-10721-6},
  abstract = {In computer vision, a series of exemplary advances have been made in several areas involving image classification, semantic segmentation, object detection, and image super-resolution reconstruction with the rapid development of deep convolutional neural network (CNN). The CNN has superior features for autonomous learning and expression, and feature extraction from original input data can be realized by means of training CNN models that match practical applications. Due to the rapid progress in deep learning technology, the structure of CNN is becoming more and more complex and diverse. Consequently, it gradually replaces the traditional machine learning methods. This paper presents an elementary understanding of CNN components and their functions, including input layers, convolution layers, pooling layers, activation functions, batch normalization, dropout, fully connected layers, and output layers. On this basis, this paper gives a comprehensive overview of the past and current research status of the applications of CNN models in computer vision fields, e.g., image classification, object detection, and video prediction. In addition, we summarize the challenges and solutions of the deep CNN, and future research directions are also discussed.},
  langid = {english},
  keywords = {Artificial Intelligence,Computer vision,Convolutional neural networks,Deep learning,Status quo review},
  note = {\url{https://doi.org/10.1007/s10462-024-10721-6}}
}

@misc{zhouDistilledOneShotFederated2021,
  title = {Distilled One-Shot Federated Learning},
  author = {Zhou, Yanlin and Pu, George and Ma, Xiyao and Li, Xiaolin and Wu, Dapeng},
  year = {2021},
  month = jun,
  number = {arXiv:2009.07999},
  eprint = {2009.07999},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2009.07999},
  urldate = {2025-06-14},
  abstract = {Current federated learning algorithms take tens of communication rounds transmitting unwieldy model weights under ideal circumstances and hundreds when data is poorly distributed. Inspired by recent work on dataset distillation and distributed one-shot learning, we propose Distilled One-Shot Federated Learning (DOSFL) to significantly reduce the communication cost while achieving comparable performance. In just one round, each client distills their private dataset, sends the synthetic data (e.g. images or sentences) to the server, and collectively trains a global model. The distilled data look like noise and are only useful to the specific model weights, i.e., become useless after the model updates. With this weight-less and gradient-less design, the total communication cost of DOSFL is up to three orders of magnitude less than FedAvg while preserving between 93\% to 99\% performance of a centralized counterpart. Afterwards, clients could switch to traditional methods such as FedAvg to finetune the last few percent to fit personalized local models with local datasets. Through comprehensive experiments, we show the accuracy and communication performance of DOSFL on both vision and language tasks with different models including CNN, LSTM, Transformer, etc. We demonstrate that an eavesdropping attacker cannot properly train a good model using the leaked distilled data, without knowing the initial model weights. DOSFL serves as an inexpensive method to quickly converge on a performant pre-trained model with less than 0.1\% communication cost of traditional methods.},
  archiveprefix = {arXiv},
  howpublished = {\url{http://arxiv.org/abs/2009.07999}},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning}
}
